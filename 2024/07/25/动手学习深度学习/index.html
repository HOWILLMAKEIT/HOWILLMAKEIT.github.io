<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>动手学习深度学习 | HOWILL'S WORLD</title><meta name="author" content="HOWILL"><meta name="copyright" content="HOWILL"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ch01_预备知识张量索引与切片行和列都是从0开始，-1可以表示最后一行or最后一列 访问元素: 例如[1:3,1:]表示访问第1、2行以及从第一列开始的所有列  [::3,::2]	 	::3表示切片，每三行取一行  [3]表示第三行	[,3]表示第三列 创建张量torch.arange(12)#随机torch.randn(3, 4)#全0torch.zeros((2,3,4))   #赋值to">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学习深度学习">
<meta property="og:url" content="https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="HOWILL&#39;S WORLD">
<meta property="og:description" content="ch01_预备知识张量索引与切片行和列都是从0开始，-1可以表示最后一行or最后一列 访问元素: 例如[1:3,1:]表示访问第1、2行以及从第一列开始的所有列  [::3,::2]	 	::3表示切片，每三行取一行  [3]表示第三行	[,3]表示第三列 创建张量torch.arange(12)#随机torch.randn(3, 4)#全0torch.zeros((2,3,4))   #赋值to">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://howillmakeit.github.io/img/434052135_1607214906781670_1513540648802057013_n.jpg">
<meta property="article:published_time" content="2024-07-25T05:47:20.000Z">
<meta property="article:modified_time" content="2024-09-23T02:58:25.213Z">
<meta property="article:author" content="HOWILL">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://howillmakeit.github.io/img/434052135_1607214906781670_1513540648802057013_n.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '动手学习深度学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-23 10:58:25'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/434052135_1607214906781670_1513540648802057013_n.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/background.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="HOWILL'S WORLD"><span class="site-name">HOWILL'S WORLD</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">动手学习深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-25T05:47:20.000Z" title="发表于 2024-07-25 13:47:20">2024-07-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-23T02:58:25.213Z" title="更新于 2024-09-23 10:58:25">2024-09-23</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="动手学习深度学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="ch01-预备知识"><a href="#ch01-预备知识" class="headerlink" title="ch01_预备知识"></a>ch01_预备知识</h1><h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h4><p>行和列都是从0开始，-1可以表示最后一行or最后一列</p>
<p>访问元素: 例如[1:3,1:]表示访问第1、2行以及从第一列开始的所有列</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240726170521499.png" alt="image-20240726170521499"></p>
<p>[::3,::2]	 	::3表示切片，每三行取一行</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240726170626631.png" alt="image-20240726170626631"></p>
<p>[3]表示第三行	[,3]表示第三列</p>
<h4 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">12</span>)</span><br><span class="line"><span class="comment">#随机</span></span><br><span class="line">torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">#全0</span></span><br><span class="line">torch.zeros((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240726172228760.png" alt="image-20240726172228760"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#赋值</span></span><br><span class="line">torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Z的shape和数据类型都与y相同</span></span><br><span class="line">Z = torch.zeros_like(Y)</span><br></pre></td></tr></table></figure>



<h4 id="访问张量"><a href="#访问张量" class="headerlink" title="访问张量"></a>访问张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape <span class="comment">#torch.size([12])</span></span><br><span class="line">x.numel() <span class="comment">#12</span></span><br></pre></td></tr></table></figure>

<h4 id="修改张量"><a href="#修改张量" class="headerlink" title="修改张量"></a>修改张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X= x.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">X.shape <span class="comment">#torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<h4 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h4><p>加减乘除求幂</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">x + y, x - y, x * y, x / y, x ** y  <span class="comment"># **运算符是求幂运算</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>, <span class="number">10.</span>]),</span><br><span class="line"> tensor([-<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">6.</span>]),</span><br><span class="line"> tensor([ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">16.</span>]),</span><br><span class="line"> tensor([<span class="number">0.5000</span>, <span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">4.0000</span>]),</span><br><span class="line"> tensor([ <span class="number">1.</span>,  <span class="number">4.</span>, <span class="number">16.</span>, <span class="number">64.</span>]))</span><br></pre></td></tr></table></figure>

<p>更多计算</p>
<p>指数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure>

<p>求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()<span class="comment">#结果是一个元素</span></span><br></pre></td></tr></table></figure>

<h4 id="连结张量"><a href="#连结张量" class="headerlink" title="连结张量"></a>连结张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), <span class="comment">#在行上合并</span></span><br><span class="line">torch.cat((X, Y), dim=<span class="number">1</span>)  <span class="comment">#在列上合并		</span></span><br></pre></td></tr></table></figure>

<h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p><strong>即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作</strong>]。 这种机制的工作方式如下：</p>
<ol>
<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>
<li>对生成的数组执行按元素操作。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br><span class="line"><span class="comment">########################</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a + b</span><br><span class="line"><span class="comment">########################</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<h4 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tensor-&gt;numpy</span></span><br><span class="line">A = X.numpy()</span><br><span class="line"><span class="comment">#numpy-&gt;tensor</span></span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="comment">#当tensor大小为1时，tensor-&gt;python标量 </span></span><br><span class="line">a= torch.tensor([<span class="number">3.5</span>])</span><br></pre></td></tr></table></figure>





<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>本节将简要介绍使用<code>pandas</code>预处理原始数据，并将原始数据转换为张量格式的步骤。 后面的章节将介绍更多的数据预处理技术。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># &#x27;..&#x27;代表本目录的父目录</span></span><br><span class="line"><span class="comment"># exist_ok=True表示假如已经存在data目录，不会重新创建且不会报错</span></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># data_file存储data目录下文件house_tiny.csv的路径</span></span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 用write的方式打开</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>这里缺失了数据，这里我们考虑插值法处理</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727114348965.png" alt="image-20240727114348965"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="comment"># inputs获取第0、1列 outputs获取第2列</span></span><br><span class="line"><span class="comment"># .iloc是pandas库中基于位置进行索引的函数</span></span><br><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, -<span class="number">1</span>]</span><br><span class="line"><span class="comment"># .mean用于获取均值，numeric_only=True表示只考虑数值</span></span><br><span class="line">inputs = inputs.fillna(inputs.mean(numeric_only=<span class="literal">True</span>))</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727114906568.png" alt="image-20240727114906568"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建虚拟变量  dummy_na=True表示要将缺失值（NaN）转换为一个新的虚拟变量</span></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727115335144.png" alt="image-20240727115335144"></p>
<p>转化为张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727115521841.png" alt="image-20240727115521841"></p>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h3 id="线性代数实现"><a href="#线性代数实现" class="headerlink" title="线性代数实现"></a>线性代数实现</h3><p><strong>标量</strong>：用只有一个元素的张量表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br></pre></td></tr></table></figure>

<p><strong>向量</strong>：<strong>标量值组成的列表</strong></p>
<p>认为列向量是向量的默认方向</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>用索引访问到的当个元素也是张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br><span class="line"><span class="comment"># tensor(3)</span></span><br></pre></td></tr></table></figure>

<p>向量的长度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(x)</span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure>

<p><strong>矩阵</strong>：用m*n的张量表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>转制</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure>

<p>对于对称矩阵，满足：</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727153227299.png" alt="image-20240727153227299"></p>
<p><strong>张量</strong>：[<strong>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</strong>]。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727153540597.png" alt="image-20240727153540597"></p>
<h3 id="张量算法"><a href="#张量算法" class="headerlink" title="张量算法"></a>张量算法</h3><p>两个矩阵按元素相乘*<strong>Hadamard积*</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure>

<h4 id="降维求和"><a href="#降维求和" class="headerlink" title="降维求和"></a>降维求和</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以[<strong>指定张量沿哪一个轴来通过求和降低维度</strong>]。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，<strong>因此输入轴0的维数在输出形状中消失</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 沿着第0轴进行求和</span></span><br><span class="line"><span class="comment"># A是一个5*4的矩阵</span></span><br><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line">(tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), torch.Size([<span class="number">4</span>]))</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727155109468.png" alt="image-20240727155109468"></p>
<p>也可以沿着两个维度求和</p>
<p>例如下列代码对X沿着0轴和1轴求和</p>
<p>可以理解为先让两个3*4矩阵相加，得到一个3 * 4矩阵</p>
<p>然后将每一个长度为4的向量相加，得到tensor([60, 66, 72, 78])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">X_sum_axis01 = X.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">X_sum_axis01</span><br><span class="line"></span><br><span class="line">(tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">          [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">          [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">          [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">          [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]]),</span><br><span class="line">tensor([<span class="number">60</span>, <span class="number">66</span>, <span class="number">72</span>, <span class="number">78</span>])</span><br></pre></td></tr></table></figure>

<h4 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">A_sum_axis1,sum_A</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 降维求和结果</span></span><br><span class="line">(tensor([ <span class="number">6.</span>, <span class="number">22.</span>, <span class="number">38.</span>, <span class="number">54.</span>, <span class="number">70.</span>]),</span><br><span class="line"> <span class="comment"># 非降维求和结果，5*1，还是两个维度</span></span><br><span class="line"> tensor([[ <span class="number">6.</span>],</span><br><span class="line">         [<span class="number">22.</span>],</span><br><span class="line">         [<span class="number">38.</span>],</span><br><span class="line">         [<span class="number">54.</span>],</span><br><span class="line">         [<span class="number">70.</span>]]))</span><br></pre></td></tr></table></figure>



<p>如果我们想沿[<strong>某个轴计算<code>A</code>元素的累积总和</strong>]， 比如<code>axis=0</code>（按行计算），可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A,A.cumsum(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>],</span><br><span class="line">         [<span class="number">24.</span>, <span class="number">28.</span>, <span class="number">32.</span>, <span class="number">36.</span>],</span><br><span class="line">         [<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]]))</span><br></pre></td></tr></table></figure>





<h4 id="平均值"><a href="#平均值" class="headerlink" title="平均值"></a>平均值</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean() , A.<span class="built_in">sum</span>() / A.numel()</span><br></pre></td></tr></table></figure>

<p>同样，计算平均值的函数也可以沿指定轴降低张量的维度</p>
<p>例如，求A沿着0轴方向的平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<h4 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.dot(x, y)</span><br></pre></td></tr></table></figure>

<p><strong>我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure>



<h4 id="矩阵-向量积"><a href="#矩阵-向量积" class="headerlink" title="矩阵-向量积"></a>矩阵-向量积</h4><p>例如A是一个m*n的矩阵，x是一个长度为n的向量</p>
<p>则A与x的积为一个长度为m的列向量，其中第i个元素为矩阵中第i行向量与x的点积</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727162328237.png" alt="image-20240727162328237"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A, x)</span><br><span class="line"></span><br><span class="line">(torch.Size([<span class="number">5</span>, <span class="number">4</span>]), torch.Size([<span class="number">4</span>]), tensor([ <span class="number">14.</span>,  <span class="number">38.</span>,  <span class="number">62.</span>,  <span class="number">86.</span>, <span class="number">110.</span>]))</span><br></pre></td></tr></table></figure>



<h4 id="矩阵-矩阵乘法"><a href="#矩阵-矩阵乘法" class="headerlink" title="矩阵-矩阵乘法"></a>矩阵-矩阵乘法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.mm(A, B)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.matual(A,B)</span><br></pre></td></tr></table></figure>

<p><code>torch.mm</code> 的全称是 “matrix multiply”，它遵循传统的矩阵乘法规则。</p>
<p><code>torch.matmul</code> 是一个更为通用的函数，它可以处理多种维度的张量乘法，不仅限于二维矩阵。<code>torch.matmul</code> 可以用于矩阵和矩阵之间的乘法，也可以用于矩阵和向量之间的乘法，或者更高维度的张量乘法。它的工作方式取决于输入张量的形状和维度。</p>
<h4 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h4><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240730120311633.png" alt="image-20240730120311633"></p>
<p>L2范数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure>

<p>类似于向量的L2范数，Frobenius范数（Frobenius norm）是矩阵元素平方和的平方根，也可以用torch.norm计算</p>
<p>L1范数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>在深度学习中，我们经常试图解决优化问题： <em>最大化</em>分配给观测数据的概率; <em>最小化</em>预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</p>
<h2 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算*"></a>矩阵计算*</h2><p>主要是讲矩阵如何求导数</p>
<p>基础知识：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/263777564">矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)</a></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731104153413.png" alt="image-20240731104153413"></p>
<p>向量关于向量求导的结果是一个矩阵</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731104815010.png" alt="image-20240731104815010"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731105049555.png" alt="image-20240731105049555"></p>
<p>右下角括号内的数字是：</p>
<p>(m,l,k,n)</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731110635856.png" alt="image-20240731110635856"></p>
<h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导*"></a>自动求导*</h2><p>原理：</p>
<p>构造计算图</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731112300974.png" alt="image-20240731112300974"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731112539492.png" alt="image-20240731112539492"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731112917005.png" alt="image-20240731112917005"></p>
<p>反向累积：</p>
<p>时间复杂度：O(n) 通常正向和反向的代价类似</p>
<p>空间复杂度O(n),需要存储正向的所有中间结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># requires_grad=True表示需要存储梯度的数值</span></span><br><span class="line"><span class="comment"># x.grad就可以访问这个存储的梯度</span></span><br><span class="line">x=torch.arange(<span class="number">4.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">12.</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_() <span class="comment"># 清空梯度</span></span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>



<p>向量对向量求导，结果应该是一个矩阵，但是</p>
<p><strong>深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class="line"><span class="comment"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x </span><br><span class="line"><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>



<h3 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h3><p>有时，我们希望[<strong>将某些计算移动到记录的计算图之外</strong>]。 </p>
<p>假如，y是作为x的函数被计算的，z则作为y和x的函数，但是，我们希望将y视为一个常数，不希望对z求关于x 的导数时，梯度经y流到x</p>
<p>解决方案是分离y，返回一个新变量u，u丢弃计算图中如何计算y的任何信息</p>
<p>在下列代码中，求的是Z&#x3D;u * x的偏导数，而不是Z&#x3D;x* * x * x的偏导数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u</span><br><span class="line"></span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<h3 id="Python控制流的梯度计算"><a href="#Python控制流的梯度计算" class="headerlink" title="Python控制流的梯度计算"></a>Python控制流的梯度计算</h3><p>使用自动微分的一个好处是： [<strong>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</strong>]。 在下面的代码中，<code>while</code>循环的迭代次数和<code>if</code>语句的结果都取决于输入<code>a</code>的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># randn用于创建服从标准正态分布的张量</span></span><br><span class="line"><span class="comment"># size=()表示创建的是标量</span></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>

<p><code>f</code>函数在其输入<code>a</code>中是分段线性的。 换言之，对于任何<code>a</code>，存在某个常量标量<code>k</code>，使得<code>f(a)=k*a</code>，其中<code>k</code>的值取决于输入<code>a</code>，因此可以用<code>d/a</code>验证梯度是否正确。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.grad == d / a</span><br><span class="line"></span><br><span class="line">tensor(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<h1 id="ch02-线性神经网络"><a href="#ch02-线性神经网络" class="headerlink" title="ch02_线性神经网络"></a>ch02_线性神经网络</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归就是对于具有线性关系的自变量和因变量，找到最好的参数来满足这个线性关系</p>
<p>因此，我们需要：一种度量方式——损失</p>
<p>​				一种能够更新模型以提高模型预测质量的方法。——随机梯度下降</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801122008005.png" alt="image-20240801122008005"></p>
<h4 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h4><p>为了解释<em>线性回归</em>，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的<strong>数据集</strong>。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为<em>训练数据集</em>（<strong>training data set</strong>） 或<em>训练集</em>（<strong>training set</strong>）。 每行数据（比如一次房屋交易相对应的数据）称为<em>样本</em>（<strong>sample</strong>）， 也可以称为<em>数据点</em>（<strong>data point</strong>）或<em>数据样本</em>（<strong>data instance</strong>）。 我们把试图预测的目标（比如预测房屋价格）称为<em>标签</em>（<strong>label</strong>）或<em>目标</em>（<strong>target</strong>）。 预测所依据的自变量（面积和房龄）称为<em>特征</em>（<strong>feature</strong>）或<em>协变量</em>（<strong>covariate</strong>）。<br>$$<br>对索引为i的样本，其输入表示为\mathbf{x}^{(i)} &#x3D; [x_1^{(i)}, x_2^{(i)}]^\top，<br>其对应的标签是y^{(i)}。<br>$$</p>
<h4 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h4><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801115554997.png" alt="image-20240801115554997"></p>
<p>model parameters 模型参数就是这里的w和b，训练的最终目标就是找到最好的w和b</p>
<p>线性模型可以视为单层神经网络（输出层不当成一层）</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801115659649.png" alt="image-20240801115659649"></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>均方损失函数<br>$$<br>\hat{y}^{(i)}代表估计值<br>$$</p>
<p>$$<br>l^{(i)}(\mathbf{w}, b) &#x3D; \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2<br>$$</p>
<p>下图中的L就是损失函数，它是平方误差的平均值</p>
<p>我们<strong>希望找到w和b，使得损失最小化</strong></p>
<img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801121145738.png" alt="image-20240801121145738" style="zoom:150%;">



<h4 id="随机梯度下降SGD"><a href="#随机梯度下降SGD" class="headerlink" title="随机梯度下降SGD"></a>随机梯度下降SGD</h4><p>Stochastic Gradient Descent</p>
<p>这种方法几乎可以优化所有深度学习模型它通过<strong>不断地在损失函数递减的方向上更新参数</strong>来降低误差。</p>
<p>直观理解：</p>
<p>选定一个随机起始点W0，找到函数在这一点的梯度，然后沿着负梯度方向找到下一个点W1，依次类推直到损失最小</p>
<p>每次移动的向量是固定的，这个长度就是学习率</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801122802801.png" alt="image-20240801122631556"></p>
<p>由于在真个训练集上计算梯度的成本太高</p>
<p>我们可以随机采样b个样本来计算近似损失，这就是<strong>小批量随机梯度下降</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806081113409.png" alt="image-20240806081113409"></p>
<h3 id="线性回归从0开始实现"><a href="#线性回归从0开始实现" class="headerlink" title="线性回归从0开始实现"></a>线性回归从0开始实现</h3><h4 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a><strong>生成数据集</strong></h4><p>这里是借助正态分布模拟真实数据<br>$$<br>我们使用线性模型参数\mathbf{w} &#x3D; [2, -3.4]^\top、b &#x3D; 4.2<br>和噪声项\epsilon生成数据集及其标签：<br>$$</p>
<p>$$<br>\mathbf{y}&#x3D; \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.<br>$$</p>
<p> #@save标签表明这个函数是dzl包的内置函数，也就是说，在实际的应用中，我们通常不需要自己实现这个函数，这里写出来是展示一下原理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w))) <span class="comment">#行向量</span></span><br><span class="line">    y = torch.matmul(X, w) + b	</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape) <span class="comment">#再加上e，表示微小的误差</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 放回特征X 和标签y</span></span><br><span class="line"><span class="comment"># reshape((-1, 1))表示重塑为列向量，其中 -1 是一个特殊的参数，表示该维度的大小由其他维度的大小和张量的总元素数量决定。</span></span><br></pre></td></tr></table></figure>



<p><strong><code>features</code>中的每一行都包含一个二维数据样本， <code>labels</code>中的每一行都包含一维标签值（一个标量）</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806085449212.png" alt="image-20240806085449212"></p>
<h4 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h4><p>训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。</p>
<p><strong>定义一个<code>data_iter</code>函数</strong>，生成大小为<code>batch_size</code>的小批量**]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples)) <span class="comment">#生成样本索引index</span></span><br><span class="line">    random.shuffle(indices) <span class="comment">#打乱索引顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line">        <span class="comment"># 根据index取值</span></span><br><span class="line"><span class="comment"># 注意到yield是写在for循环内部的  </span></span><br></pre></td></tr></table></figure>

<p><code>yield</code> 返回这两个张量子集组成的元组，但不会像普通函数返回值那样退出函数。相反，它会暂停函数的执行，并保存当前状态</p>
<p>当生成器在下一次被请求值时，它会从上次离开的地方恢复执行</p>
<p>当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。 上面实现的迭代对教学来说很好，<strong>但它的执行效率很低，可能会在实际问题上陷入麻烦</strong>。 例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。 在深度学习框架中实现的内置迭代器效率要高得多， 它可以处理存储在文件中的数据和数据流提供的数据。</p>
<h4 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><p>我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。 因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。</p>
<p>我们使用自动微分来计算梯度。</p>
<h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br></pre></td></tr></table></figure>

<h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save y_hat代表预测值</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>

<h4 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h4><p>params是参数集（包含w和b）</p>
<p>lr是学习率</p>
<p>batch_size 是批量的大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            <span class="comment"># 对每一个参数进行迭代</span></span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            <span class="comment"># 清空梯度</span></span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>使用 <code>with torch.no_grad():</code> 时，告诉 PyTorch 在这个代码块中不需要跟踪梯度，因此可以减少内存消耗并加速计算</p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p> 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法<code>sgd</code>来更新模型参数。</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806094030348.png" alt="image-20240806094030348"></p>
<p>在每个<em>迭代周期</em>（epoch）中，我们使用<code>data_iter</code>函数遍历整个数据集， 并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。 这里的迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是<strong>超参数</strong>，分别设为3和0.03。 <strong>设置超参数很棘手</strong>，需<strong>要通过反复试验进行调整</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lr = 0.03</span><br><span class="line">num_epochs = 3</span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># 小批量损失</span></span><br><span class="line">        <span class="comment">#l中的所有元素被加到一起，并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 经过一个周期的优化后的损失</span></span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.028825</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.000090</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.000047</span></span><br></pre></td></tr></table></figure>

<p>因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。 因此，我们可以通过[<strong>比较真实参数和通过训练学到的参数来评估训练的成功程度</strong>]。 事实上，真实参数和通过训练学到的参数确实非常接近。</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806095801126.png" alt="image-20240806095801126"></p>
<h3 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h3><p>在过去的几年里，出于对深度学习强烈的兴趣， 许多公司、学者和业余爱好者开发了各种成熟的开源框架。 这些框架可以自动化基于梯度的学习算法中重复性的工作。 </p>
<p>我们只运用了： （1）通过张量来进行数据存储和线性代数； （2）通过自动微分来计算梯度。 实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。</p>
<p>对于标准深度学习模型，我们可以[<strong>使用框架的预定义好的层</strong>]。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量<code>net</code>，它是一个<code>Sequential</code>类的实例。 <code>Sequential</code>类将多个层串联在一起。 当给定输入数据时，<code>Sequential</code>实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要<code>Sequential</code>。 但是由于以后几乎所有的模型都是多层的，在这里使用<code>Sequential</code>会让你熟悉“标准的流水线”。</p>
<p>nn.Linear(2,1)的两个参数规定了输入&#x2F;输出张量的wei’du</p>
<p>输入张量（特征），形状为 <code>(batch_size, 2)</code> </p>
<p>输出张量（预测值&#x2F;标签），形状为 <code>(batch_size, 1)</code> </p>
<p>而参数（w&amp;b）则储存在该层神经网络的内部</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span>  torch.utils <span class="keyword">import</span>  data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">true_w= torch.tensor([<span class="number">2</span>,-<span class="number">3.4</span>])</span><br><span class="line">true_b= <span class="number">4.2</span></span><br><span class="line">featrues,labels = d2l.synthetic_data(true_w,true_b,<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = d2l.load_array((featrues,labels),batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn是神经网络的缩写</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 权重参数w</span></span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)           <span class="comment"># 偏置参数b</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment"># 定义优化算法</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.03</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X),y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step() <span class="comment"># 更新参数</span></span><br><span class="line">    l = loss(net(featrues),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.000330</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.000103</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.000103</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.grad)</span><br></pre></td></tr></table></figure>





<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>回归和分类的区别</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814105001744.png" alt="image-20240814105001744"></p>
<p><strong>softmax运算获取一个向量并将它映射为概率</strong></p>
<p>假如我们有4个特征和3个可能的输出类别，我们为每个输入计算三个<em>未规范化的预测</em>（logit）：<br>$$<br>\begin{aligned}<br>o_1 &amp;&#x3D; x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\<br>o_2 &amp;&#x3D; x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\<br>o_3 &amp;&#x3D; x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.<br>\end{aligned}<br>$$<br>为了更简洁地表达模型，我们仍然使用线性代数符号。 通过向量形式表达为o&#x3D; Wx + b</p>
<p>而我们希望模型的输出可以代表对应类的概率，那么我们就需要softmax函数：</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814111909419.png" alt="image-20240814111909419"></p>
<p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个<em>线性模型</em>（linear model）</p>
<p><strong>损失函数：</strong></p>
<p><strong>交叉熵损失</strong>常用于衡量两个概率的区别<br>$$<br>l(\mathbf{y}, \hat{\mathbf{y}}) &#x3D; - \sum_{j&#x3D;1}^q y_j \log \hat{y}_j.<br>$$<br>结合softmax函数，可以得到</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814120039988.png" alt="image-20240814120039988"></p>
<p>即损失函数的导数是真实概率和预测概率的区别，这不是巧合，在任何指数族分布模型中 ， 对数似然的梯度正是由此得出的。 这使梯度计算在实践中变得容易很多。</p>
<h3 id="图像分类数据集"><a href="#图像分类数据集" class="headerlink" title="图像分类数据集"></a>图像分类数据集</h3><p>@save标记表明这个函数在d2l包中以及储存，这里只是展示具体实现</p>
<p><strong>MNIST</strong>数据集是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的<strong>Fashion-MNIST</strong>数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line">d2l.use_svg_display()  <span class="comment"># 使用svg格式显示图片</span></span><br></pre></td></tr></table></figure>

<p>Fashion-MNIST由10个类别的图像组成， 每个类别由<em>训练数据集</em>（train dataset）中的6000张图像 和<em>测试数据集</em>（test dataset）中的1000张图像组成。 因此，训练集和测试集分别包含60000和10000张图像。 测试数据集不会用于训练，只用于评估模型性能&#x2F;。</p>
<p>在上一级目录中的data文件夹中下载Fashion-MNIST</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)s</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>每个输入图像的高度和宽度均为28像素。 数据集由灰度图像组成，其通道数为1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br><span class="line"><span class="comment"># torch.Size([1, 28, 28])</span></span><br></pre></td></tr></table></figure>

<p>Fashion-MNIST中包含的10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。</p>
<p>这个函数的输入是一个列表，输出也是一个列表</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"><span class="comment"># eg：</span></span><br><span class="line"><span class="comment"># labels = [7, 2, 1, 5, 9]</span></span><br><span class="line"><span class="comment"># [&#x27;sneaker&#x27;, &#x27;trouser&#x27;, &#x27;t-shirt&#x27;, &#x27;sandal&#x27;, &#x27;ankle boot&#x27;]</span></span><br></pre></td></tr></table></figure>

<p>用于绘制图像的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    <span class="comment">#  _ 表示我们不使用 plt 返回的第一个对象（通常是图形对象）。</span></span><br><span class="line">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure>

<p>读取小批量数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;同时使用4个进程来读取数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle=True表示每个epoch开始时，数据将被打乱</span></span><br><span class="line">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;4.79 sec &#x27;</span> <span class="comment"># 表示读取一遍数据需要4.79秒</span></span><br></pre></td></tr></table></figure>



<h4 id="整合组件"><a href="#整合组件" class="headerlink" title="整合组件"></a>整合组件</h4><p>load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。</p>
<p>这个函数返回训练集和验证集的数据迭代器。<br>此外，这个函数还接受一个可选参数<code>resize</code>，用来将图像大小调整为另一种形状。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure>

<p>读取一批训练集中的数据，X中的数据是32张1通道64X64像素的图片，而y中则是这张图片的标签，为32个0-9的数字，代表10个类别</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64</span></span><br></pre></td></tr></table></figure>



<h3 id="softmax回归从0开始实现"><a href="#softmax回归从0开始实现" class="headerlink" title="softmax回归从0开始实现"></a>softmax回归从0开始实现</h3><p>补充知识</p>
<p><strong>用序列和y同时索引y_hat：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>,<span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]</span><br></pre></td></tr></table></figure>

<p>结果：tensor（[0.1000，0.5000]）</p>
<p>这个操作的意思是根据对应组别和分类标号获得对应的预测值，y_hat[[0, 1], y]代表获得第0组的第0类预测值和第1组的第2类预测值</p>
<p><strong>评估模式和训练模式</strong>：</p>
<p>在 PyTorch 中，模型有两种基本模式：训练模式（training mode）和评估模式（evaluation mode）</p>
<p>在 PyTorch 中，可以通过以下方式在模型的两种模式之间切换：</p>
<ul>
<li><code>net.train()</code>：将模型设置为训练模式。</li>
<li><code>net.eval()</code>：将模型设置为评估模式。</li>
</ul>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814173404091.png" alt="image-20240814173404091"></p>
<p><strong>具体实现：</strong></p>
<p>注意：以下代码针对jupyter，与pycharm不完全兼容，因此可能出现动图不出现等情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">            self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line"></span><br><span class="line">        d2l.plt.draw()</span><br><span class="line">        d2l.plt.pause(<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815082838148.png" alt="image-20240815082838148"></p>
<p><strong>预测：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815082853861.png" alt="image-20240815082853861"></p>
<h3 id="softmax回归的简洁实现"><a href="#softmax回归的简洁实现" class="headerlink" title="softmax回归的简洁实现"></a>softmax回归的简洁实现</h3><p>出现多线程问题的解决方案:</p>
<p><code>if __name__==&#39;__main__</code>的作用</p>
<blockquote>
<p>当.py文件被<strong>直接运行</strong>时，<code>if __name__ == &#39;__main__&#39;</code>之下的代码块将被运行；<br>当.py文件<strong>以模块形式被导入</strong>时，<code>if __name__ == &#39;__main__&#39;</code>之下的代码块不被运行。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013700358/article/details/82753019">pytorch使用出现”RuntimeError: An attempt has been made to start a new process before the…” 解决方法-CSDN博客</a>\</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">banch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(banch_size)</span><br><span class="line"><span class="comment">#print(len(train_iter))</span></span><br><span class="line"><span class="comment"># train_iter的长度是235，即234*256 加上一组不超过256的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义softmax回归模型</span></span><br><span class="line"><span class="comment"># Flatten展平层，用于调整输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="comment"># m-&gt;module 代表模型中的层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weight</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对net中的所有线性层进行权重初始化</span></span><br><span class="line">net.apply(init_weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化算法</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以下是d2l包缺少的train_ch3函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在n个变量上累加</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n       <span class="comment"># 创建一个长度为 n 的列表，初始化所有元素为0.0。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):           <span class="comment"># 累加</span></span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):                <span class="comment"># 重置累加器的状态，将所有元素重置为0.0</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):     <span class="comment"># 获取所有数据</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算正确的数量</span></span><br><span class="line"><span class="string">    :param y_hat:</span></span><br><span class="line"><span class="string">    :param y:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)            <span class="comment"># 在每行中找到最大值的索引，以确定每个样本的预测类别</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算指定数据集的精度</span></span><br><span class="line"><span class="string">    :param net:</span></span><br><span class="line"><span class="string">    :param data_iter:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()                  <span class="comment"># 通常会关闭一些在训练时启用的行为</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在动画中绘制数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量的绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">            self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span><br><span class="line">        )</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        向图表中添加多个数据点</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :param y:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型一轮</span></span><br><span class="line"><span class="string">    :param net:是要训练的神经网络模型</span></span><br><span class="line"><span class="string">    :param train_iter:是训练数据的数据迭代器，用于遍历训练数据集</span></span><br><span class="line"><span class="string">    :param loss:是用于计算损失的损失函数</span></span><br><span class="line"><span class="string">    :param updater:是用于更新模型参数的优化器</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):  <span class="comment"># 用于检查一个对象是否属于指定的类（或类的子类）或数据类型。</span></span><br><span class="line">        net.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练损失总和， 训练准确总和， 样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:  <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):  <span class="comment"># 用于检查一个对象是否属于指定的类（或类的子类）或数据类型。</span></span><br><span class="line">            <span class="comment"># 使用pytorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()  <span class="comment"># 方法用于计算损失的平均值</span></span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制（自定义）的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape())</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型（）</span></span><br><span class="line"><span class="string">    :param net:</span></span><br><span class="line"><span class="string">    :param train_iter:</span></span><br><span class="line"><span class="string">    :param test_iter:</span></span><br><span class="line"><span class="string">    :param loss:</span></span><br><span class="line"><span class="string">    :param num_epochs:</span></span><br><span class="line"><span class="string">    :param updater:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        trans_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, trans_metrics + (test_acc,))</span><br><span class="line">        train_loss, train_acc = trans_metrics</span><br><span class="line">        <span class="built_in">print</span>(trans_metrics)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    进行预测</span></span><br><span class="line"><span class="string">    :param net:</span></span><br><span class="line"><span class="string">    :param test_iter:</span></span><br><span class="line"><span class="string">    :param n:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">global</span> X, y</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true + <span class="string">&quot;\n&quot;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n]</span><br><span class="line">    )</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>



<h1 id="ch03-多层感知机MLP"><a href="#ch03-多层感知机MLP" class="headerlink" title="ch03_多层感知机MLP"></a>ch03_多层感知机MLP</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>出现于上世纪60年代的算法</p>
<p>给定输入x，权重W，偏移b，感知机输出为 1&#x2F;0或者1&#x2F;-1，本质是一个二分类</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815205946737.png" alt="image-20240815205946737"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815205937769.png" alt="image-20240815205937769"></p>
<p>训练感知机</p>
<p>原理：如果y与&lt;W,x&gt;+b的积为负，说明y与后者异号，说明以W和b为参数的预测错误，则更新W和b</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815210402776.png" alt="image-20240815210402776"></p>
<p>这个损失函数的意思是，当y*… &lt;&#x3D; 0 的时候，才会有梯度，才需要更新</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815210616431.png" alt="image-20240815210616431"></p>
<p><strong>感知机的问题：</strong>只能产生线性分割面，无法拟合XOR函数</p>
<p>例如下图中，红色小球为一类，绿色小球为1类，无法通过感知器去切分</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815211427820.png" alt="image-20240815211427820"></p>
<p>思考如何解决XOR问题</p>
<p>同样四个小球，只要我们进行两次划分，最后结合两次的结果，就可以得到</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815212101754.png" alt="image-20240815212101754"></p>
<p>这就展现了多层感知机的基本思想</p>
<h3 id="单隐藏层"><a href="#单隐藏层" class="headerlink" title="单隐藏层"></a><strong>单隐藏层</strong></h3><h4 id="单类分类"><a href="#单类分类" class="headerlink" title="单类分类"></a><strong>单类分类</strong></h4><p>输入层：n维向量</p>
<p>隐藏层： W1是m x n矩阵，b是m维向量</p>
<p>输出层  W2是m维矩阵，b是标量</p>
<p>公式如下：式1中的西格玛函数为元素的激活函数、每个隐藏层都有自己的激活函数</p>
<p>W2转置的原因是其为列向量，而h也是一个长度为m的列向量</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815213229995.png" alt="image-20240815213229995"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815221307798.png" alt="image-20240815221307798"></p>
<p>激活函数必须为<strong>非线性</strong>的，分析：</p>
<p>如果激活函数为线性的，那么隐藏层和输出层的操作相当于对特征做线性变化，这样和用一个简单的线性层去完成没区别、</p>
<h4 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h4><p>softmax函数的主要作用是对数据进行规范化</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815220502134.png" alt="image-20240815220502134"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815210905106.png" alt="image-20240815210905106"></p>
<p>在上图中，从输入层到隐藏层的箭头错综复杂，这是因为任意一个特征值对向量h中每一个值都有影响</p>
<p>而上图隐藏层小球的个数，其实就代表隐藏层的大小，隐藏层将给定数目的特征映射到<strong>人为设定大小</strong>（<strong>超参数</strong>）的向量上</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>在感知机一章中，用到的这个二分类函数，其实也算一个激活函数，但是这个函数的变化太硬了<br>$$<br>f(x) &#x3D;<br>\begin{cases}<br>  1 &amp; \text{if } x &gt; 0, \<br>  \text{0} &amp; \text{otherwise}.<br>\end{cases}<br>$$</p>
<p>sigmoid函数：将输入投影到(0,1)<br>$$<br>\text{sigmoid}(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$<br>Tanh函数：讲述人投影到（-1，1）<br>$$<br>\tanh(x) &#x3D; \frac{1 - e^{-2x}}{1 + e^{-2x}}<br>$$<br>ReLU函数:<br>$$<br>\text{ReLU}(x) &#x3D; \max(x, 0)<br>$$</p>
<h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><p>就是将每一个隐藏层的输出（向量h）作为下一个隐藏层的输入</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815221948725.png" alt="image-20240815221948725"></p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="单隐藏层多分类"><a href="#单隐藏层多分类" class="headerlink" title="单隐藏层多分类"></a>单隐藏层多分类</h4><p>注意训练函数还是用的train_ch3，再次回顾训练过程：遍历训练集数据，用net模型计算后，利用交叉熵损失函数计算损失（该函数包含<strong>softmax</strong>函数，会对net计算的结果进行规范），接着利用损失计算梯度，更新参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">banch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(banch_size)</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">num_hiddens = <span class="number">256</span> <span class="comment"># 隐藏层大小 单隐藏层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐藏层参数设置</span></span><br><span class="line">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 输出层参数</span></span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X @ W1 + b1)  <span class="comment"># 这里的@相当于 torch.matmul</span></span><br><span class="line">    <span class="keyword">return</span> (H @ W2 + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到，相比于只使用线性模型，加上隐藏层后，损失率明显下降，准确率略微提升</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817104034302.png" alt="image-20240817104034302"></p>
<h4 id="多隐藏层多分类"><a href="#多隐藏层多分类" class="headerlink" title="多隐藏层多分类"></a>多隐藏层多分类</h4><p>增加层数，记得上一个隐藏层的输出是下一个隐藏层的输入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">128</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">64</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>实验结果发现，在同样的10次循环的情况下，使用三个隐藏层的效果不如单隐藏层</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817111452520.png" alt="image-20240817111452520"></p>
<p>可以看到训练误差的表现很好，但是泛化误差的表现并不好，这可能是发生了<strong>过拟合</strong></p>
<h4 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h4><p>主要区别是net的定义直接用神经网络实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只有linear层有参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure>

<p>Flatten展平层：将输入的多维数据展平为一维数据</p>
<p>Linear全连接层：将输入的784个特征映射到256个特征</p>
<p>ReLU：激活函数</p>
<p>Linear全连接层：将256个特征映射到10个特征</p>
<h2 id="模型选择-过拟合与欠拟合"><a href="#模型选择-过拟合与欠拟合" class="headerlink" title="模型选择-过拟合与欠拟合"></a>模型选择-过拟合与欠拟合</h2><p><strong>误差</strong></p>
<p>训练误差：模型在训练数据上的误差 【模拟考】</p>
<p>泛化误差：模型在新数据上的误差  	【高考】【最关心】</p>
<p><strong>数据集</strong></p>
<p>验证数据集：评估模型好坏的数据集</p>
<p>测试数据集：只用一次的数据集，相当于结果，不能用这个数据集去调参</p>
<p>数据集不够的时候的<strong>验证方式</strong> </p>
<p>数据集分成k快，跑k次</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817113014971.png" alt="image-20240817113014971"></p>
<p><strong>过拟合和欠拟合</strong></p>
<p>过拟合：相当于把每一个样本都记住了，过拟合发生在模型在训练数据上表现得很好，但在新的、未见过的数据上表现很差的情况下。</p>
<p>欠拟合：模型太简单，特征太多，拟合效果不好</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817114250534.png" alt="image-20240817114250534"></p>
<p><strong>模型容量</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817114846859.png" alt="image-20240817114846859"></p>
<p><strong>模型容量的影响</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817114940284.png" alt="image-20240817114940284"></p>
<p>根本上，我们需要的还是将泛化误差往下降</p>
<p><strong>估计模型容量</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817115702547.png" alt="image-20240817115702547"></p>
<p><strong>VC维</strong></p>
<p>对于一个分类模型，VC等于一个最大的数据集的大小。</p>
<p>例如: 2维输入的感知机，VC&#x3D;3 （最多分类3个点）</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817120211849.png" alt="image-20240817120211849"></p>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817120433723.png" alt="image-20240817120433723"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line">max_degree = <span class="number">20</span> <span class="comment"># 从0开始，最高到19次幂</span></span><br><span class="line">n_train,n_test = <span class="number">100</span>,<span class="number">100</span> <span class="comment"># 数据集大小</span></span><br><span class="line">true_w = np.zeros(max_degree) <span class="comment"># 分配空间</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>]= np.array([<span class="number">5</span>,<span class="number">1.2</span>,-<span class="number">3.4</span>,<span class="number">5.6</span>]) <span class="comment"># 这里最高只需要到三阶</span></span><br><span class="line"></span><br><span class="line">features = np.random.normal(size = (n_train+n_test,<span class="number">1</span>))</span><br><span class="line">np.random.shuffle(features) <span class="comment"># 打乱数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算各个特征值的0到max_degree-1次幂，相当于features的行数不变，列数增加</span></span><br><span class="line">poly_features = np.power(features,np.arange(max_degree).reshape(<span class="number">1</span>,-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (max_degree):</span><br><span class="line">    <span class="comment">#  math.gamma(i+1) ： i的阶乘</span></span><br><span class="line">    poly_features[:,i]  /= math.gamma(i+<span class="number">1</span>)  <span class="comment"># x的n次方除以n的阶乘</span></span><br><span class="line"></span><br><span class="line">labels = np.dot(poly_features,true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>,size=labels.shape) <span class="comment"># 加上一点噪声</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy ndarray转换为tensor</span></span><br><span class="line">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class="line">    torch.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估损失</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features,test_features,train_lables,test_lables,</span></span><br><span class="line"><span class="params">          num_epochs = <span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>) <span class="comment"># 均方误差损失函数</span></span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 不设置偏置</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape,<span class="number">1</span>,bias=<span class="literal">False</span>))</span><br><span class="line">    banch_size = <span class="built_in">min</span>(<span class="number">10</span>,train_lables.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features,train_lables.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                                banch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features,test_lables.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                              banch_size,is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.01</span>) <span class="comment"># 优化函数</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每20轮看一次结果</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss <span class="subst">&#123;evaluate_loss(net,test_iter,loss):f&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure>

<p><strong>训练：</strong></p>
<p><strong>1、使用三阶多项式去拟合</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>

<p>最终结果：</p>
<p>epoch 400,loss 0.009408<br>weight: [[ 4.9953513  1.1967007 -3.3998032  5.5985456]]</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817130615698.png" alt="image-20240817130615698"></p>
<p><strong>2、使用线性函数去拟合（欠拟合）</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>

<p>epoch 400,loss 7.965484<br>weight: [[3.5459476 3.5083208]]</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817130607110.png" alt="image-20240817130607110"></p>
<p><strong>3、使用19阶函数去拟合（过拟合）</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在1500轮过后，拟合结果似乎还不错</p>
<p>epoch 1500,loss 0.011771<br>weight: [[ 4.9755540e+00  1.2619114e+00 -3.3151577e+00  5.2241902e+00<br>  -3.6112809e-01  1.1908938e+00  3.0346635e-01  2.6848632e-01<br>   1.5733847e-01 -7.8884050e-02  4.8131108e-02  2.5263547e-03<br>   2.0420229e-01 -2.0368829e-01  6.8688519e-02 -1.2085145e-01<br>   7.9493999e-02 -5.8916792e-02  7.3894918e-02  9.7813845e-02]]</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817130548690.png" alt="image-20240817130548690"></p>
<p>改变了一下true_w,观察到了<strong>过拟合现象</strong></p>
<p>true_w[0:4] &#x3D; np.array([77, 22.4, -221, 22.5])</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817131309496.png" alt="image-20240817131309496"></p>
<h2 id="权重衰退"><a href="#权重衰退" class="headerlink" title="权重衰退"></a>权重衰退</h2><p>weight decay 通常也被称为：L2正则化</p>
<p>最常见的处理过拟合的方法</p>
<ul>
<li>通过限制参数值的选择范围</li>
<li>通常不限制偏移b</li>
</ul>
<p>使用均方范数作为硬性限制,subject to 表示受限于<br>$$<br>\begin{align*}<br>\min_  \quad l(\mathbf{w}, b)<br>\text{subject to}  |\mathbf{w}|^2\leq 0<br>\end{align*}<br>$$<br>使用均方范数作为柔软性限制,lambda作为超参数，控制了这个正则项的重要程度<br>$$<br>\begin{align*}<br>\min_  \quad l(\mathbf{w}, b) + \frac{\lambda}{2} |\mathbf{w}|^2<br>\end{align*}<br>$$<br><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817194335164.png" alt="image-20240817194335164"></p>
<p><strong>原理</strong>：<br>过拟合的本质是因为有的参数其实我们并不需要&#x2F;有的参数数值过分偏离，应该让这些参数趋近于0，而损失函数中新的一项的加入，就起到这个作用</p>
<p>参数更新法则：</p>
<p>其实就相当于loss函数由两项构成，区别就是求梯度的时候会多减去一项</p>
<p>在每次更新参数的时候，会先将权重放小，这就是<strong>权重衰退</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817195728302.png" alt="image-20240817195728302"></p>
<h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><p>生成数据集用到的公式</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817200749539.png" alt="image-20240817200749539"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span>  torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line"><span class="comment"># 这里的数据集很小/简单 但是特征值却有200个，所以很容易发生过拟合</span></span><br><span class="line">n_train,n_test,num_inputs,batch_size = <span class="number">20</span>,<span class="number">100</span>,<span class="number">200</span>,<span class="number">5</span></span><br><span class="line">true_w,true_b = torch.ones(num_inputs,<span class="number">1</span>)*<span class="number">0.01</span>,<span class="number">0.05</span> <span class="comment"># 权重是列向量</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w,true_b,n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data,batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w,true_b,n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data,batch_size,is_train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(num_inputs,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w,b]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义L2范式惩罚，这里话没有引入lamda</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L2_penalty</span>(<span class="params">W</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(W.<span class="built_in">pow</span>(<span class="number">2</span>))/<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w,b = init_params()</span><br><span class="line">    net = <span class="keyword">lambda</span> X:d2l.linreg(X,w,b) <span class="comment"># 之前定义的线性回归 lambda是匿名函数关键字 X是输入</span></span><br><span class="line">    loss = d2l.squared_loss</span><br><span class="line">    num_epoch,lr =<span class="number">100</span>,<span class="number">0.003</span></span><br><span class="line">    trainer = torch.optim.SGD([w,b],lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            <span class="comment"># 损失函数加上L2范式惩罚</span></span><br><span class="line">            l = loss(net(X),y) + lambd*L2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span>(epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>,train_loss<span class="subst">&#123;d2l.evaluate_loss(net,train_iter,loss):f&#125;</span>&#x27;</span></span><br><span class="line">                  <span class="string">f&#x27;test_loss<span class="subst">&#123;d2l.evaluate_loss(net,test_iter,loss):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w的L2范数是： &quot;</span>,torch.norm(w).item())</span><br><span class="line">train(lambd=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------&quot;</span>)</span><br><span class="line">train(lambd=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#明显过拟合</span></span><br><span class="line">结果：epoch100,train_loss0<span class="number">.000000</span>test_loss74<span class="number">.905751</span></span><br><span class="line">w的L2范数是：  <span class="number">12.894696235656738</span></span><br><span class="line"><span class="comment">#训练误差增加，但是测试误差减小，这就是我们想要的效果</span></span><br><span class="line">epoch100,train_loss0<span class="number">.000719</span>test_loss0<span class="number">.012278</span></span><br><span class="line">w的L2范数是：  <span class="number">0.04233487695455551</span></span><br></pre></td></tr></table></figure>

<p><strong>简洁实现</strong></p>
<p>主要的区别在于优化器的定义：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = torch.optim.SGD([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], </span><br><span class="line">    lr=lr)</span><br><span class="line"><span class="comment">#&#x27;weight_decay&#x27;是绝大部分框架的优化算法里面都会提供的选项</span></span><br></pre></td></tr></table></figure>



<h2 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h2><p>dropout</p>
<p>一个好的模型需要对输入数据的扰动鲁棒</p>
<ul>
<li>使用有噪音的数据等价于Tikhonov正则</li>
<li>丢弃法：在层之间加入噪音</li>
</ul>
<p>X’是加入噪音项后的x，但是我们希望期望不变<br>$$<br>E[X’] &#x3D; x<br>$$<br>dropout的定义：每个元素有p概率变为0，1-p概率变大，这样期望不变<br>$$<br>X’ &#x3D;<br>\begin{cases}<br>  0 &amp; \text{with probability } p, \<br>  \frac{x_i}{1-p} &amp; \text{otherwise}.<br>\end{cases}<br>$$<br>dropout通常作用在隐藏全连接层的输出上<br>$$<br>h &#x3D; \sigma(\mathbf{W_1} \mathbf{x} + b_1) \<br>h’ &#x3D; \text{dropout}(h) \<br>\hat{\mathbf{y}} &#x3D; \mathbf{W}_2 h’ + b_2 \<br>y &#x3D; \text{softmax}(\hat{\mathbf{y}})<br>$$<br><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818104912646.png" alt="image-20240818104912646"></p>
<p><strong>正则项只在训练中使用，用于参数的更新</strong></p>
<p><strong>但是在推理&#x2F;预测的时候，我们正常计算就可以，也就是说此时丢弃法直接返回输入</strong><br>$$<br>\mathbf{h} &#x3D; \text{dropout}(\mathbf{h})<br>$$<br>dropout的诞生理念是用多个子神经网络取平均，但是在实际实验过程中，其效果和正则项差不多，所以现在主流认为dropout是一个正则项</p>
<h3 id="代码实现-3"><a href="#代码实现-3" class="headerlink" title="代码实现"></a>代码实现</h3><p>softmax函数封装在CrossEntropyLoss函数内部</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X,dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span>&lt;=dropout&lt;=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    <span class="comment"># 随机生成向量(rand：0-1的正态分布），大于dropout为1，小于dropout为0</span></span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask*X/(<span class="number">1.0</span>-dropout)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;test&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># X = torch.arange(16,dtype=torch.float32).reshape((4,4))</span></span><br><span class="line"><span class="comment"># print(X)</span></span><br><span class="line"><span class="comment"># print(dropout_layer(X,1))</span></span><br><span class="line"><span class="comment"># print(dropout_layer(X,0))</span></span><br><span class="line"><span class="comment"># print(dropout_layer(X,0.5))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型 有两个隐藏层的多层感知机</span></span><br><span class="line">num_inputs,num_outputs,num_hiddens1,num_hiddens2 = <span class="number">784</span>,<span class="number">10</span>,<span class="number">256</span>,<span class="number">256</span></span><br><span class="line">dropout1 = <span class="number">0.2</span></span><br><span class="line">dropout2 = <span class="number">0.5</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span> (nn.Module): <span class="comment">#括号内表示net继承自nn.Module</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_inputs,num_outputs,num_hiddens1,num_hiddens2,</span></span><br><span class="line"><span class="params">              is_training = <span class="literal">True</span></span>):</span><br><span class="line">       <span class="comment"># 为net类的self对象调用父类的_init_函数</span></span><br><span class="line">       <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">       self.num_inputs = num_inputs</span><br><span class="line">       self.training = is_training</span><br><span class="line">       self.lin1 = nn.Linear(num_inputs,num_hiddens1)</span><br><span class="line">       self.lin2 = nn.Linear(num_hiddens1,num_hiddens2)</span><br><span class="line">       self.lin3 = nn.Linear(num_hiddens2,num_outputs)</span><br><span class="line">       self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward 是一个定义模型前向传播的具体行为的方法</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">       H1 = self.relu(self.lin1(X.reshape(-<span class="number">1</span>, self.num_inputs)))</span><br><span class="line">       <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">           H1 = dropout_layer(H1, dropout1)</span><br><span class="line">       H2 = self.relu(self.lin2(H1))</span><br><span class="line">       <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">           H2 = dropout_layer(H2, dropout2)</span><br><span class="line">       out = self.lin3(H2)</span><br><span class="line">       <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br><span class="line">num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;---concise----------------------------------------------------------------------------------------&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 主要区别还是在模型的定义上</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818114334999.png" alt="image-20240818114334999"></p>
<p>如果不用dropout，出现了过拟合的情况</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818115008315.png" alt="image-20240818115008315"></p>
<h2 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h2><p>当模型的深度比较大时，求梯度的两个问题<br>$$<br>1.5^{100} &#x3D; 4 * 10^{17}\<br>0.8^{100} &#x3D; 2*10^{-10}<br>$$</p>
<ul>
<li><p>梯度爆炸</p>
<ul>
<li>对学习率敏感<ul>
<li>lr太大-&gt;大参数值-&gt;大梯度</li>
<li>lr太小-&gt;训练无进展</li>
<li>可能需要在训练过程中不断调整学习率</li>
</ul>
</li>
</ul>
</li>
<li><p>梯度消失 </p>
<ul>
<li>梯度值变为0 <ul>
<li>不管学习率怎么取，都没有进展</li>
<li>对底部尤为严重，仅仅顶部层训练的好，无法让神经网络更深</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>让训练更加稳定</strong></p>
<p>目标：控制梯度值得方位</p>
<ul>
<li>将乘法变加法 ResNet ，LSTM</li>
<li>归一化  梯度归一化，梯度裁剪</li>
<li>合理的权重初始和激活函数</li>
</ul>
<p>这一节主要展示第三点：<strong>模型初始化</strong></p>
<p>让每一层的输出方差都是一个常数，这是我们的<strong>设计目标</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818132235329.png" alt="image-20240818132235329"></p>
<p> <strong>权重初始化</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818132525410.png" alt="image-20240818132525410"></p>
<p>以MLP为例，如果我们希望每一层的输出的方差相同，需要满足<br>$$<br>\mathbf{n_{t-1}}\mathbf{\sigma} &#x3D; 1\<br>\mathbf{n_{t}}\mathbf{\sigma} &#x3D; 1<br>$$<br>其中n_t表示第t层的输出的维度，sigma是每一层权重的方差</p>
<p><strong>公式推导：</strong></p>
<p>输出的方差相同</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818133105087.png" alt="image-20240818133105087"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818133114054.png" alt="image-20240818133114054"></p>
<p>梯度的方差相同</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818133513812.png" alt="image-20240818133513812"></p>
<h3 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h3><p>$$<br>要同时满足\mathbf{n_{in}}\mathbf{\sigma} &#x3D; 1 和<br>\mathbf{n_{out}}\mathbf{\sigma} &#x3D; 1是困难的<br>$$</p>
<p>因此，我们做出一定的权衡：<br>$$<br>\frac{1}{2}(\mathbf{n_{in}}+\mathbf{n_{out}})\mathbf{\sigma}^2 &#x3D;1<br>$$<br>从而，我们可以得到<strong>初始化权重的方法</strong>：<br>$$<br>Xavier初始化从均值为零，方差\<br>\sigma^2 &#x3D; \frac{2}{n_\mathrm{in} + n_\mathrm{out}}<br>的高斯分布中采样权重。<br>$$<br>注意：每一层的初始化函数都不相同，只要满足这个条件，就可以保证每一层的输出的方差和每一层的梯度的方差大致相同</p>
<p>同理，<strong>激活函数设置的原则</strong>：尽量减小对输出值方差、梯度方差的影响<br>$$<br>\mathbf{f}(\mathbf{x}) &#x3D; \mathbf{x}的影响就是最小的<br>$$<br>所以，我们可以在0附近采取泰勒展开等方式，让激活函数向f函数靠拢</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818140735616.png" alt="image-20240818140735616"></p>
<p>最后，别忘了，我们限制权重方位，调整优化函数的目的是：</p>
<p><strong>保持数值稳定性</strong></p>
<h1 id="实战：Kaggle房价预测"><a href="#实战：Kaggle房价预测" class="headerlink" title="实战：Kaggle房价预测"></a>实战：Kaggle房价预测</h1><p>比赛链接</p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a></p>
<p>比赛的形式是，Kaggle给出训练数据集和测试数据集</p>
<p><strong>测试数据集</strong>没有标签，用于最后的评估</p>
<p>我们需要自行将训练数据集进行分割，一部分用作<strong>验证数据集</strong></p>
<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><p>使用api接口下载数据集的方法：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41847262/article/details/126154548">kaggle注册以及数据集下载全流程_kaggle数据集下载-CSDN博客</a></p>
<p>这里我们直接下载压缩包，用绝对路径访问</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">r&#x27;C:\Users\86181\Desktop\project\deeplearning_code\data\kaggle_house_price\train.csv&#x27;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">r&#x27;C:\Users\86181\Desktop\project\deeplearning_code\data\kaggle_house_price\test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_data.iloc[<span class="number">0</span>:<span class="number">5</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(test_data.iloc[<span class="number">0</span>:<span class="number">5</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>

<p>(1460, 81)<br>(1459, 80)<br>   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice<br>0   1          60       RL         65.0       WD        Normal     208500<br>1   2          20       RL         80.0       WD        Normal     181500<br>2   3          60       RL         68.0       WD        Normal     223500<br>3   4          70       RL         60.0       WD       Abnorml     140000<br>4   5          60       RL         84.0       WD        Normal     250000<br>     Id  MSSubClass MSZoning  LotFrontage  YrSold SaleType SaleCondition<br>0  1461          20       RH         80.0    2010       WD        Normal<br>1  1462          20       RL         81.0    2010       WD        Normal<br>2  1463          60       RL         74.0    2010       WD        Normal<br>3  1464          60       RL         78.0    2010       WD        Normal<br>4  1465         120       RL         43.0    2010       WD        Normal</p>
<p>可见训练数据集有1460个数据，79个特征，1个标签</p>
<p>测试数据集没有标签</p>
<h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>我们采用来标准化数据<br>$$<br>\mathbf{x} \leftarrow \frac{\mathbf{x}-\mathbf{u}}{\sigma}<br>$$<br>在不指定axis的情况下，.mean和.std函数沿着0轴进行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">all_features = pd.concat((train_data.iloc[:,<span class="number">1</span>:-<span class="number">1</span>],test_data.iloc[:,<span class="number">1</span>:-<span class="number">1</span>])) <span class="comment"># id不是特征值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;print(all_features.shape)&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 提取数值特征列的索引，也就是列名</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;print(numeric_features[:10])&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std()))</span><br><span class="line"><span class="comment"># 在标准化数据之后，均值为0，因此我们可以将缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>处理离散值（字符串）采用独热编码表示</p>
<p>采用独热编码表示后，特征会增加</p>
<p>此时all_features的形状为(2919,300)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在标准化数据之后，均值为0，因此我们可以将缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p><strong>通过Values属性,从<code>pandas</code>格式中提取NumPy格式，并将其转换为张量表示</strong></p>
<p>pandas老版本的独热编码只有0和1，而新版本则改成true和false</p>
<p>所以要加上all_features &#x3D; all_features * 1，转成0和1</p>
<p>或者将独热编码写成all_features &#x3D; pd.get_<a target="_blank" rel="noopener" href="https://search.bilibili.com/all?from_source=webcommentline_search&keyword=dummies&seid=14398139424397262989">dummies</a>(all_features, dummy_na&#x3D;True,dtype&#x3D;int)，强制转化后的数据类型是int型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features = all_features * <span class="number">1</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line"><span class="comment"># 测试集的特征</span></span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(</span><br><span class="line">    train_data.SalePrice.values.reshape(-<span class="number">1</span>, <span class="number">1</span>), dtype=torch.float32)</span><br></pre></td></tr></table></figure>



<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>注意，我们这里用于评估准确度的是房价的误差<br>$$<br>\frac{y-\hat{y}}{y}<br>$$<br><strong>一种方法是用价格预测的对数来衡量差异,这也是比赛官方给出的衡量误差情况的方法</strong><br>$$<br>\sqrt{\frac{1}{n}\sum_{n} \left( \log(y_i) - \log(g_i) \right)^2}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">in_features = all_features.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单层线性回归</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Linear(in_features,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 衡量误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_rmse</span>(<span class="params">net,features,lables</span>):</span><br><span class="line">    clipped_preds = torch.clamp(net(features),<span class="number">1</span>,<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">    rmse = torch.sqrt(loss(clipped_preds.log(),lables.log()))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net,train_features,train_labels,test_features,test_labels,</span></span><br><span class="line"><span class="params">          num_epochs,learning_rate,weight_decay,batch_size</span>):</span><br><span class="line">    train_ls,test_ls = [],[]</span><br><span class="line">    train_iter = d2l.load_array((train_features,train_labels),batch_size)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(),lr = learning_rate,</span><br><span class="line">                                weight_decay = weight_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net,train_features,train_labels))</span><br><span class="line">        <span class="keyword">if</span>(test_labels) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net,test_features,test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls,test_ls</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>一开始写这段代码的时候，return写在了for X，y循环的内部</p>
<p>导致返回的是第一轮的第一次训练的损失，特别大</p>
<p><strong>k折交叉验证</strong></p>
<p>X_part，y_part就是数据集其中的一折</p>
<p>如果不用作验证集，就拼接到训练集上</p>
<p>idx是slice对象，如果idx是（0，20）就从0开始索引，到20结束（不包括20）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># K折交叉验证</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_k_fold_data</span>(<span class="params">k,i,X,y</span>):</span><br><span class="line">    <span class="comment"># i是当前被用作验证集的折数的编号</span></span><br><span class="line">    <span class="keyword">assert</span> k &gt;<span class="number">1</span></span><br><span class="line">    <span class="comment"># 每一折的大小 101//5 = 20  注意，多余的部分例如101多出来的那个1，不会当作一折</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># slice 对象，用于索引</span></span><br><span class="line">        idx = <span class="built_in">slice</span>(j * fold_size, (j + <span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part,y_part = X[idx,:],y[idx] <span class="comment"># 获取当前折的数据</span></span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid,y_valid = X_part,y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">    X_train = torch.cat([X_train,X[k*fold_size:]], <span class="number">0</span>)</span><br><span class="line">    y_train = torch.cat([y_train,y[k*fold_size:]], <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 多余的数据拼到训练集里</span></span><br><span class="line">    X_train = torch.cat(X_train,X[k*fold_size:], <span class="number">0</span>)</span><br><span class="line">    y_train = torch.cat(y_train,y[k*fold_size:], <span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure>

<p>注意：k次训练之间是独立的，重复k次训练只是为了获取更为<strong>客观的误差</strong>，方便后续调参</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">k_fold</span>(<span class="params">k,X_train,y_train,num_epochs,learning_rate,weight_decay,batch_size</span>):</span><br><span class="line">    train_l_sum,valid_l_sum = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="comment"># 每一次拿到第i折的数据，拿去训练</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># 每一次训练都是独立的，因此要重新生成一个net</span></span><br><span class="line">        data = get_k_fold_data(k,i,X_train,y_train)</span><br><span class="line">        net = get_net()</span><br><span class="line">        train_ls,valid_ls = train(net,*data,num_epochs,learning_rate,</span><br><span class="line">                                 weight_decay,batch_size)</span><br><span class="line">        train_l_sum += train_ls[-<span class="number">1</span>] <span class="comment"># 累积误差</span></span><br><span class="line">        valid_l_sum += valid_ls[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 每次训练的误差</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;折<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>，训练log rmse<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;验证log rmse<span class="subst">&#123;<span class="built_in">float</span>(valid_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># k次训练的平均误差</span></span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure>

<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>定义各种超参数，并进行训练</p>
<p>这一步可以<strong>反复尝试进行调参</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,</span><br><span class="line">                          weight_decay, batch_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>-折验证: 平均训练log rmse: <span class="subst">&#123;<span class="built_in">float</span>(train_l):f&#125;</span>, &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;平均验证log rmse: <span class="subst">&#123;<span class="built_in">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>折1，训练log rmse0.131836, 验证log rmse0.144131<br>折2，训练log rmse0.129436, 验证log rmse0.146554<br>折3，训练log rmse0.128286, 验证log rmse0.142680<br>折4，训练log rmse0.133345, 验证log rmse0.135618<br>折5，训练log rmse0.126239, 验证log rmse0.166833<br>5-折验证: 平均训练log rmse: 0.129828, <strong>平均验证log rmse: 0.147163</strong></p>
<p>把num_epochs改成2000-&gt;过拟合</p>
<p>给net加入一个隐藏层-&gt;过拟合</p>
<p>5-折验证: 平均训练log rmse: 0.098884, <strong>平均验证log rmse: 0.205867</strong></p>
<h3 id="提交Kaggle预测"><a href="#提交Kaggle预测" class="headerlink" title="提交Kaggle预测"></a>提交Kaggle预测</h3><p>在调好各种超参数以后，我们在完整的训练集上训练最后一次，然后对测试集进行预测，将结果存放在,csv文件中</p>
<p>submission.csv文件会放在同一个目录下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data,</span></span><br><span class="line"><span class="params">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class="line">    net = get_net()</span><br><span class="line">    <span class="comment"># 这次训练用的是完整的train_features，同时不需要验证</span></span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练log rmse：<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将网络应用于测试集。</span></span><br><span class="line">    <span class="comment"># .detach() 方法在PyTorch中用于从当前的计算图中分离出张量，使得这个张量不会参与梯度计算</span></span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    <span class="comment"># 将其重新格式化以导出到Kaggle</span></span><br><span class="line">    test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, -<span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># submission只有两列</span></span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_and_pred(train_features, test_features, train_labels, test_data,</span><br><span class="line">                num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>

<p>在kaggle网站提交后，得分0.35246，排名4191</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240819135000707.png" alt="image-20240819135000707"></p>
<p><strong>kaggle网站上还有别人的优秀代码可以看</strong></p>
<h1 id="ch04-深度学习计算"><a href="#ch04-深度学习计算" class="headerlink" title="ch04_深度学习计算"></a>ch04_深度学习计算</h1><p>神经网络看起来很复杂，节点很多，层数多，参数更多。但核心部分或组件不多，把这些组件确定后，这个神经网络基本就确定了。这些核心组件包括：<br>（1）层：神经网络的基本结构，将输入张量转换为输出张量。<br>（2）模型：层构成的网络。<br>（3）损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。<br>（4）优化器：如何是损失函数最小，这就涉及到优化器。</p>
<h2 id="模型构造"><a href="#模型构造" class="headerlink" title="模型构造"></a>模型构造</h2><p>在PyTorch中，<code>Module</code>是所有神经网络组件的基类，它提供了一个框架来构建具有层次结构的模型</p>
<p>回顾多层感知机的模型定义：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>,<span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>Sequential本质上是一个继承自Module的类，按照顺序执行输入的层</p>
<p>想直观地了解块是如何工作的，最简单的方法就是自己实现一个</p>
<p><strong>顺序块：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment">#这里的 args是我们传入的层</span></span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module <span class="comment"># 按照顺序存放层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<p><strong>自定义块：</strong></p>
<p>当然，我们并不总是只用到顺序快，而只需要修改上述两个函数，我们就可以自定义一个符合我们需求的块</p>
<p>__inti__函数定义了我们需要的层、参数,__forward__函数定义了前向计算方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure>



<h2 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h2><h3 id="访问参数"><a href="#访问参数" class="headerlink" title="访问参数"></a>访问参数</h3><p>同样回顾多层感知机的模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>,<span class="number">8</span>), <span class="comment"># net[0]</span></span><br><span class="line">                    nn.ReLU(),		<span class="comment">#net[1]</span></span><br><span class="line">                    nn.Linear(<span class="number">8</span>,<span class="number">1</span>)) <span class="comment"># net[2]</span></span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(net(X))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">OrderedDict([(&#x27;weight&#x27;, tensor([[-0.3272,  0.2718,  0.0853, -0.1085,  0.3155, -0.2714, -0.0330, -0.2700]])), (&#x27;bias&#x27;, tensor([-0.3074]))])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 访问某个数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="comment"># 访问全部参数</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span></span><br><span class="line"><span class="string">(&#x27;0.weight&#x27;, torch.Size([8, 4])) (&#x27;0.bias&#x27;, torch.Size([8])) (&#x27;2.weight&#x27;, torch.Size([1, 8])) (&#x27;2.bias&#x27;, torch.Size([1]))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p><code>state_dict()</code>方法用于返回模块（<code>nn.Module</code>）的所有参数和缓存的字典</p>
<p>如果我们将多个块相互嵌套，参数命名约定是如何工作的?</p>
<p>注意理解最后输出的三个Sequential的嵌套关系</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (0): Sequential(</span></span><br><span class="line"><span class="string">    (block 0): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    (block 1): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    (block 2): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    (block 3): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (1): Linear(in_features=4, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>我们也可以像通过嵌套列表索引一样访问它们</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].bias.data</span><br></pre></td></tr></table></figure>



<h3 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h3><p><strong>内置初始化：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net[<span class="number">0</span>].apply(init_normal) <span class="comment"># 对指定层应用</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_constant)</span><br></pre></td></tr></table></figure>

<p><strong>自定义初始化：</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240823133403145.png" alt="image-20240823133403145"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br></pre></td></tr></table></figure>

<p><strong>参数绑定</strong></p>
<p>有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>这个例子表明第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。</p>
<p>由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</p>
<h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2><p>其实自定义层和自定义网络没有本质区别，我们只需继承基础层类并实现前向传播功能。</p>
<p>不带参数的层：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br></pre></td></tr></table></figure>

<p>带参数的层</p>
<p><code>in_units</code>和<code>units</code>，分别表示输入数和输出数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line">    </span><br><span class="line"> linear = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>



<h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。 因此，现在是时候学习如何加载和存储权重向量和整个模型了。</p>
<h3 id="read-and-write-张量"><a href="#read-and-write-张量" class="headerlink" title="read_and_write_张量"></a>read_and_write_张量</h3><p>对于单个张量，我们可以直接调用<code>load</code>和<code>save</code>函数分别读写它们。 这两个函数都要求我们提供一个名称，<code>save</code>要求将要保存的变量作为输入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>存储一个张量列表</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y],<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="read-and-write-模型"><a href="#read-and-write-模型" class="headerlink" title="read_and_write_模型"></a>read_and_write_模型</h3><p>深度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。</p>
<p>为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br><span class="line"><span class="comment"># 保存参数</span></span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br><span class="line"><span class="comment"># 恢复模型</span></span><br><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>



<h2 id="GPU使用"><a href="#GPU使用" class="headerlink" title="GPU使用"></a>GPU使用</h2><p>默认情况下，张量创建在CPU上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;device(type=&#x27;cpu&#x27;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>使用GPU创建</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>)) <span class="comment"># 在第二块GPU上创建</span></span><br></pre></td></tr></table></figure>

<p>如果我们希望用X+Y，但二者创建在不同的GPU上，那么可以将<code>X</code>传输到第二个GPU并在那里执行操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意，这里Z 和X 共享相同的数据和梯度</span></span><br><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># </span></span><br><span class="line">Z = X.cuda(<span class="number">1</span>).detach()</span><br><span class="line">Y + Z</span><br></pre></td></tr></table></figure>

<p>类似地，神经网络模型可以指定设备。 下面的代码将模型参数放在GPU上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure>



<h1 id="ch05-卷积神经网络"><a href="#ch05-卷积神经网络" class="headerlink" title="ch05_卷积神经网络"></a>ch05_卷积神经网络</h1><p>“Convolutional Neural Network ——CNN</p>
<p>适合于计算机视觉的神经网络架构。</p>
<ol>
<li><p><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</p>
</li>
<li><p><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p>
</li>
</ol>
<p>简单来说，神经网络通过先看图像的小部分（局部性），然后不管这些小部分在图像中怎么移动（平移不变性），都能把它们认出来。</p>
<h2 id="从全连接层到卷积"><a href="#从全连接层到卷积" class="headerlink" title="从全连接层到卷积"></a>从全连接层到卷积</h2><ul>
<li><p>将输入和输出从向量变形为矩阵</p>
</li>
<li><p>将权重变形为4-D的张量</p>
<p>为什么权重变形为4维呢？因为在w_{i,j,k,l}中，i,j代表输出的点在输出矩阵中的位置，k,l代表输入点在输入的图（或者矩阵）中的位置，要想完全记录所有的权重，需要一个4维张量。</p>
</li>
<li><p>V是对W的重新索引<br>$$<br>\mathbf{v_{i,j,a,b}} &#x3D; \mathbf{w_{i,j,i+a,j+b}}<br>$$</p>
</li>
</ul>
<p>现在，我们可以将全连接层表示为：其中U表示偏置<br>$$<br>\begin{aligned} \left[\mathbf{H}\right]<em>{i, j} &amp;&#x3D; [\mathbf{U}]</em>{i, j} + \sum_k \sum_l[\mathsf{W}]<em>{i, j, k, l}  [\mathbf{X}]</em>{k, l}\ &amp;&#x3D;  [\mathbf{U}]<em>{i, j} +<br>\sum_a \sum_b [\mathsf{V}]</em>{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}<br>$$<br><strong>#原则1 平移不变性</strong></p>
<p>假设我们目标的特征值在输入中发生了平移，就像小猫从图片的左下角移动到左上角，我们希望着仅导致隐藏表示H的平移，也就是说<strong>V和U实际上不依赖于（i，j）的数值</strong>，即：<br>$$<br>解决方案：  \mathbf{v_{i,j,a,b}} &#x3D;  \mathbf{v_{a,b}}\<br>即H定义为<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum_a\sum_b [\mathbf{V}]</em>{a, b} [\mathbf{X}]_{i+a, j+b}<br>$$<br><strong>这就是交叉相换</strong>&#x2F;<strong>二维卷积</strong></p>
<p><strong>#原则2 局部性</strong><br>$$<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum_a\sum_b [\mathbf{V}]</em>{a, b} [\mathbf{X}]_{i+a, j+b}<br>$$<br>评估H时，我们不应该用远离x_i,j的参数，相当于我们只考虑附近的元素</p>
<p>解决方案：<br>$$<br>|a|&gt; \Delta或|b| &gt; \Delta的范围之外，我们可以设置[\mathbf{V}]<em>{a, b} &#x3D; 0。\<br>因此，我们可以将[\mathbf{H}]</em>{i, j}重写为\</p>
<p>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum</em>{a &#x3D; -\Delta}^{\Delta} \sum_{b &#x3D; -\Delta}^{\Delta} [\mathbf{V}]<em>{a, b}  [\mathbf{X}]</em>{i+a, j+b}<br>$$</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824104448021.png" alt="image-20240824104448021"></p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>什么是卷积？</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110256292.png" alt="image-20240824110256292"></p>
<p>用一个卷积核不断地去扫描输入矩阵，点乘后求和映射到输出矩阵</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110015411.png" alt="image-20240824110015411"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110621791.png" alt="image-20240824110621791"></p>
<p>交叉相关和二维卷积在实际使用中没有区别，只是两个不同的数学概念，我们用的其实是交叉相关，不过习惯叫成卷积层</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110939967.png" alt="image-20240824110939967"></p>
<p>一维&#x2F;三维地数据也可用卷积层</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824111106025.png" alt="image-20240824111106025"></p>
<p><strong>总结：</strong></p>
<ul>
<li><strong>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出</strong></li>
<li><strong>核矩阵和偏移是需要学习的参数</strong></li>
<li><strong>核矩阵的大小是超参数</strong></li>
</ul>
<h3 id="代码实现-4"><a href="#代码实现-4" class="headerlink" title="代码实现"></a>代码实现</h3><p>定义互相关运算</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824111807444.png" alt="image-20240824111807444"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 互相关运算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X,K</span>):</span><br><span class="line">    h,w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>]-h+<span class="number">1</span>,X.shape[<span class="number">1</span>]-w+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i,j] = (X[i:i+h,j:j+w]*K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<p>自定义二维卷积层</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 二维卷积层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x,self.weight) + self.bias</span><br></pre></td></tr></table></figure>

<p>接下来，我们进行一个简单的应用：检测图片中不同颜色的边缘</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X</span></span><br><span class="line"><span class="string">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>如果我们构造卷积核：K &#x3D; torch.tensor([[1.0, -1.0]])</p>
<p>这个卷积核可以对<strong>垂直边缘</strong>进行检测，水平边缘则不行</p>
<p><strong>学习卷积核</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，输入通道：1，输出通道：1，卷积核形状：（1，2）</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>训练结果：</p>
<p>epoch 2, loss 5.172<br>epoch 4, loss 0.976<br>epoch 6, loss 0.208<br>epoch 8, loss 0.053<br>epoch 10, loss 0.016<br>tensor([[[[-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232]]]],<br>       grad_fn&#x3D;<ConvolutionBackward0>)</ConvolutionBackward0></p>
<h2 id="卷积层的填充和步幅"><a href="#卷积层的填充和步幅" class="headerlink" title="卷积层的填充和步幅"></a>卷积层的填充和步幅</h2><p>给定32X32的输入，应用7层5X5的卷积核，最后输出的大小为4X4，也就是说<strong>最多使用7层卷积层</strong></p>
<p>在深度学习中，我们希望增加神经网络层数</p>
<p><strong>填充</strong></p>
<p>在输入数据周围填上一圈数据，输出的行数+2，列数+2</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825093104898.png" alt="image-20240825093104898"></p>
<p><strong>步幅</strong></p>
<p>给定输入大小为224X224，在使用5X5的卷积核的情况下，需要55层才将输出降低到4X4，因此需要大量的计算</p>
<p>相当于让我们的窗口以更大的幅度移动，这会减小输出的大小</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825094000404.png" alt="image-20240825094000404"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825094230288.png" alt="image-20240825094230288"></p>
<p><strong>填充和步幅是卷积层的超参数</strong>，填充使输出线性扩大，步幅使输出成倍减小</p>
<h3 id="代码实现-5"><a href="#代码实现-5" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d,X</span>):</span><br><span class="line">    <span class="comment">#  将输入数据X重塑为适合卷积操作的形状。</span></span><br><span class="line">    <span class="comment">#  这里X原本的形状是(8,8)，重塑后变为(1,1,8,8)</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>,<span class="number">1</span>)+X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3X3的卷积核，填充为1【上下左右各填充1行】</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;torch.Size([8, 8])，可以看到经过一层卷积层后形状不变&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># padding的第一个参数表示填充的行数</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=(<span class="number">5</span>,<span class="number">3</span>),padding=(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#步幅------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 注意到8可以被2整除，会变成4X4矩阵</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (8-3+0+3)/3 = 2  (8-5+2+4)/4 = 2 [向下取整]</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=(<span class="number">3</span>,<span class="number">5</span>),padding=(<span class="number">0</span>,<span class="number">1</span>),stride=(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br></pre></td></tr></table></figure>



<h2 id="输入和输出通道"><a href="#输入和输出通道" class="headerlink" title="输入和输出通道"></a>输入和输出通道</h2><p>彩色图片有RGB三个通道，灰度图片有黑白灰三种颜色，一个通道</p>
<p>如果输入有多个通道，每个通道有都有一个卷积核，结果是所有通道卷积结果的和</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825102123898.png" alt="image-20240825102123898"><br>$$<br>Y &#x3D; \sum_{i&#x3D;0}^{c_{i}} (X_{i,:,:}^{\prime}) \star W_{i,:,:}<br>$$<br>如果我们需要多个输出通道，那么就需要多个<strong>三维卷积核</strong>，<strong>每个三维卷积核生成一个通道</strong><br>$$<br>c_{i}表示输入通道数,c_{o}表示输出通道数\<br>输入 X：c_{i}\times n_{h} \times n_{w} \<br>核W：c_{o} \times c_{i}\times k_{h} \times k_{w}\<br>输出Y :c_{o} \times m_{h} \times m_{w}\<br>Y_{i,:,:} &#x3D; X\star W_{i,:,:,:} ; \text{for i in 1,…}c_{o}<br>$$</p>
<ul>
<li><strong>每一个输出通道可以识别特定模式</strong></li>
<li>三维卷积核识别并组合输入中的模式</li>
</ul>
<p>通俗理解，假如我们希望识别图片中的猫，一个三维卷积核识别轮廓，一个识别猫头，一个识别猫毛……最后组合起来，就能将猫识别出来</p>
<p><strong>1X1卷积层</strong></p>
<p>卷积核的形状为（1.1），这样的卷积核不识别空间模式，只是融合通道</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825103801816.png" alt="image-20240825103801816"><br>$$<br>相当于输入形状为n_{h}n_{w}\times c_{i} \text{， }权重为c_{i} \times c_{o}的全连接层<br>$$<br><strong>总结</strong></p>
<p>输出通道数是超参数</p>
<p>每个输出通道对应一个独立的三维卷积核</p>
<h3 id="代码实现-6"><a href="#代码实现-6" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多通道的互相关运算</span></span><br><span class="line"><span class="comment"># K是三维卷积核</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X,K</span>):</span><br><span class="line">    <span class="comment"># zip(X,K) 函数将 X 和 K 中的元素配对，形成一系列的元组，每个元组包含一个输入张量和一个卷积核</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x,k) <span class="keyword">for</span> x,k <span class="keyword">in</span> <span class="built_in">zip</span>(X,K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># K是四维卷积核</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X,K</span>):</span><br><span class="line">    <span class="comment"># torch.stack将多个张量沿着一个新的维度合并成一个更大的张量。</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X,k) <span class="keyword">for</span> k <span class="keyword">in</span> K],<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2,3,3)</span></span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"><span class="comment"># (3,2,2,2)</span></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(corr2d_multi_in_out(X, K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证1X1卷积层等价于全连接层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X,K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h*w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    Y = torch.matmul(K,X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line">X = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X,K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X,K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>((Y1-Y2).<span class="built_in">sum</span>())) &lt; <span class="number">1e-6</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化操作（如最大池化）有助于增加网络对小的平移的不变性。即使特征在池化窗口内发生小的平移，池化层仍然可以捕捉到最重要的特征（例如最大值），也就是说，<strong>使用了池化层后，即便输入发生了一定的平移，输出的结果可以保持不变</strong></p>
<p>二维最大池化：返回滑动窗口中的最大值</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825121547242.png" alt="image-20240825121547242"></p>
<p>池化层则对卷积层的输出进行下采样</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825122540693.png" alt="image-20240825122540693"></p>
<p><strong>池化层并不能像卷积层一样融合多个输入通道的结果，池化层每一个通道都有一个独立的输出</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825122941515.png" alt="image-20240825122941515"></p>
<p><strong>总结</strong></p>
<ul>
<li>池化层返回窗口的最大值或平均值</li>
<li>作用是缓解卷积层对位置的敏感性</li>
<li>同样有窗口大小、填充、步幅作为超参数</li>
</ul>
<h3 id="代码实现-7"><a href="#代码实现-7" class="headerlink" title="代码实现"></a>代码实现</h3><p>深度学习框架中的步幅与池化窗口的大小相同，意义是两个窗口之间是没有重叠的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line"><span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充和步幅</span></span><br><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>) <span class="comment"># 3X3的窗口</span></span><br><span class="line"><span class="comment"># 深度学习框架中的步幅与池化窗口的大小相同，意义是两个窗口之间是没有重叠的</span></span><br><span class="line"><span class="built_in">print</span>(pool2d(X))</span><br><span class="line"><span class="comment"># 手动设定填充和步幅</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>) <span class="comment"># 4-3+1*2+2/2=2</span></span><br><span class="line"><span class="built_in">print</span>(pool2d(X))</span><br><span class="line"></span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(pool2d(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多通道</span></span><br><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>) <span class="comment"># 注意这里沿着0/1拼接的区别</span></span><br><span class="line"><span class="built_in">print</span>(X.shape) <span class="comment"># 1,2,4,4</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(pool2d(X)) <span class="comment"># 可以看到每个通道之间的输出是独立的</span></span><br></pre></td></tr></table></figure>





<h2 id="卷积神经网络-LeNet"><a href="#卷积神经网络-LeNet" class="headerlink" title="卷积神经网络_LeNet"></a>卷积神经网络_LeNet</h2><p>设计之初是为了实现一个手写数字识别的应用</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240826074439501.png" alt="image-20240826074439501"></p>
<p>LeNet的输入是的图片，然后放到一个5X5的<strong>卷积层</strong>中，接着采用2X2的<strong>池化层[<strong>步幅与池化层形状相同]，再用5X5的</strong>卷积层</strong>，最后用2X2的<strong>池化层</strong>处理后拉成一个向量，输入到<strong>全连接层</strong>，再输入到<strong>全连接层</strong>，最后输入到<strong>高斯层</strong>【类似于一个softmax】得到输出结果</p>
<p>注意这个过程中的通道数目变化，只有卷积层会影响通道数目</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240826075326758.png" alt="image-20240826075326758"></p>
<h3 id="代码实现-8"><a href="#代码实现-8" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(</span><br><span class="line">    Reshape(), <span class="comment"># 将输入图片转化为28X28</span></span><br><span class="line">    <span class="comment"># 28-5+4+1 = 28</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    <span class="comment"># 28-2+2/2 = 14</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 14-5+1 = 10</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,kernel_size=<span class="number">5</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    <span class="comment"># 10-2+2/2 =5</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 16X5X5</span></span><br><span class="line">    <span class="comment"># (1,16,5,5)展平到(1,400)</span></span><br><span class="line">    nn.Flatten(), <span class="comment"># 从第一维展平到最后一维，第0维通常是批量大小</span></span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>,<span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>,<span class="number">10</span>),</span><br><span class="line">    )</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X = torch.rand(size=(1,1,28,28),dtype = torch.float32)</span></span><br><span class="line"><span class="string">for layer in net:</span></span><br><span class="line"><span class="string">    X = layer(X)</span></span><br><span class="line"><span class="string">    print(layer.__class__.__name__,&#x27;output shape:\t&#x27;,X.shape)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">banch_size = <span class="number">256</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size=banch_size)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net,data_iter,device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net,torch.nn.Module): <span class="comment"># 检查net是否为torch.nn.Module类/子类</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            <span class="comment"># 获取参数列表的第一个参数的设备</span></span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X,<span class="built_in">list</span>):</span><br><span class="line">            X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 张量的转化方法</span></span><br><span class="line">            X = X.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        metric.add(d2l.accuracy(net(X),y),y.numel())</span><br><span class="line">        <span class="keyword">return</span> metric[<span class="number">0</span>]/metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net,train_iter,test_iter,num_epochs,lr,device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>,device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>,xlim=[<span class="number">1</span>,num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>,<span class="string">&#x27;train acc&#x27;</span>,<span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer = d2l.Timer()</span><br><span class="line">    <span class="comment"># len(train_iter)是训练集的批次数</span></span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enumerate(train_iter) 会将 train_iter 中的每个元素与其索引（从0开始）封装成元组 (index, (X, y))。</span></span><br><span class="line"><span class="string">        for 循环中的 i 就是这个索引，它表示当前是第几个批次。i 是一个整数，每次循环迭代时递增。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i,(X,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            y=y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat,y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            metric.add(l*X.shape[<span class="number">0</span>],d2l.accuracy(y_hat,y),X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>]/metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>]/metric[<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># 每 num_batches // 5 个批次打印一次，最后一个数据打印一次，相当于打印五次或六次</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="comment"># 训练结束</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">            <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>



<p>训练结果：</p>
<p>loss 0.487, train acc 0.816, test acc 0.840<br>66594.0 examples&#x2F;sec on cuda:0</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240828081313830.png" alt="image-20240828081313830"></p>
<h1 id="ch06-现代卷积神经网络"><a href="#ch06-现代卷积神经网络" class="headerlink" title="ch06_现代卷积神经网络"></a>ch06_现代卷积神经网络</h1><h2 id="深度卷积神经网络（AlexNet）"><a href="#深度卷积神经网络（AlexNet）" class="headerlink" title="深度卷积神经网络（AlexNet）"></a>深度卷积神经网络（AlexNet）</h2><p>在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。</p>
<p>另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，认为<strong>特征本身应该被学习</strong>。此外，他们还认为，在合理地复杂性前提下，<strong>特征应该由多个共同学习的神经网络层</strong>组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体<em>AlexNet</em>。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，</p>
<p>深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素：</p>
<ul>
<li><p><strong>数据</strong>，在2009年以前，研究使用的数据集都是非常小的，2009年，<strong>ImageNet</strong>数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。</p>
</li>
<li><p><strong>硬件</strong>，图形处理器<em>（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的4×4矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为</em>通用GPU（general-purpose GPUs，GPGPU）来销售。</p>
<p>当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上<strong>并行化</strong>的操作。 于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新<a target="_blank" rel="noopener" href="https://code.google.com/archive/p/cuda-convnet/">cuda-convnet</a>几年来它一直是行业标准，并推动了深度学习热潮。</p>
</li>
</ul>
<p>AlexNet首次证明了<strong>学习到的特征可以超越手工设计的特征</strong></p>
<p>下图左侧是LeNet的架构，右侧是AlexNet的简化版</p>
<p>可以看到相比于LeNet，AlexNet的窗口更大，增加了三个卷积层</p>
<p>更多细节：</p>
<ul>
<li>采用ReLU作为激活函数（减缓梯度消失）</li>
<li>隐藏全连接层后加入了丢弃层</li>
<li>数据增强（例如会将图片进行随机截取、随机调整亮度、色温等等，可以理解为扩充了数据集）</li>
</ul>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240828181032751.png" alt="image-20240828181032751"></p>
<h3 id="代码实现-9"><a href="#代码实现-9" class="headerlink" title="代码实现"></a>代码实现</h3><p>注意，我们这里使用的是Fashion-MNIST数据集，这是因为如果使用ImageNet，会需要数个小时的时间</p>
<p>Fashion-MNIST中的图片是28X28，我们将它们增加到224X224，通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用AlexNet架构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span></span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 另外，输出通道的数目远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>), <span class="comment"># 加入噪音，防止过拟合</span></span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察形状</span></span><br><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">num_epochs=<span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240828221115006.png" alt="image-20240828221115006"></p>
<h2 id="使用块的网络（VGG）"><a href="#使用块的网络（VGG）" class="headerlink" title="使用块的网络（VGG）"></a>使用块的网络（VGG）</h2><p>基本思想：用可重复使用的卷积快来构建深度神经网络</p>
<p>VGG块：</p>
<ul>
<li>3X3卷积核（padding 1）</li>
<li>2X2最大池化层（stride 2）</li>
</ul>
<p>VGG架构：多个VGG块后连接全连接层，不同次数的重复块得到不同的架构：VGG-16,VGG-19</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829073757999.png" alt="image-20240829073757999"></p>
<p>相较于AlexNet,VGG块使用的窗口更小，同时网络更深</p>
<h3 id="代码实现-10"><a href="#代码实现-10" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_convs表示一个vgg块中卷积层的数量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs,in_channels,out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels,out_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># convolutional architecture</span></span><br><span class="line">conv_arch = ((<span class="number">1</span>,<span class="number">64</span>),(<span class="number">1</span>,<span class="number">128</span>),(<span class="number">2</span>,<span class="number">256</span>),(<span class="number">2</span>,<span class="number">512</span>),(<span class="number">2</span>,<span class="number">512</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs,out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        <span class="comment"># 注意这里是append的是一个线性层，也就是说每一块之间是隔开的</span></span><br><span class="line">        conv_blks.append(vgg_block(num_convs,in_channels,out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*conv_blks,nn.Flatten(),</span><br><span class="line">                         <span class="comment"># 这里没有泛化，因为输入是224X224才这样写</span></span><br><span class="line">                         nn.Linear(out_channels*<span class="number">7</span>*<span class="number">7</span>,<span class="number">4096</span>),nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                         nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>),nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                         nn.Linear(<span class="number">4096</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次经过vgg块处理后，高宽都除以2，经过5个vgg块后，高宽变为7X7</span></span><br><span class="line"><span class="comment"># 注意到每次高宽减半，通道数都翻倍，这是以这种经典的设计模式</span></span><br><span class="line">net = vgg(conv_arch)</span><br><span class="line">X = torch.randn(size=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算量太大了，所以这里构造小一点的通道数</span></span><br><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829081350419.png" alt="image-20240829081350419"></p>
<h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><p>Network in Network</p>
<p>之前的神经网络中，都在卷积层的最后加入了全连接层</p>
<p>以AlexNet为例，第一个全连接层的参数个数为：256X5X5X4096&#x3D; 26M【将（256，5，5）拉平后银蛇到4096】，这个参数个数是很大的</p>
<p>NIN的思想就用卷积层代替全连接层，这样能够有更少的参数个数</p>
<p> <strong>NiN块</strong></p>
<ul>
<li>卷积层+两个1X1卷积层，1x1卷积层在保持空间维度不变的情况下，实现跨通道的特征融合，这相当于对每个像素点进行全连接操作，从而在不损失分辨率的前提下增加非线性特性</li>
<li>起到全连接层的作用</li>
</ul>
<p><strong>NiN架构</strong> </p>
<ul>
<li>无全连接层</li>
<li>交替使用NiN块和步幅为2的最大池化层，逐步较小高宽比和增大通道数</li>
<li>最后使用平均池化层得到输出</li>
</ul>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829164933918.png" alt="image-20240829164933918"></p>
<h3 id="代码实现-11"><a href="#代码实现-11" class="headerlink" title="代码实现"></a>代码实现</h3><p><code>nn.AdaptiveAvgPool2d((1,1))</code> 是 PyTorch 库中的一个自适应平均池化层，其作用是将输入的多通道二维特征图转换为具有固定尺寸的特征图，具体来说，这里的输出尺寸被指定为 1x1。参数 <code>(1,1)</code> 指定了输出特征图的高度和宽度，即输出特征图的每个通道都会被压缩成一个单一的数值，这个数值是对应输入通道上所有像素值的平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#  定义NiN块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels,out_channels,kernel_size,strides,padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 参数个数： out_channels*out_channels*1*1</span></span><br><span class="line">        nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 定义NiN模型</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>,<span class="number">96</span>,kernel_size=<span class="number">11</span>,strides=<span class="number">4</span>,padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>,<span class="number">256</span>,kernel_size=<span class="number">5</span>,strides=<span class="number">1</span>,padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>,<span class="number">384</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nin_block(<span class="number">384</span>,<span class="number">10</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># 全局平均池化层，对每个通道内的二维矩阵分别求平均值</span></span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">    nn.Flatten()</span><br><span class="line"> )</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829171427362.png" alt="image-20240829171427362"></p>
<h2 id="含并行连结的网络（GoogLeNet）"><a href="#含并行连结的网络（GoogLeNet）" class="headerlink" title="含并行连结的网络（GoogLeNet）"></a>含并行连结的网络（GoogLeNet）</h2><p>Google做的，只是致敬LeNet</p>
<p>Inception块：四个路径从<strong>不同层面抽取信息</strong>，然后在输出通道维合并</p>
<p>高宽都不变，只有通道数改变</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829172033156.png" alt="image-20240829172033156"></p>
<p>以输入192X28X28的输入，观察通道数的变化，蓝色块抽取信息，白色块融合通道信息</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829172331822.png" alt="image-20240829172331822"></p>
<p>跟单3X3&#x2F;5X5卷积层相比，Inception块的参数更少</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829172903646.png" alt="image-20240829172903646"></p>
<p><strong>GoogLeNet架构</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829173050742.png" alt="image-20240829173050742"></p>
<p><strong>Inception的后续变种</strong></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829173953903.png" alt="image-20240829173953903"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829174424705.png" alt="image-20240829174424705"></p>
<p>当然这里并不是纯深度的上百层，这里有一点广度学习的思想</p>
<h3 id="代码实现-12"><a href="#代码实现-12" class="headerlink" title="代码实现"></a>代码实现</h3><p>注意到我们在前面NiN的模型中，使用函数去定义块，这里使用的是类，这是因为这里的块含有<strong>并行结构</strong>，需要我们<strong>定义新的forward函数</strong></p>
<p>在 Python 类的构造函数 <code>__init__</code> 中，<code>**kwargs</code> 表示“关键字参数”（keyword arguments），它允许你将不定数量的命名参数传递给函数。这些命名参数在函数内部以字典形式存在。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inception</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels,c1,c2,c3,c4,**kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1 1X1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels,c1,kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2 1X1卷积层+3X3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels,c2[<span class="number">0</span>],kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>],c2[<span class="number">1</span>],kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3 1X1卷积层+5X5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels,c3[<span class="number">0</span>],kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>],c3[<span class="number">1</span>],kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4 3X3最大池化层+1X1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels,c4,kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连接输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1,p2,p3,p4),dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># stage 1</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># stage 2</span></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>,<span class="number">192</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># stage 3</span></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>,<span class="number">64</span>,(<span class="number">96</span>,<span class="number">128</span>),(<span class="number">16</span>,<span class="number">32</span>),<span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>,<span class="number">128</span>,(<span class="number">128</span>,<span class="number">192</span>),(<span class="number">32</span>,<span class="number">96</span>),<span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># stage 4</span></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>,<span class="number">192</span>,(<span class="number">96</span>,<span class="number">208</span>),(<span class="number">16</span>,<span class="number">48</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>,<span class="number">160</span>,(<span class="number">112</span>,<span class="number">224</span>),(<span class="number">24</span>,<span class="number">64</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>,<span class="number">128</span>,(<span class="number">128</span>,<span class="number">256</span>),(<span class="number">24</span>,<span class="number">64</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>,<span class="number">112</span>,(<span class="number">144</span>,<span class="number">288</span>),(<span class="number">32</span>,<span class="number">64</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>,<span class="number">256</span>,(<span class="number">160</span>,<span class="number">320</span>),(<span class="number">32</span>,<span class="number">128</span>),<span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># stage 5</span></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>,<span class="number">256</span>,(<span class="number">160</span>,<span class="number">320</span>),(<span class="number">32</span>,<span class="number">128</span>),<span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>,<span class="number">384</span>,(<span class="number">192</span>,<span class="number">384</span>),(<span class="number">48</span>,<span class="number">128</span>),<span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整个模型：</span></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="comment"># 展示每一块的形状</span></span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829205212799.png" alt="image-20240829205212799"></p>
<h2 id="批量归一化（batch-norm"><a href="#批量归一化（batch-norm" class="headerlink" title="批量归一化（batch_norm)"></a>批量归一化（batch_norm)</h2><p>对于一个较深的神经网络</p>
<ul>
<li>上层梯度大，底层梯度小</li>
<li>数据在最底部</li>
<li>底层的训练较慢</li>
<li>底层一变化，所有都得跟着变</li>
<li>这导致顶部的层需要重新学习多次</li>
<li>导致收敛变慢</li>
</ul>
<p>批量归一化的核心思想是规范化（normalization）网络中的激活输出，使其具有固定的均值和方差。<br>$$<br>具体而言，批量归一化层计算这个批量的平均值\mathbf{u}和方差\mathbf{\sigma}\<br>然后对每一个输出进行规范化 \mathbf{x} &#x3D; \lambda\frac{\mathbf{x}-\mathbf{u}}{\sigma}+\mathbf{B}<br>$$<br>不考虑两个参数，归一化后，这一组数据的平均值为0，方差为1</p>
<p>式子中的lambda和B是可以学习的参数</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829211511069.png" alt="image-20240829211511069"></p>
<p>作用在特征维的意思是将对每一个特征（每一列）求平均值和方差</p>
<p>作用在通道维的意思是对每个通道的数据进行归一化，使得每个通道的数据的均值为0，方差为1，相当于一个通道看作一个特征</p>
<p><strong>批量归一化在做什么？</strong></p>
<ul>
<li><p>最初论文是想用它来减少内部协变量转移</p>
</li>
<li><p>后续有论文指出它的作用其实是可能是通过在每个小批量里加入随机噪音来控制模型复杂度</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829212429850.png" alt="image-20240829212429850"></p>
</li>
<li><p>因此没有必要和丢弃法混合使用</p>
</li>
</ul>
<p><strong>作用</strong>：可以允许较大的学习率，可<strong>加速收敛速度</strong>，一般不会改变模型的精度</p>
<h3 id="代码实现-13"><a href="#代码实现-13" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># eps被加到方差的计算中，以确保分母不会变得太小</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">banch_norm</span>(<span class="params">X,gamma,beta,moving_mean,moving_var,eps,momentum</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 预测模式</span></span><br><span class="line">        X_hat = (X-moving_mean)/torch.sqrt(moving_var+eps)</span><br><span class="line">    <span class="keyword">else</span> : <span class="comment"># 训练模式</span></span><br><span class="line">        <span class="comment"># 判断X是否为二维/四维数组</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 二维数组</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X-mean)**<span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 数据有四个维度0，1，2，3，dim=(0,2,3)的意思就是把第一三四维降到1，只保留第二维（核的输入通道）</span></span><br><span class="line">            <span class="comment"># 注意，这里只有mean和var的形状改变为（1，？,1,1)，最后Y的形状和输入的形状是相同的，只是进行了归一化</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X-mean)**<span class="number">2</span>).mean(dim=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdim=<span class="literal">True</span>)</span><br><span class="line">        X_hat = (X-mean)/torch.sqrt(var+eps)</span><br><span class="line">        <span class="comment"># 将当前批量的均值以一定的权重融入到了移动平均均值中。</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma*X_hat + beta</span><br><span class="line">    <span class="keyword">return</span> Y,moving_mean.data,moving_var.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># BatchNorm层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_features,num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>,num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>,num_features,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        Y,self.moving_mean,self.moving_var = banch_norm(X,self.gamma,self.beta,self.moving_mean,self.moving_var,eps=<span class="number">1e-5</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用到LeNet</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>), BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X = torch.rand(size=(1,1,28,28),dtype = torch.float32)</span></span><br><span class="line"><span class="string">for layer in net:</span></span><br><span class="line"><span class="string">    X = layer(X)</span></span><br><span class="line"><span class="string">    print(layer.__class__.__name__,&#x27;output shape:\t&#x27;,X.shape)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902205208733.png" alt="image-20240902205208733"></p>
<h2 id="残差网络-ResNet"><a href="#残差网络-ResNet" class="headerlink" title="残差网络(ResNet)"></a>残差网络(ResNet)</h2><p>在下图中，星星是我们想要学习到的最优点，每一块区域是我们可以学到的模型的范围</p>
<p>范围越大，模型越复杂</p>
<p>可以看到，在左图中，随着模型越来越复杂，可学习到的最优点离星星的距离并不会更近，而右图则越来越近</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902210226767.png" alt="image-20240902210226767"></p>
<p>ResNet的核心思想是，将原先的小网络作为新网络的子集，即<br>$$<br>\mathbf{f}(x) &#x3D; \mathbf{g}(x)+x \<br>x代表原先小网络的输出结果<br>$$<br>这种架构称为跳跃连接</p>
<p>ResNet引入了“残差学习”的概念，即网络学习的是输入和输出之间的残差（或差异），而不是直接学习输出。这意味着网络的每个层不仅尝试映射输入到输出，而且尝试映射输入到输出与输入之间的差异。</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902210631163.png" alt="image-20240902210631163"></p>
<p>ResNet块</p>
<ul>
<li>第一块高宽减半</li>
<li>后接多个高宽不变的ResNet块</li>
<li>每一块都有一个跳跃连接</li>
</ul>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902212124537.png" alt="image-20240902212124537"></p>
<h3 id="代码实现-14"><a href="#代码实现-14" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,input_channels,num_channels,use_1x1conv=<span class="literal">False</span>,stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels,num_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>,stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels,num_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels,num_channels,kernel_size=<span class="number">1</span>,stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ResNet 模型</span></span><br><span class="line"><span class="comment"># 前两层比较常规</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span><br><span class="line"><span class="params">                 first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="comment"># 对于b3,b4，b5的第一个ResNet块，我们希望它具有高宽减半的效果</span></span><br><span class="line">        <span class="comment"># 对于b2，由于前面两层已经采取了高宽减半，所以这里做特殊处理</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902214550047.png" alt="image-20240902214550047"></p>
<p><strong>为什么ResNet可以训练上千层的模型？</strong></p>
<ul>
<li>乘法变加法，可以缓解梯度消失。在f(X)+X中，梯度消失的一方会被另一方取代。</li>
<li><strong>恒等快捷连接</strong>：ResNet通过引入恒等快捷连接（Identity Shortcut Connections），即在网络中添加跳过一层或多层的连接，使得梯度可以直接传播到前面的层。这种设计有效地解决了梯度消失问题，因为它允许梯度在反向传播过程中直接流向网络的较早层级。</li>
<li>加深模型可以退化为浅层模型</li>
<li>梯度高速通道，因为有高速通道的存在，底层拿到的梯度也会比较大，收敛速度会更快</li>
</ul>
<h1 id="实战：Kaggle图片分类"><a href="#实战：Kaggle图片分类" class="headerlink" title="实战：Kaggle图片分类"></a>实战：Kaggle图片分类</h1><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/classify-leaves">https://www.kaggle.com/c/classify-leaves</a></p>
<p>1、关于pd.read_csv函数中header参数的解释：</p>
<p>默认为0，表示第0行为表头，这样read_csv以后，DataFrame数据中仍然有表头，但是用iloc索引第0行得到的就是数据了</p>
<p>如果设定header&#x3D;None，表示没有表头，这样得到的数据中第一行为表头</p>
<p>2、用DataFrame.iloc索引数据的时候，第0列就是数据列，不会包含id列</p>
<p>3、保存模型的时候，有两种后缀</p>
<ul>
<li>.pth: PyTorch 中最常见的模型文件后缀之一，它通常用来保存模型的权重参数（<code>state_dict</code>）。这种方式只保存模型的参数，不保存模型的结构</li>
<li><code>.ckpt</code> 后缀通常用于保存模型的检查点（checkpoint），它不仅包含模型的权重参数，还可能包含优化器状态、当前epoch、训练历史等信息。这种文件通常用于训练过程中的断点续训，以便在训练过程中断时可以从最近的检查点恢复训练。</li>
</ul>
<p>4、..argmax(dim&#x3D;-1)可以返回每一行数值最大处的列索引</p>
<p>5、.append 函数和.extend函数的区别：</p>
<ul>
<li>append：如果添加的是一个列表，那么这个列表会被作为一个整体添加为一个元素</li>
<li>extend：如果可迭代对象是一个列表，那么它的所有元素将被展开并添加到原列表中</li>
</ul>
<h1 id="ch07-硬件介绍"><a href="#ch07-硬件介绍" class="headerlink" title="ch07_硬件介绍"></a>ch07_硬件介绍</h1><p>参考《动手学深度学习》p31-p35</p>
<h1 id="ch08-计算机视觉"><a href="#ch08-计算机视觉" class="headerlink" title="ch08_计算机视觉"></a>ch08_计算机视觉</h1><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://howillmakeit.github.io">HOWILL</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://howillmakeit.github.io" target="_blank">HOWILL'S WORLD</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/img/434052135_1607214906781670_1513540648802057013_n.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/21/mysql-0/" title="mysql"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">mysql</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="数据结构（更新完毕）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据结构（更新完毕）</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/434052135_1607214906781670_1513540648802057013_n.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HOWILL</div><div class="author-info__description">just a blog </div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HOWILLMAKEIT"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HOWILLMAKEIT" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ch01-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">ch01_预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">1.0.1.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-number">1.0.1.1.</span> <span class="toc-text">索引与切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">1.0.1.2.</span> <span class="toc-text">创建张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E5%BC%A0%E9%87%8F"><span class="toc-number">1.0.1.3.</span> <span class="toc-text">访问张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E5%BC%A0%E9%87%8F"><span class="toc-number">1.0.1.4.</span> <span class="toc-text">修改张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97"><span class="toc-number">1.0.1.5.</span> <span class="toc-text">张量运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9E%E7%BB%93%E5%BC%A0%E9%87%8F"><span class="toc-number">1.0.1.6.</span> <span class="toc-text">连结张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">1.0.1.7.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96"><span class="toc-number">1.0.1.8.</span> <span class="toc-text">类型转化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.0.2.</span> <span class="toc-text">数据预处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.1.1.</span> <span class="toc-text">线性代数实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.2.</span> <span class="toc-text">张量算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">降维求和</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">非降维求和</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E5%80%BC"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">平均值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%82%B9%E7%A7%AF"><span class="toc-number">1.1.2.4.</span> <span class="toc-text">点积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-number">1.1.2.5.</span> <span class="toc-text">矩阵-向量积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">1.1.2.6.</span> <span class="toc-text">矩阵-矩阵乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">1.1.2.7.</span> <span class="toc-text">范数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="toc-number">1.2.</span> <span class="toc-text">矩阵计算*</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-number">1.3.</span> <span class="toc-text">自动求导*</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">1.3.1.</span> <span class="toc-text">分离计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Python%E6%8E%A7%E5%88%B6%E6%B5%81%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.3.2.</span> <span class="toc-text">Python控制流的梯度计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ch02-%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">ch02_线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0"><span class="toc-number">2.1.0.1.</span> <span class="toc-text">基本元素</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.0.2.</span> <span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.0.3.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DSGD"><span class="toc-number">2.1.0.4.</span> <span class="toc-text">随机梯度下降SGD</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.1.</span> <span class="toc-text">线性回归从0开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.1.3.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.1.4.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.1.5.</span> <span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">2.1.1.6.</span> <span class="toc-text">定义优化算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.1.1.7.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.2.</span> <span class="toc-text">线性回归的简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">2.2.</span> <span class="toc-text">softmax回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.2.1.</span> <span class="toc-text">图像分类数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E7%BB%84%E4%BB%B6"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">整合组件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.2.2.</span> <span class="toc-text">softmax回归从0开始实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.2.3.</span> <span class="toc-text">softmax回归的简洁实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ch03-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BAMLP"><span class="toc-number">3.</span> <span class="toc-text">ch03_多层感知机MLP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">3.1.</span> <span class="toc-text">感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-number">3.1.1.</span> <span class="toc-text">单隐藏层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">单类分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">多类分类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.2.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-number">3.1.3.</span> <span class="toc-text">多隐藏层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.1.4.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">3.1.4.1.</span> <span class="toc-text">单隐藏层多分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">3.1.4.2.</span> <span class="toc-text">多隐藏层多分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.1.4.3.</span> <span class="toc-text">简洁实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">3.2.</span> <span class="toc-text">模型选择-过拟合与欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="toc-number">3.3.</span> <span class="toc-text">权重衰退</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">3.3.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text">丢弃法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">3.4.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.5.</span> <span class="toc-text">数值稳定性和模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Xavier%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.5.1.</span> <span class="toc-text">Xavier初始化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E6%88%98%EF%BC%9AKaggle%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-number">4.</span> <span class="toc-text">实战：Kaggle房价预测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.0.1.</span> <span class="toc-text">获取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="toc-number">4.0.2.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">4.0.3.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">4.0.4.</span> <span class="toc-text">模型选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4Kaggle%E9%A2%84%E6%B5%8B"><span class="toc-number">4.0.5.</span> <span class="toc-text">提交Kaggle预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ch04-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97"><span class="toc-number">5.</span> <span class="toc-text">ch04_深度学习计算</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0"><span class="toc-number">5.1.</span> <span class="toc-text">模型构造</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="toc-number">5.2.</span> <span class="toc-text">参数管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E5%8F%82%E6%95%B0"><span class="toc-number">5.2.1.</span> <span class="toc-text">访问参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="toc-number">5.2.2.</span> <span class="toc-text">初始化参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="toc-number">5.3.</span> <span class="toc-text">自定义层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6"><span class="toc-number">5.4.</span> <span class="toc-text">读取文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#read-and-write-%E5%BC%A0%E9%87%8F"><span class="toc-number">5.4.1.</span> <span class="toc-text">read_and_write_张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#read-and-write-%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.4.2.</span> <span class="toc-text">read_and_write_模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPU%E4%BD%BF%E7%94%A8"><span class="toc-number">5.5.</span> <span class="toc-text">GPU使用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ch05-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">ch05_卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%88%B0%E5%8D%B7%E7%A7%AF"><span class="toc-number">6.1.</span> <span class="toc-text">从全连接层到卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">6.2.</span> <span class="toc-text">卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-4"><span class="toc-number">6.2.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">6.3.</span> <span class="toc-text">卷积层的填充和步幅</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-5"><span class="toc-number">6.3.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">6.4.</span> <span class="toc-text">输入和输出通道</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-6"><span class="toc-number">6.4.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">6.5.</span> <span class="toc-text">池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-7"><span class="toc-number">6.5.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-LeNet"><span class="toc-number">6.6.</span> <span class="toc-text">卷积神经网络_LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-8"><span class="toc-number">6.6.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ch06-%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text">ch06_现代卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88AlexNet%EF%BC%89"><span class="toc-number">7.1.</span> <span class="toc-text">深度卷积神经网络（AlexNet）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-9"><span class="toc-number">7.1.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88VGG%EF%BC%89"><span class="toc-number">7.2.</span> <span class="toc-text">使用块的网络（VGG）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-10"><span class="toc-number">7.2.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88NiN%EF%BC%89"><span class="toc-number">7.3.</span> <span class="toc-text">网络中的网络（NiN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-11"><span class="toc-number">7.3.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88GoogLeNet%EF%BC%89"><span class="toc-number">7.4.</span> <span class="toc-text">含并行连结的网络（GoogLeNet）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-12"><span class="toc-number">7.4.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88batch-norm"><span class="toc-number">7.5.</span> <span class="toc-text">批量归一化（batch_norm)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-13"><span class="toc-number">7.5.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C-ResNet"><span class="toc-number">7.6.</span> <span class="toc-text">残差网络(ResNet)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-14"><span class="toc-number">7.6.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E6%88%98%EF%BC%9AKaggle%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB"><span class="toc-number">8.</span> <span class="toc-text">实战：Kaggle图片分类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ch07-%E7%A1%AC%E4%BB%B6%E4%BB%8B%E7%BB%8D"><span class="toc-number">9.</span> <span class="toc-text">ch07_硬件介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ch08-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="toc-number">10.</span> <span class="toc-text">ch08_计算机视觉</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83"><span class="toc-number">10.1.</span> <span class="toc-text">微调</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/21/mysql-0/" title="mysql">mysql</a><time datetime="2024-09-20T23:09:52.000Z" title="发表于 2024-09-21 07:09:52">2024-09-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="动手学习深度学习">动手学习深度学习</a><time datetime="2024-07-25T05:47:20.000Z" title="发表于 2024-07-25 13:47:20">2024-07-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="数据结构（更新完毕）">数据结构（更新完毕）</a><time datetime="2024-07-09T03:16:06.000Z" title="发表于 2024-07-09 11:16:06">2024-07-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/" title="乌萨奇通讯录项目报告">乌萨奇通讯录项目报告</a><time datetime="2024-06-17T03:22:06.000Z" title="发表于 2024-06-17 11:22:06">2024-06-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By HOWILL</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'hTBg743qGPvcDLWFJT7BFooo-gzGzoHsz',
      appKey: 'ChcUQ7xit2zqaqHtFQ468Gnu',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>