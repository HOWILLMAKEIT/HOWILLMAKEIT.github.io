<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>动手学习深度学习 | HOWILL'S WORLD</title><meta name="author" content="HOWILL"><meta name="copyright" content="HOWILL"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据操作 + 数据预处理N维数组是机器学习和神经网络学习的主要数据结构 创建数组的需要：形状、数据类型、每个元素的值 张量索引与切片行和列都是从0开始，-1可以表示最后一行or最后一列 访问元素: 例如[1:3,1:]表示访问第1、2行以及从第一列开始的所有列  [::3,::2]	 	::3表示切片，每三行取一行  [3]表示第三行	[,3]表示第三列 创建张量torch.arange(12)#">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学习深度学习">
<meta property="og:url" content="https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="HOWILL&#39;S WORLD">
<meta property="og:description" content="数据操作 + 数据预处理N维数组是机器学习和神经网络学习的主要数据结构 创建数组的需要：形状、数据类型、每个元素的值 张量索引与切片行和列都是从0开始，-1可以表示最后一行or最后一列 访问元素: 例如[1:3,1:]表示访问第1、2行以及从第一列开始的所有列  [::3,::2]	 	::3表示切片，每三行取一行  [3]表示第三行	[,3]表示第三列 创建张量torch.arange(12)#">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://howillmakeit.github.io/img/434052135_1607214906781670_1513540648802057013_n.jpg">
<meta property="article:published_time" content="2024-07-25T05:47:20.000Z">
<meta property="article:modified_time" content="2024-07-31T13:31:01.769Z">
<meta property="article:author" content="HOWILL">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://howillmakeit.github.io/img/434052135_1607214906781670_1513540648802057013_n.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '动手学习深度学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-31 21:31:01'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/434052135_1607214906781670_1513540648802057013_n.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/background.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="HOWILL'S WORLD"><span class="site-name">HOWILL'S WORLD</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">动手学习深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-25T05:47:20.000Z" title="发表于 2024-07-25 13:47:20">2024-07-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-31T13:31:01.769Z" title="更新于 2024-07-31 21:31:01">2024-07-31</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="动手学习深度学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="数据操作-数据预处理"><a href="#数据操作-数据预处理" class="headerlink" title="数据操作 + 数据预处理"></a>数据操作 + 数据预处理</h2><p>N维数组是机器学习和神经网络学习的<strong>主要数据结构</strong></p>
<p>创建数组的需要：形状、数据类型、每个元素的值</p>
<h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h4><p>行和列都是从0开始，-1可以表示最后一行or最后一列</p>
<p>访问元素: 例如[1:3,1:]表示访问第1、2行以及从第一列开始的所有列</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240726170521499.png" alt="image-20240726170521499"></p>
<p>[::3,::2]	 	::3表示切片，每三行取一行</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240726170626631.png" alt="image-20240726170626631"></p>
<p>[3]表示第三行	[,3]表示第三列</p>
<h4 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">12</span>)</span><br><span class="line"><span class="comment">#随机</span></span><br><span class="line">torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">#全0</span></span><br><span class="line">torch.zeros((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240726172228760.png" alt="image-20240726172228760"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#赋值</span></span><br><span class="line">torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Z的shape和数据类型都与y相同</span></span><br><span class="line">Z = torch.zeros_like(Y)</span><br></pre></td></tr></table></figure>



<h4 id="访问张量"><a href="#访问张量" class="headerlink" title="访问张量"></a>访问张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape <span class="comment">#torch.size([12])</span></span><br><span class="line">x.numel() <span class="comment">#12</span></span><br></pre></td></tr></table></figure>

<h4 id="修改张量"><a href="#修改张量" class="headerlink" title="修改张量"></a>修改张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X= x.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">X.shape <span class="comment">#torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<h4 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h4><p>加减乘除求幂</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">x + y, x - y, x * y, x / y, x ** y  <span class="comment"># **运算符是求幂运算</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>, <span class="number">10.</span>]),</span><br><span class="line"> tensor([-<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">6.</span>]),</span><br><span class="line"> tensor([ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">16.</span>]),</span><br><span class="line"> tensor([<span class="number">0.5000</span>, <span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">4.0000</span>]),</span><br><span class="line"> tensor([ <span class="number">1.</span>,  <span class="number">4.</span>, <span class="number">16.</span>, <span class="number">64.</span>]))</span><br></pre></td></tr></table></figure>

<p>更多计算</p>
<p>指数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure>

<p>求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()<span class="comment">#结果是一个元素</span></span><br></pre></td></tr></table></figure>

<h4 id="连结张量"><a href="#连结张量" class="headerlink" title="连结张量"></a>连结张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), <span class="comment">#在行上合并</span></span><br><span class="line">torch.cat((X, Y), dim=<span class="number">1</span>)  <span class="comment">#在列上合并		</span></span><br></pre></td></tr></table></figure>

<h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p><strong>即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作</strong>]。 这种机制的工作方式如下：</p>
<ol>
<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>
<li>对生成的数组执行按元素操作。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br><span class="line"><span class="comment">########################</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a + b</span><br><span class="line"><span class="comment">########################</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<h4 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tensor-&gt;numpy</span></span><br><span class="line">A = X.numpy()</span><br><span class="line"><span class="comment">#numpy-&gt;tensor</span></span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="comment">#当tensor大小为1时，tensor-&gt;python标量 </span></span><br><span class="line">a= torch.tensor([<span class="number">3.5</span>])</span><br></pre></td></tr></table></figure>





<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>本节将简要介绍使用<code>pandas</code>预处理原始数据，并将原始数据转换为张量格式的步骤。 后面的章节将介绍更多的数据预处理技术。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># &#x27;..&#x27;代表本目录的父目录</span></span><br><span class="line"><span class="comment"># exist_ok=True表示假如已经存在data目录，不会重新创建且不会报错</span></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># data_file存储data目录下文件house_tiny.csv的路径</span></span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 用write的方式打开</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>这里缺失了数据，这里我们考虑插值法处理</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727114348965.png" alt="image-20240727114348965"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="comment"># inputs获取第0、1列 outputs获取第2列</span></span><br><span class="line"><span class="comment"># .iloc是pandas库中基于位置进行索引的函数</span></span><br><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, -<span class="number">1</span>]</span><br><span class="line"><span class="comment"># .mean用于获取均值，numeric_only=True表示只考虑数值</span></span><br><span class="line">inputs = inputs.fillna(inputs.mean(numeric_only=<span class="literal">True</span>))</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727114906568.png" alt="image-20240727114906568"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建虚拟变量  dummy_na=True表示要将缺失值（NaN）转换为一个新的虚拟变量</span></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727115335144.png" alt="image-20240727115335144"></p>
<p>转化为张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727115521841.png" alt="image-20240727115521841"></p>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h3 id="线性代数实现"><a href="#线性代数实现" class="headerlink" title="线性代数实现"></a>线性代数实现</h3><p><strong>标量</strong>：用只有一个元素的张量表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br></pre></td></tr></table></figure>

<p><strong>向量</strong>：<strong>标量值组成的列表</strong></p>
<p>认为列向量是向量的默认方向</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>用索引访问到的当个元素也是张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br><span class="line"><span class="comment"># tensor(3)</span></span><br></pre></td></tr></table></figure>

<p>向量的长度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(x)</span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure>

<p><strong>矩阵</strong>：用m*n的张量表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>转制</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure>

<p>对于对称矩阵，满足：</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727153227299.png" alt="image-20240727153227299"></p>
<p><strong>张量</strong>：[<strong>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</strong>]。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727153540597.png" alt="image-20240727153540597"></p>
<h3 id="张量算法"><a href="#张量算法" class="headerlink" title="张量算法"></a>张量算法</h3><p>两个矩阵按元素相乘*<strong>Hadamard积*</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure>

<h4 id="降维求和"><a href="#降维求和" class="headerlink" title="降维求和"></a>降维求和</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以[<strong>指定张量沿哪一个轴来通过求和降低维度</strong>]。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，<strong>因此输入轴0的维数在输出形状中消失</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 沿着第0轴进行求和</span></span><br><span class="line"><span class="comment"># A是一个5*4的矩阵</span></span><br><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line">(tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), torch.Size([<span class="number">4</span>]))</span><br></pre></td></tr></table></figure>

<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727155109468.png" alt="image-20240727155109468"></p>
<p>也可以沿着两个维度求和</p>
<p>例如下列代码对X沿着0轴和1轴求和</p>
<p>可以理解为先让两个3*4矩阵相加，得到一个3 * 4矩阵</p>
<p>然后将每一个长度为4的向量相加，得到tensor([60, 66, 72, 78])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">X_sum_axis01 = X.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">X_sum_axis01</span><br><span class="line"></span><br><span class="line">(tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">          [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">          [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">          [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">          [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]]),</span><br><span class="line">tensor([<span class="number">60</span>, <span class="number">66</span>, <span class="number">72</span>, <span class="number">78</span>])</span><br></pre></td></tr></table></figure>

<h4 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">A_sum_axis1,sum_A</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 降维求和结果</span></span><br><span class="line">(tensor([ <span class="number">6.</span>, <span class="number">22.</span>, <span class="number">38.</span>, <span class="number">54.</span>, <span class="number">70.</span>]),</span><br><span class="line"> <span class="comment"># 非降维求和结果，5*1，还是两个维度</span></span><br><span class="line"> tensor([[ <span class="number">6.</span>],</span><br><span class="line">         [<span class="number">22.</span>],</span><br><span class="line">         [<span class="number">38.</span>],</span><br><span class="line">         [<span class="number">54.</span>],</span><br><span class="line">         [<span class="number">70.</span>]]))</span><br></pre></td></tr></table></figure>



<p>如果我们想沿[<strong>某个轴计算<code>A</code>元素的累积总和</strong>]， 比如<code>axis=0</code>（按行计算），可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A,A.cumsum(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>],</span><br><span class="line">         [<span class="number">24.</span>, <span class="number">28.</span>, <span class="number">32.</span>, <span class="number">36.</span>],</span><br><span class="line">         [<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]]))</span><br></pre></td></tr></table></figure>





<h4 id="平均值"><a href="#平均值" class="headerlink" title="平均值"></a>平均值</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean() , A.<span class="built_in">sum</span>() / A.numel()</span><br></pre></td></tr></table></figure>

<p>同样，计算平均值的函数也可以沿指定轴降低张量的维度</p>
<p>例如，求A沿着0轴方向的平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<h4 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.dot(x, y)</span><br></pre></td></tr></table></figure>

<p><strong>我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure>



<h4 id="矩阵-向量积"><a href="#矩阵-向量积" class="headerlink" title="矩阵-向量积"></a>矩阵-向量积</h4><p>例如A是一个m*n的矩阵，x是一个长度为n的向量</p>
<p>则A与x的积为一个长度为m的列向量，其中第i个元素为矩阵中第i行向量与x的点积</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240727162328237.png" alt="image-20240727162328237"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A, x)</span><br><span class="line"></span><br><span class="line">(torch.Size([<span class="number">5</span>, <span class="number">4</span>]), torch.Size([<span class="number">4</span>]), tensor([ <span class="number">14.</span>,  <span class="number">38.</span>,  <span class="number">62.</span>,  <span class="number">86.</span>, <span class="number">110.</span>]))</span><br></pre></td></tr></table></figure>



<h4 id="矩阵-矩阵乘法"><a href="#矩阵-矩阵乘法" class="headerlink" title="矩阵-矩阵乘法"></a>矩阵-矩阵乘法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.mm(A, B)</span><br></pre></td></tr></table></figure>



<h4 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h4><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240730120311633.png" alt="image-20240730120311633"></p>
<p>L2范数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure>

<p>类似于向量的L2范数，Frobenius范数（Frobenius norm）是矩阵元素平方和的平方根，也可以用torch.norm计算</p>
<p>L1范数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>在深度学习中，我们经常试图解决优化问题： <em>最大化</em>分配给观测数据的概率; <em>最小化</em>预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</p>
<h2 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算*"></a>矩阵计算*</h2><p>主要是讲矩阵如何求导数</p>
<p>基础知识：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/263777564">矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)</a></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240731104153413.png" alt="image-20240731104153413"></p>
<p>向量关于向量求导的结果是一个矩阵</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240731104815010.png" alt="image-20240731104815010"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240731105049555.png" alt="image-20240731105049555"></p>
<p>右下角括号内的数字是：</p>
<p>(m,l,k,n)</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240731110635856.png" alt="image-20240731110635856"></p>
<h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导*"></a>自动求导*</h2><p>原理：</p>
<p>构造计算图</p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240731112300974.png" alt="image-20240731112300974"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240731112539492.png" alt="image-20240731112539492"></p>
<p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240731112917005.png" alt="image-20240731112917005"></p>
<p>反向累积：</p>
<p>时间复杂度：O(n) 通常正向和反向的代价类似</p>
<p>空间复杂度O(n),需要存储正向的所有中间结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># requires_grad=True表示需要存储梯度的数值</span></span><br><span class="line"><span class="comment"># x.grad就可以访问这个存储的梯度</span></span><br><span class="line">x=torch.arange(<span class="number">4.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">12.</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_() <span class="comment"># 清空梯度</span></span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>



<p>向量对向量求导，结果应该是一个矩阵，但是</p>
<p><strong>深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class="line"><span class="comment"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x </span><br><span class="line"><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>



<h3 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h3><p>有时，我们希望[<strong>将某些计算移动到记录的计算图之外</strong>]。 </p>
<p>假如，y是作为x的函数被计算的，z则作为y和x的函数，但是，我们希望将y视为一个常数，不希望对z求关于x 的导数时，梯度经y流到x</p>
<p>解决方案是分离y，返回一个新变量u，u丢弃计算图中如何计算y的任何信息</p>
<p>在下列代码中，求的是Z&#x3D;u * x的偏导数，而不是Z&#x3D;x* * x * x的偏导数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u</span><br><span class="line"></span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<h3 id="Python控制流的梯度计算"><a href="#Python控制流的梯度计算" class="headerlink" title="Python控制流的梯度计算"></a>Python控制流的梯度计算</h3><p>使用自动微分的一个好处是： [<strong>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</strong>]。 在下面的代码中，<code>while</code>循环的迭代次数和<code>if</code>语句的结果都取决于输入<code>a</code>的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># randn用于创建服从标准正态分布的张量</span></span><br><span class="line"><span class="comment"># size=()表示创建的是标量</span></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>

<p><code>f</code>函数在其输入<code>a</code>中是分段线性的。 换言之，对于任何<code>a</code>，存在某个常量标量<code>k</code>，使得<code>f(a)=k*a</code>，其中<code>k</code>的值取决于输入<code>a</code>，因此可以用<code>d/a</code>验证梯度是否正确。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.grad == d / a</span><br><span class="line"></span><br><span class="line">tensor(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>





<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://howillmakeit.github.io">HOWILL</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">https://howillmakeit.github.io/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://howillmakeit.github.io" target="_blank">HOWILL'S WORLD</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/img/434052135_1607214906781670_1513540648802057013_n.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/26/leetcode%E5%88%B7%E9%A2%98%E6%97%A5%E8%AE%B0/" title="leetcode刷题日记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">leetcode刷题日记</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/25/python/" title="python学习记录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">python学习记录</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/434052135_1607214906781670_1513540648802057013_n.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HOWILL</div><div class="author-info__description">just a blog </div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HOWILLMAKEIT"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HOWILLMAKEIT" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">数据操作 + 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-number">1.1.1.</span> <span class="toc-text">索引与切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.2.</span> <span class="toc-text">创建张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.3.</span> <span class="toc-text">访问张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.4.</span> <span class="toc-text">修改张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97"><span class="toc-number">1.1.5.</span> <span class="toc-text">张量运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9E%E7%BB%93%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.6.</span> <span class="toc-text">连结张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.7.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96"><span class="toc-number">1.1.8.</span> <span class="toc-text">类型转化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">数据预处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.</span> <span class="toc-text">线性代数实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%AE%97%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">张量算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-number">2.2.1.</span> <span class="toc-text">降维求和</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-number">2.2.2.</span> <span class="toc-text">非降维求和</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E5%80%BC"><span class="toc-number">2.2.3.</span> <span class="toc-text">平均值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%82%B9%E7%A7%AF"><span class="toc-number">2.2.4.</span> <span class="toc-text">点积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-number">2.2.5.</span> <span class="toc-text">矩阵-向量积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">2.2.6.</span> <span class="toc-text">矩阵-矩阵乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">2.2.7.</span> <span class="toc-text">范数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="toc-number">3.</span> <span class="toc-text">矩阵计算*</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-number">4.</span> <span class="toc-text">自动求导*</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">4.1.</span> <span class="toc-text">分离计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Python%E6%8E%A7%E5%88%B6%E6%B5%81%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">4.2.</span> <span class="toc-text">Python控制流的梯度计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.</span> <span class="toc-text">线性回归</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/31/%E3%80%8A%E7%BC%96%E7%A8%8B%E7%8F%A0%E7%8E%91/" title="《编程珠玑》阅读与思考">《编程珠玑》阅读与思考</a><time datetime="2024-07-31T11:30:41.000Z" title="发表于 2024-07-31 19:30:41">2024-07-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/26/leetcode%E5%88%B7%E9%A2%98%E6%97%A5%E8%AE%B0/" title="leetcode刷题日记">leetcode刷题日记</a><time datetime="2024-07-26T05:17:17.000Z" title="发表于 2024-07-26 13:17:17">2024-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="动手学习深度学习">动手学习深度学习</a><time datetime="2024-07-25T05:47:20.000Z" title="发表于 2024-07-25 13:47:20">2024-07-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/25/python/" title="python学习记录">python学习记录</a><time datetime="2024-07-25T01:55:20.000Z" title="发表于 2024-07-25 09:55:20">2024-07-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="数据结构（更新完毕）">数据结构（更新完毕）</a><time datetime="2024-07-09T03:16:06.000Z" title="发表于 2024-07-09 11:16:06">2024-07-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By HOWILL</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'hTBg743qGPvcDLWFJT7BFooo-gzGzoHsz',
      appKey: 'ChcUQ7xit2zqaqHtFQ468Gnu',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>