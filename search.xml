<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>mysql</title>
      <link href="/2024/09/21/mysql-0/"/>
      <url>/2024/09/21/mysql-0/</url>
      
        <content type="html"><![CDATA[<h1 id="MYSQL"><a href="#MYSQL" class="headerlink" title="MYSQL"></a>MYSQL</h1><h2 id="关系型数据管理系统"><a href="#关系型数据管理系统" class="headerlink" title="关系型数据管理系统"></a>关系型数据管理系统</h2><p>在关系型数据库中，<strong>所有数据都以表格的形式</strong>存储，每个表格被称为一个“关系”。每个表由列（字段）和行（记录）组成，每一行代表一个数据项，每一列代表数据项的一个属性。</p><p><strong>不同的表格之间通过关联字段来建立联系</strong></p><p>SQL是一种编程语言，几乎所有的关系型数据库都支持SQL语句</p><p>SQL语句<strong>并不区分大小写，但是为了提高可读性，</strong></p><p>人们会把关键字大写，表名列名和其他名称小写</p><p>各种关键字</p><p><img src="/2024/09/21/mysql-0/image-20240507130228975.png" alt="image-20240507130228975"></p><h2 id="可视化工具"><a href="#可视化工具" class="headerlink" title="可视化工具"></a>可视化工具</h2><p><strong>MYSQL workbench</strong> </p><p><strong>教程：</strong></p><p>【MySql Workbench 基本使用（数据库及表的创建、数据导入导出、用户和权限等）】<a href="https://www.bilibili.com/video/BV1Qg41127LE?vd_source=50cd1a4c87dfd3bdc7a70a5d22a771ed">https://www.bilibili.com/video/BV1Qg41127LE?vd_source=50cd1a4c87dfd3bdc7a70a5d22a771ed</a></p><p>遇到的第一个问题：</p><p><a href="https://blog.csdn.net/houor/article/details/124976076">修复MySQL Workbench不能访问Server Status的问题_mysqlbench server status-CSDN博客</a></p><h3 id="SCHEMA和DATABASE-的关系"><a href="#SCHEMA和DATABASE-的关系" class="headerlink" title="SCHEMA和DATABASE 的关系"></a>SCHEMA和DATABASE 的关系</h3><p><a href="https://blog.51cto.com/u_16213631/8369324">SCHEMA和database的关系mysql database与schema的区别_mob64ca1403c772的技术博客_51CTO博客</a></p><p>SCHEMA可以理解成是把DATABASE里面的东西分类存放好</p><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><p><a href="https://developer.baidu.com/article/detail.html?id=2343117">使用MySQL Workbench创建数据库-百度开发者中心 (baidu.com)</a></p><p>在 MySQL 中，模式是数据库的同义词。创建新模式也意味着创建新数据库。</p><p>也就是说</p><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><img src="/2024/09/21/mysql-0/image-20240507165548719.png" alt="image-20240507165548719" style="zoom: 200%;"><p>主键不为空唯一</p><p><strong>PK</strong>：Primary Key，表示该字段是主键。</p><p><strong>NN</strong>：Not Null，表示该字段不能为空。</p><p><strong>UQ</strong>：Unique，表示该字段的值必须是唯一的。</p><p><strong>B</strong>Binary 用于指定字符类型的字段是否区分大小写</p><p><strong>AI</strong>：Auto Increment，表示该字段的值会自动增加。</p><p><strong>UN</strong>：Unsigned，表示该字段的值是非负的（通常用于整数类型）。</p><p><strong>ZF</strong>：Zero Fill，表示该字段在显示时会在前面填充零（通常用于整数类型）</p><p><strong>G</strong>：Generated Column 生成列</p><p>生成列是一种特殊的列，它的值是基于表中其他列的值通过某种计算或表达式自动生成的。生成列可以是虚拟的（<code>VIRTUAL</code>），也可以是存储的（<code>STORED</code>）。虚拟生成列的值是在查询时计算得出的，而存储生成列的值是在插入或更新记录时计算并存储在数据库中的。</p><h3 id="查询器"><a href="#查询器" class="headerlink" title="查询器"></a>查询器</h3><p><img src="/2024/09/21/mysql-0/image-20240507170937154.png" alt="image-20240507170937154"></p><p>这个Query1就是查询器，可以运行所有SQL代码</p><img src="/2024/09/21/mysql-0/image-20240507171145751.png" alt="image-20240507171145751" style="zoom:200%;"><p>执行选中部分（不选中执行全部）执行光标所在行</p><p>ctrl + s 可以生成脚本</p><p><img src="/2024/09/21/mysql-0/image-20240507171548773.png" alt="image-20240507171548773"></p><p>收藏按钮，对于那些经常使用的代码，可以收藏起来，会显示在右边随时使用。</p><h3 id="数据导出与恢复"><a href="#数据导出与恢复" class="headerlink" title="数据导出与恢复"></a>数据导出与恢复</h3><h3 id="在QT中连接数据库"><a href="#在QT中连接数据库" class="headerlink" title="在QT中连接数据库"></a>在QT中连接数据库</h3><p><a href="https://blog.csdn.net/fengyezhen66/article/details/48656707">QT5.3连接mysql数据库_mysql workbench查看qt数据库-CSDN博客</a></p><p><img src="/2024/09/21/mysql-0/image-20240507175827112.png" alt="image-20240507175827112"></p><p>HostName:localhost</p><p>username:root</p><p>setpassword: qazwsxcde</p><p>setDatabaseName ( )</p><p>借助可视化软件以后，连接逻辑和本地数据库不太相同</p><p>因为这些软件不仅为本地数据库服务，也为外部数据库服务</p><p>新的连接逻辑</p><p><img src="/2024/09/21/mysql-0/image-20240507174243263.png" alt="image-20240507174243263"></p><p>运行代码后就算连接成功了，在QT中做出的改变会自动同步到workbench中</p><p>如何连接多个数据库</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">QSqlDatabase db1 = QSqlDatabase::<span class="built_in">addDatabase</span>(<span class="string">&quot;QMYSQL&quot;</span>, <span class="string">&quot;first_db_connection&quot;</span>);</span><br><span class="line">db1.<span class="built_in">setHostName</span>(<span class="string">&quot;host1&quot;</span>);</span><br><span class="line">db1.<span class="built_in">setDatabaseName</span>(<span class="string">&quot;database1&quot;</span>);</span><br><span class="line">db1.<span class="built_in">setUserName</span>(<span class="string">&quot;user1&quot;</span>);</span><br><span class="line">db1.<span class="built_in">setPassword</span>(<span class="string">&quot;password1&quot;</span>);</span><br><span class="line"></span><br><span class="line">QSqlDatabase db2 = QSqlDatabase::<span class="built_in">addDatabase</span>(<span class="string">&quot;QMYSQL&quot;</span>, <span class="string">&quot;second_db_connection&quot;</span>);</span><br><span class="line">db2.<span class="built_in">setHostName</span>(<span class="string">&quot;host2&quot;</span>);</span><br><span class="line">db2.<span class="built_in">setDatabaseName</span>(<span class="string">&quot;database2&quot;</span>);</span><br><span class="line">db2.<span class="built_in">setUserName</span>(<span class="string">&quot;user2&quot;</span>);</span><br><span class="line">db2.<span class="built_in">setPassword</span>(<span class="string">&quot;password2&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">QSqlQuery <span class="title">query1</span><span class="params">(db1)</span></span>;</span><br><span class="line"><span class="function">QSqlQuery <span class="title">query2</span><span class="params">(db2)</span></span>;</span><br></pre></td></tr></table></figure><p>这里创建了两个QSqlDatabase连接两个数据库 </p><p>addDatabase(“QMYSQL”, “first_db_connection”);中第二个字符串参数是：连接名称</p><p>当你用一个QSqlDatabase对象进行了多次连接时，</p><p>如果您想要在后面的代码中获取之前创建的 <code>db1</code> 连接，您可以这样做：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">QSqlDatabase db1 = QSqlDatabase::<span class="built_in">database</span>(<span class="string">&quot;first_db_connection&quot;</span>);</span><br></pre></td></tr></table></figure><p>如果您想要关闭特定的数据库连接，也可以使用连接名称：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">QSqlDatabase::database(&quot;first_db_connection&quot;).close();</span><br></pre></td></tr></table></figure><h3 id="WORKBENCH与外键"><a href="#WORKBENCH与外键" class="headerlink" title="WORKBENCH与外键"></a>WORKBENCH与外键</h3><p>[MySQL WorkBench中如何设置外键属性_mob649e8163af7d的技术博客_51CTO博客](<a href="https://blog.51cto.com/u_16175497/6776787#:~:text=%E5%9C%A8MySQL">https://blog.51cto.com/u_16175497/6776787#:~:text=在MySQL</a> Workbench中，点击”数据库”菜单，然后选择”应用模型更改”选项。,在”应用模型更改”对话框中，选择您想要应用更改的数据库连接，并点击”执行”按钮。 完成了以上步骤后，您的外键属性将成功设置。)</p><p>【MySql中用WorkBench创建表主键和外键】<a href="https://www.bilibili.com/video/BV14A4y1S7KJ?vd_source=50cd1a4c87dfd3bdc7a70a5d22a771ed">https://www.bilibili.com/video/BV14A4y1S7KJ?vd_source=50cd1a4c87dfd3bdc7a70a5d22a771ed</a></p><p>外键的功能：</p><p>在MySQL Workbench中添加外键时，<code>ON UPDATE</code>和<code>ON DELETE</code>选项用于指定当父表（被引用的表）中的记录被更新或删除时，应该如何处理子表（引用的表）中的相关记录。这些选项定义了外键约束的行为，以确保数据的引用完整性。以下是这两个选项的具体用途：</p><p>ON UPDATE选项：</p><ul><li><strong>RESTRICT</strong>：拒绝对父表的主键进行更新操作，如果子表中有相关的记录。</li><li><strong>CASCADE</strong>：当父表的主键更新时，子表中相关的外键也会自动更新为新的值。</li><li><strong>SET NULL</strong>：当父表的主键更新时，子表中相关的外键将被设置为NULL（如果外键字段允许NULL值）。</li><li><strong>NO ACTION</strong>：在SQL标准中与RESTRICT相同，表示拒绝对父表的主键进行更新操作。</li></ul><p>ON DELETE选项：</p><ul><li><strong>RESTRICT</strong>：拒绝对父表的主键进行删除操作，如果子表中有相关的记录。</li><li><strong>CASCADE</strong>：当父表的主键被删除时，子表中相关的外键记录也会被自动删除。</li><li><strong>SET NULL</strong>：当父表的主键被删除时，子表中相关的外键将被设置为NULL（如果外键字段允许NULL值）。</li><li><strong>NO ACTION</strong>：在SQL标准中与RESTRICT相同，表示拒绝对父表的主键进行删除操作</li></ul><p>CASCADE：当父表中的主键被删除的时候，子表中与之相关的那个外键的整行都会被删除</p><h2 id="创建、查询、修改表"><a href="#创建、查询、修改表" class="headerlink" title="创建、查询、修改表"></a>创建、查询、修改表</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>  player &#123;</span><br><span class="line">id <span class="type">INT</span>,   <span class="comment">-- 字段名 数据类型</span></span><br><span class="line">name <span class="type">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span> player  或者  <span class="keyword">DESC</span>  player   </span><br></pre></td></tr></table></figure><p> 可以查看表的结构</p><p>修改字段名  关键字 ： RENAME COLUMN</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">table</span> player RENAME <span class="keyword">COLUMN</span> name <span class="keyword">to</span> nick_name;</span><br></pre></td></tr></table></figure><p>修改数据类型关键字 ： MODIFY COLUMN</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">table</span> player MODIFY <span class="keyword">COLUMN</span> name <span class="type">VARCHAR</span>(<span class="number">200</span>);</span><br></pre></td></tr></table></figure><p>添加字段关键字 ：ADD COLUMN </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">table</span> player <span class="keyword">ADD</span> <span class="keyword">COLUMN</span> last_log DATETIME;</span><br><span class="line"><span class="comment">-- last_log  字段名  DATETIME 数据类型</span></span><br></pre></td></tr></table></figure><p>删除字段关键字 ：DROP COLUMN </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">table</span> player <span class="keyword">DROP</span> <span class="keyword">COLUMN</span> last_log ;</span><br></pre></td></tr></table></figure><p>删除表格</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> player；</span><br></pre></td></tr></table></figure><h2 id="创建数据库-1"><a href="#创建数据库-1" class="headerlink" title="创建数据库"></a>创建数据库</h2><p><a href="https://blog.csdn.net/Itmastergo/article/details/130006403">MySQL创建数据库（CREATE DATABASE语句）_create database st;-CSDN博客</a></p><p>命令行操作：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE test_db;</span><br></pre></td></tr></table></figure><p>创建一个名为test_db的数据库文件</p><p>使用这个数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> use game</span><br></pre></td></tr></table></figure><h2 id="sql中的数据类型"><a href="#sql中的数据类型" class="headerlink" title="sql中的数据类型"></a>sql中的数据类型</h2><p>整数类型：</p><ul><li>INT 或 INTEGER：标准整数类型。</li><li>SMALLINT：较小的整数。</li><li>TINYINT：更小的整数。</li><li>BIGINT：较大的整数。</li></ul><p>浮点类型：</p><ul><li>FLOAT：浮点数，精度较低。</li><li>DOUBLE：双精度浮点数，精度较高。</li><li>REAL：与 DOUBLE 类似，用于支持SQL标准。</li></ul><p>固定精度类型：</p><ul><li>DECIMAL 或 NUMERIC：用于存储具有固定精度和小数位数的数值。</li></ul><p>字符串类型：</p><ul><li><p>CHAR：固定长度的字符串。</p></li><li><p>VARCHAR：可变长度的字符串。</p><p>VARCHAR(255)意味着该字段可以存储最多255个字符。</p></li><li><p>TEXT：存储大文本。</p></li></ul><p>日期和时间类型：</p><ul><li>DATE：日期，格式为 YYYY-MM-DD。</li><li>TIME：时间，格式为 HH:MM:SS。</li><li>DATETIME：日期和时间，格式为 YYYY-MM-DD HH:MM:SS。</li><li>TIMESTAMP：时间戳，通常用于记录数据的最后修改时间。</li></ul><p>布尔类型：</p><ul><li>BOOLEAN：存储真值 TRUE 或 FALSE。</li></ul><p>二进制类型：</p><ul><li>BLOB：二进制大对象，用于存储图片、音频等大型二进制数据。</li><li>BINARY：固定长度的二进制数据。</li><li>VARBINARY：可变长度的二进制数据。</li></ul><p>枚举类型：</p><ul><li>ENUM：枚举类型，允许列的值是预定义的列表中的某一个。</li></ul><p>集合类型：</p><ul><li>SET：集合类型，允许列的值是预定义的列表中的零个或多个。</li></ul><h2 id="常用语句"><a href="#常用语句" class="headerlink" title="常用语句"></a>常用语句</h2><h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><p>SELECT * FORM player <strong>WHERE</strong> 条件1 <strong>AND</strong> 条件2</p><p>除了AND 还有NOT 和  OR</p><p>优先级顺序 NOT &gt;AND &gt; OR \</p><p><strong>可以用括号改变优先级</strong></p><p>eg:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">WHERE</span> level<span class="operator">&gt;=</span><span class="number">1</span> <span class="keyword">AND</span> level<span class="operator">&lt;=</span><span class="number">10</span>;</span><br></pre></td></tr></table></figure><p>BETWEEN ?  AND ?</p><p>eg:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">WHERE</span> level <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">AND</span>  <span class="number">10</span>;</span><br><span class="line"><span class="comment">-- 包括1 和 10 </span></span><br></pre></td></tr></table></figure><p>模糊查询LIKE</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">WHERE</span> name <span class="keyword">LIKE</span> <span class="string">&#x27;王%&#x27;</span>;</span><br><span class="line"><span class="comment">-- %代表任意多个字符（可以是0（</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">WHERE</span> name <span class="keyword">LIKE</span> <span class="string">&#x27;王_&#x27;</span>;</span><br><span class="line"><span class="comment">-- _代表一个字符</span></span><br></pre></td></tr></table></figure><p>正则表达式查询 REGEXP</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">WHERE</span> name REGEXP  <span class="string">&#x27;正则表达式&#x27;</span>;</span><br></pre></td></tr></table></figure><p> 查找NULL</p><p>这里不能用等于号，因为null 不等于任何数值 </p><p>– is null </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">WHERE</span> name <span class="keyword">is</span> <span class="keyword">null</span>  <span class="string">&#x27;正则表达式&#x27;</span>;</span><br></pre></td></tr></table></figure><p>查找空</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">WHERE</span> name <span class="operator">=</span>  <span class="string">&#x27;&#x27;</span>;</span><br></pre></td></tr></table></figure><p>null 是什么都没有 ， 空 是填写了一个空字符串</p><h3 id="排列"><a href="#排列" class="headerlink" title="排列"></a>排列</h3><p>升序排列 </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">ORDER</span> <span class="keyword">BY</span> level ; </span><br><span class="line"><span class="comment">-- 按照字段level 升序排列</span></span><br></pre></td></tr></table></figure><p>降序排列</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">ORDER</span> <span class="keyword">BY</span> level <span class="keyword">DESC</span>; </span><br><span class="line"><span class="comment">-- 按照字段level 升序排列</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">ORDER</span> <span class="keyword">BY</span> level <span class="keyword">DESC</span>， exp ;</span><br><span class="line"><span class="comment">-- 先按照level 降序排列</span></span><br><span class="line"><span class="comment">-- 再按照 exp升序排列 （level 相同的情况下）</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">5</span> <span class="keyword">DESC</span>， </span><br><span class="line"><span class="comment">-- 先按照第五列那个字段 降序排列</span></span><br></pre></td></tr></table></figure><h3 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h3><p>括号中是要指定的列名 *代表全集</p><p><img src="/2024/09/21/mysql-0/image-20240507212050674.png" alt="image-20240507212050674"></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">FROM</span> player ;</span><br></pre></td></tr></table></figure><h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><p><img src="/2024/09/21/mysql-0/image-20240507212450043.png" alt="image-20240507212450043"></p><p>如果没有group by 结果中会出现多组  95 5 <strong>其实这些数据可以用group by 合并起来</strong></p><p> <strong>having: 分组后过滤</strong></p><h3 id="限制数量"><a href="#限制数量" class="headerlink" title="限制数量"></a>限制数量</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> player <span class="keyword">ORDER</span> <span class="keyword">BY</span> level  </span><br><span class="line">LIMIT <span class="number">3</span> ;</span><br></pre></td></tr></table></figure><p>只查看前三名</p><h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h3><p>DISTINCT</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> sex <span class="keyword">FROM</span> player；</span><br><span class="line"><span class="comment">-- 重复内容只显示一条</span></span><br></pre></td></tr></table></figure><h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><p>UNION </p><p>写在两条语句中间就可以，默认会去除重复的记录。</p><p>不想去除重复记录就用UNION  ALL</p><h3 id="交集"><a href="#交集" class="headerlink" title="交集"></a>交集</h3><p>INTERSECT</p><p>写在两条语句中间就可以</p><h3 id="差集"><a href="#差集" class="headerlink" title="差集"></a>差集</h3><p>查找语句A</p><p>EXCEPT</p><p>查找语句B；</p><p>A-B</p><h2 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h2><p><img src="/2024/09/21/mysql-0/image-20240507213914242.png" alt="image-20240507213914242"></p><p>把一个查询的结果作为另一个语句的条件</p><p><img src="/2024/09/21/mysql-0/image-20240507214057703.png" alt="image-20240507214057703"></p><h2 id="表关联"><a href="#表关联" class="headerlink" title="表关联"></a>表关联</h2><p><img src="/2024/09/21/mysql-0/image-20240507214312155.png" alt="image-20240507214312155"></p><p><strong>内连接（INNER JOIN）</strong>：</p><ul><li>内连接是最常见的连接类型，它返回两个或多个表中匹配成功的行。</li><li>当在两个表进行内连接时，只有当每个表中的行在关联列上都有匹配的值时，这些行才会出现在结果集中。</li><li>如果表中有不匹配的行，那么这些行将不会出现在结果集中。</li><li>内连接可以看作是交叉连接的子集，只选择那些在所有相关表中都有匹配的行。</li></ul><p><strong>左连接（LEFT JOIN）</strong>：</p><ul><li>左连接又称为左外连接，它返回左表（FROM子句之前的表）的所有行，即使在右表中没有匹配的行。</li><li>如果左表中的某行在右表中没有匹配，那么结果集中该行的右表部分将为NULL。</li><li>左连接在查询时以左表为基础，确保左表的所有行都会被返回，同时附带上右表中匹配的 行。</li></ul><p><strong>右连接（RIGHT JOIN）</strong>：</p><ul><li>右连接与左连接相反，它返回右表的所有行，即使在左表中没有匹配的行。</li><li>如果右表中的某行在左表中没有匹配，那么结果集中该行的左表部分将为NULL。</li><li>右连接在查询时以右表为基础，确保右表的所有行都会被返回，同时附带上左表中匹配的行</li></ul><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>创建一次索引，以后的查询都会快很多</p><h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><p>Views</p><p><img src="/2024/09/21/mysql-0/image-20240507214809971.png" alt="image-20240507214809971"></p><p>视图是一个动态的表格</p><p>例如你可以创建一个显示 等级最高的10个人 的视图</p><p>当你修改第一名的等级后，视图也会随之改变</p>]]></content>
      
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手学习深度学习</title>
      <link href="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="ch01-预备知识"><a href="#ch01-预备知识" class="headerlink" title="ch01_预备知识"></a>ch01_预备知识</h1><h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h4><p>行和列都是从0开始，-1可以表示最后一行or最后一列</p><p>访问元素: 例如[1:3,1:]表示访问第1、2行以及从第一列开始的所有列</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240726170521499.png" alt="image-20240726170521499"></p><p>[::3,::2] ::3表示切片，每三行取一行</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240726170626631.png" alt="image-20240726170626631"></p><p>[3]表示第三行[,3]表示第三列</p><h4 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">12</span>)</span><br><span class="line"><span class="comment">#随机</span></span><br><span class="line">torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">#全0</span></span><br><span class="line">torch.zeros((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240726172228760.png" alt="image-20240726172228760"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#赋值</span></span><br><span class="line">torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Z的shape和数据类型都与y相同</span></span><br><span class="line">Z = torch.zeros_like(Y)</span><br></pre></td></tr></table></figure><h4 id="访问张量"><a href="#访问张量" class="headerlink" title="访问张量"></a>访问张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape <span class="comment">#torch.size([12])</span></span><br><span class="line">x.numel() <span class="comment">#12</span></span><br></pre></td></tr></table></figure><h4 id="修改张量"><a href="#修改张量" class="headerlink" title="修改张量"></a>修改张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X= x.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">X.shape <span class="comment">#torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure><h4 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h4><p>加减乘除求幂</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">x + y, x - y, x * y, x / y, x ** y  <span class="comment"># **运算符是求幂运算</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>, <span class="number">10.</span>]),</span><br><span class="line"> tensor([-<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">6.</span>]),</span><br><span class="line"> tensor([ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">16.</span>]),</span><br><span class="line"> tensor([<span class="number">0.5000</span>, <span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">4.0000</span>]),</span><br><span class="line"> tensor([ <span class="number">1.</span>,  <span class="number">4.</span>, <span class="number">16.</span>, <span class="number">64.</span>]))</span><br></pre></td></tr></table></figure><p>更多计算</p><p>指数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure><p>求和</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()<span class="comment">#结果是一个元素</span></span><br></pre></td></tr></table></figure><h4 id="连结张量"><a href="#连结张量" class="headerlink" title="连结张量"></a>连结张量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), <span class="comment">#在行上合并</span></span><br><span class="line">torch.cat((X, Y), dim=<span class="number">1</span>)  <span class="comment">#在列上合并</span></span><br></pre></td></tr></table></figure><h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p><strong>即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作</strong>]。 这种机制的工作方式如下：</p><ol><li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li><li>对生成的数组执行按元素操作。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br><span class="line"><span class="comment">########################</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a + b</span><br><span class="line"><span class="comment">########################</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><h4 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tensor-&gt;numpy</span></span><br><span class="line">A = X.numpy()</span><br><span class="line"><span class="comment">#numpy-&gt;tensor</span></span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="comment">#当tensor大小为1时，tensor-&gt;python标量 </span></span><br><span class="line">a= torch.tensor([<span class="number">3.5</span>])</span><br></pre></td></tr></table></figure><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>本节将简要介绍使用<code>pandas</code>预处理原始数据，并将原始数据转换为张量格式的步骤。 后面的章节将介绍更多的数据预处理技术。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># &#x27;..&#x27;代表本目录的父目录</span></span><br><span class="line"><span class="comment"># exist_ok=True表示假如已经存在data目录，不会重新创建且不会报错</span></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># data_file存储data目录下文件house_tiny.csv的路径</span></span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 用write的方式打开</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这里缺失了数据，这里我们考虑插值法处理</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727114348965.png" alt="image-20240727114348965"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="comment"># inputs获取第0、1列 outputs获取第2列</span></span><br><span class="line"><span class="comment"># .iloc是pandas库中基于位置进行索引的函数</span></span><br><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, -<span class="number">1</span>]</span><br><span class="line"><span class="comment"># .mean用于获取均值，numeric_only=True表示只考虑数值</span></span><br><span class="line">inputs = inputs.fillna(inputs.mean(numeric_only=<span class="literal">True</span>))</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727114906568.png" alt="image-20240727114906568"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建虚拟变量  dummy_na=True表示要将缺失值（NaN）转换为一个新的虚拟变量</span></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727115335144.png" alt="image-20240727115335144"></p><p>转化为张量</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727115521841.png" alt="image-20240727115521841"></p><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h3 id="线性代数实现"><a href="#线性代数实现" class="headerlink" title="线性代数实现"></a>线性代数实现</h3><p><strong>标量</strong>：用只有一个元素的张量表示</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br></pre></td></tr></table></figure><p><strong>向量</strong>：<strong>标量值组成的列表</strong></p><p>认为列向量是向量的默认方向</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>用索引访问到的当个元素也是张量</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br><span class="line"><span class="comment"># tensor(3)</span></span><br></pre></td></tr></table></figure><p>向量的长度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(x)</span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><p><strong>矩阵</strong>：用m*n的张量表示</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>转制</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure><p>对于对称矩阵，满足：</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727153227299.png" alt="image-20240727153227299"></p><p><strong>张量</strong>：[<strong>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</strong>]。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727153540597.png" alt="image-20240727153540597"></p><h3 id="张量算法"><a href="#张量算法" class="headerlink" title="张量算法"></a>张量算法</h3><p>两个矩阵按元素相乘*<strong>Hadamard积*</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure><h4 id="降维求和"><a href="#降维求和" class="headerlink" title="降维求和"></a>降维求和</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以[<strong>指定张量沿哪一个轴来通过求和降低维度</strong>]。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，<strong>因此输入轴0的维数在输出形状中消失</strong>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 沿着第0轴进行求和</span></span><br><span class="line"><span class="comment"># A是一个5*4的矩阵</span></span><br><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line">(tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), torch.Size([<span class="number">4</span>]))</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727155109468.png" alt="image-20240727155109468"></p><p>也可以沿着两个维度求和</p><p>例如下列代码对X沿着0轴和1轴求和</p><p>可以理解为先让两个3*4矩阵相加，得到一个3 * 4矩阵</p><p>然后将每一个长度为4的向量相加，得到tensor([60, 66, 72, 78])</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">X_sum_axis01 = X.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">X_sum_axis01</span><br><span class="line"></span><br><span class="line">(tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">          [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">          [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">          [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">          [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]]),</span><br><span class="line">tensor([<span class="number">60</span>, <span class="number">66</span>, <span class="number">72</span>, <span class="number">78</span>])</span><br></pre></td></tr></table></figure><h4 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">A_sum_axis1,sum_A</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 降维求和结果</span></span><br><span class="line">(tensor([ <span class="number">6.</span>, <span class="number">22.</span>, <span class="number">38.</span>, <span class="number">54.</span>, <span class="number">70.</span>]),</span><br><span class="line"> <span class="comment"># 非降维求和结果，5*1，还是两个维度</span></span><br><span class="line"> tensor([[ <span class="number">6.</span>],</span><br><span class="line">         [<span class="number">22.</span>],</span><br><span class="line">         [<span class="number">38.</span>],</span><br><span class="line">         [<span class="number">54.</span>],</span><br><span class="line">         [<span class="number">70.</span>]]))</span><br></pre></td></tr></table></figure><p>如果我们想沿[<strong>某个轴计算<code>A</code>元素的累积总和</strong>]， 比如<code>axis=0</code>（按行计算），可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A,A.cumsum(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>],</span><br><span class="line">         [<span class="number">24.</span>, <span class="number">28.</span>, <span class="number">32.</span>, <span class="number">36.</span>],</span><br><span class="line">         [<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]]))</span><br></pre></td></tr></table></figure><h4 id="平均值"><a href="#平均值" class="headerlink" title="平均值"></a>平均值</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean() , A.<span class="built_in">sum</span>() / A.numel()</span><br></pre></td></tr></table></figure><p>同样，计算平均值的函数也可以沿指定轴降低张量的维度</p><p>例如，求A沿着0轴方向的平均值</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.dot(x, y)</span><br></pre></td></tr></table></figure><p><strong>我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure><h4 id="矩阵-向量积"><a href="#矩阵-向量积" class="headerlink" title="矩阵-向量积"></a>矩阵-向量积</h4><p>例如A是一个m*n的矩阵，x是一个长度为n的向量</p><p>则A与x的积为一个长度为m的列向量，其中第i个元素为矩阵中第i行向量与x的点积</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240727162328237.png" alt="image-20240727162328237"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A, x)</span><br><span class="line"></span><br><span class="line">(torch.Size([<span class="number">5</span>, <span class="number">4</span>]), torch.Size([<span class="number">4</span>]), tensor([ <span class="number">14.</span>,  <span class="number">38.</span>,  <span class="number">62.</span>,  <span class="number">86.</span>, <span class="number">110.</span>]))</span><br></pre></td></tr></table></figure><h4 id="矩阵-矩阵乘法"><a href="#矩阵-矩阵乘法" class="headerlink" title="矩阵-矩阵乘法"></a>矩阵-矩阵乘法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.mm(A, B)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.matual(A,B)</span><br></pre></td></tr></table></figure><p><code>torch.mm</code> 的全称是 “matrix multiply”，它遵循传统的矩阵乘法规则。</p><p><code>torch.matmul</code> 是一个更为通用的函数，它可以处理多种维度的张量乘法，不仅限于二维矩阵。<code>torch.matmul</code> 可以用于矩阵和矩阵之间的乘法，也可以用于矩阵和向量之间的乘法，或者更高维度的张量乘法。它的工作方式取决于输入张量的形状和维度。</p><h4 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h4><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240730120311633.png" alt="image-20240730120311633"></p><p>L2范数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure><p>类似于向量的L2范数，Frobenius范数（Frobenius norm）是矩阵元素平方和的平方根，也可以用torch.norm计算</p><p>L1范数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p>在深度学习中，我们经常试图解决优化问题： <em>最大化</em>分配给观测数据的概率; <em>最小化</em>预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</p><h2 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算*"></a>矩阵计算*</h2><p>主要是讲矩阵如何求导数</p><p>基础知识：</p><p><a href="https://zhuanlan.zhihu.com/p/263777564">矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)</a></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731104153413.png" alt="image-20240731104153413"></p><p>向量关于向量求导的结果是一个矩阵</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731104815010.png" alt="image-20240731104815010"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731105049555.png" alt="image-20240731105049555"></p><p>右下角括号内的数字是：</p><p>(m,l,k,n)</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731110635856.png" alt="image-20240731110635856"></p><h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导*"></a>自动求导*</h2><p>原理：</p><p>构造计算图</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731112300974.png" alt="image-20240731112300974"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731112539492.png" alt="image-20240731112539492"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240731112917005.png" alt="image-20240731112917005"></p><p>反向累积：</p><p>时间复杂度：O(n) 通常正向和反向的代价类似</p><p>空间复杂度O(n),需要存储正向的所有中间结果</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># requires_grad=True表示需要存储梯度的数值</span></span><br><span class="line"><span class="comment"># x.grad就可以访问这个存储的梯度</span></span><br><span class="line">x=torch.arange(<span class="number">4.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">12.</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_() <span class="comment"># 清空梯度</span></span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>向量对向量求导，结果应该是一个矩阵，但是</p><p><strong>深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class="line"><span class="comment"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x </span><br><span class="line"><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><h3 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h3><p>有时，我们希望[<strong>将某些计算移动到记录的计算图之外</strong>]。 </p><p>假如，y是作为x的函数被计算的，z则作为y和x的函数，但是，我们希望将y视为一个常数，不希望对z求关于x 的导数时，梯度经y流到x</p><p>解决方案是分离y，返回一个新变量u，u丢弃计算图中如何计算y的任何信息</p><p>在下列代码中，求的是Z&#x3D;u * x的偏导数，而不是Z&#x3D;x* * x * x的偏导数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u</span><br><span class="line"></span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><h3 id="Python控制流的梯度计算"><a href="#Python控制流的梯度计算" class="headerlink" title="Python控制流的梯度计算"></a>Python控制流的梯度计算</h3><p>使用自动微分的一个好处是： [<strong>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</strong>]。 在下面的代码中，<code>while</code>循环的迭代次数和<code>if</code>语句的结果都取决于输入<code>a</code>的值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># randn用于创建服从标准正态分布的张量</span></span><br><span class="line"><span class="comment"># size=()表示创建的是标量</span></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure><p><code>f</code>函数在其输入<code>a</code>中是分段线性的。 换言之，对于任何<code>a</code>，存在某个常量标量<code>k</code>，使得<code>f(a)=k*a</code>，其中<code>k</code>的值取决于输入<code>a</code>，因此可以用<code>d/a</code>验证梯度是否正确。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.grad == d / a</span><br><span class="line"></span><br><span class="line">tensor(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="ch02-线性神经网络"><a href="#ch02-线性神经网络" class="headerlink" title="ch02_线性神经网络"></a>ch02_线性神经网络</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归就是对于具有线性关系的自变量和因变量，找到最好的参数来满足这个线性关系</p><p>因此，我们需要：一种度量方式——损失</p><p>​一种能够更新模型以提高模型预测质量的方法。——随机梯度下降</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801122008005.png" alt="image-20240801122008005"></p><h4 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h4><p>为了解释<em>线性回归</em>，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的<strong>数据集</strong>。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为<em>训练数据集</em>（<strong>training data set</strong>） 或<em>训练集</em>（<strong>training set</strong>）。 每行数据（比如一次房屋交易相对应的数据）称为<em>样本</em>（<strong>sample</strong>）， 也可以称为<em>数据点</em>（<strong>data point</strong>）或<em>数据样本</em>（<strong>data instance</strong>）。 我们把试图预测的目标（比如预测房屋价格）称为<em>标签</em>（<strong>label</strong>）或<em>目标</em>（<strong>target</strong>）。 预测所依据的自变量（面积和房龄）称为<em>特征</em>（<strong>feature</strong>）或<em>协变量</em>（<strong>covariate</strong>）。<br>$$<br>对索引为i的样本，其输入表示为\mathbf{x}^{(i)} &#x3D; [x_1^{(i)}, x_2^{(i)}]^\top，<br>其对应的标签是y^{(i)}。<br>$$</p><h4 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h4><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801115554997.png" alt="image-20240801115554997"></p><p>model parameters 模型参数就是这里的w和b，训练的最终目标就是找到最好的w和b</p><p>线性模型可以视为单层神经网络（输出层不当成一层）</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801115659649.png" alt="image-20240801115659649"></p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>均方损失函数<br>$$<br>\hat{y}^{(i)}代表估计值<br>$$</p><p>$$<br>l^{(i)}(\mathbf{w}, b) &#x3D; \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2<br>$$</p><p>下图中的L就是损失函数，它是平方误差的平均值</p><p>我们<strong>希望找到w和b，使得损失最小化</strong></p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801121145738.png" alt="image-20240801121145738" style="zoom:150%;"><h4 id="随机梯度下降SGD"><a href="#随机梯度下降SGD" class="headerlink" title="随机梯度下降SGD"></a>随机梯度下降SGD</h4><p>Stochastic Gradient Descent</p><p>这种方法几乎可以优化所有深度学习模型它通过<strong>不断地在损失函数递减的方向上更新参数</strong>来降低误差。</p><p>直观理解：</p><p>选定一个随机起始点W0，找到函数在这一点的梯度，然后沿着负梯度方向找到下一个点W1，依次类推直到损失最小</p><p>每次移动的向量是固定的，这个长度就是学习率</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240801122802801.png" alt="image-20240801122631556"></p><p>由于在真个训练集上计算梯度的成本太高</p><p>我们可以随机采样b个样本来计算近似损失，这就是<strong>小批量随机梯度下降</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806081113409.png" alt="image-20240806081113409"></p><h3 id="线性回归从0开始实现"><a href="#线性回归从0开始实现" class="headerlink" title="线性回归从0开始实现"></a>线性回归从0开始实现</h3><h4 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a><strong>生成数据集</strong></h4><p>这里是借助正态分布模拟真实数据<br>$$<br>我们使用线性模型参数\mathbf{w} &#x3D; [2, -3.4]^\top、b &#x3D; 4.2<br>和噪声项\epsilon生成数据集及其标签：<br>$$</p><p>$$<br>\mathbf{y}&#x3D; \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.<br>$$</p><p> #@save标签表明这个函数是dzl包的内置函数，也就是说，在实际的应用中，我们通常不需要自己实现这个函数，这里写出来是展示一下原理</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w))) <span class="comment">#行向量</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape) <span class="comment">#再加上e，表示微小的误差</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 放回特征X 和标签y</span></span><br><span class="line"><span class="comment"># reshape((-1, 1))表示重塑为列向量，其中 -1 是一个特殊的参数，表示该维度的大小由其他维度的大小和张量的总元素数量决定。</span></span><br></pre></td></tr></table></figure><p><strong><code>features</code>中的每一行都包含一个二维数据样本， <code>labels</code>中的每一行都包含一维标签值（一个标量）</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806085449212.png" alt="image-20240806085449212"></p><h4 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h4><p>训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。</p><p><strong>定义一个<code>data_iter</code>函数</strong>，生成大小为<code>batch_size</code>的小批量**]</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples)) <span class="comment">#生成样本索引index</span></span><br><span class="line">    random.shuffle(indices) <span class="comment">#打乱索引顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line">        <span class="comment"># 根据index取值</span></span><br><span class="line"><span class="comment"># 注意到yield是写在for循环内部的  </span></span><br></pre></td></tr></table></figure><p><code>yield</code> 返回这两个张量子集组成的元组，但不会像普通函数返回值那样退出函数。相反，它会暂停函数的执行，并保存当前状态</p><p>当生成器在下一次被请求值时，它会从上次离开的地方恢复执行</p><p>当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。 上面实现的迭代对教学来说很好，<strong>但它的执行效率很低，可能会在实际问题上陷入麻烦</strong>。 例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。 在深度学习框架中实现的内置迭代器效率要高得多， 它可以处理存储在文件中的数据和数据流提供的数据。</p><h4 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><p>我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。 因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。</p><p>我们使用自动微分来计算梯度。</p><h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br></pre></td></tr></table></figure><h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save y_hat代表预测值</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><h4 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h4><p>params是参数集（包含w和b）</p><p>lr是学习率</p><p>batch_size 是批量的大小</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            <span class="comment"># 对每一个参数进行迭代</span></span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            <span class="comment"># 清空梯度</span></span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure><p>使用 <code>with torch.no_grad():</code> 时，告诉 PyTorch 在这个代码块中不需要跟踪梯度，因此可以减少内存消耗并加速计算</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p> 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法<code>sgd</code>来更新模型参数。</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806094030348.png" alt="image-20240806094030348"></p><p>在每个<em>迭代周期</em>（epoch）中，我们使用<code>data_iter</code>函数遍历整个数据集， 并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。 这里的迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是<strong>超参数</strong>，分别设为3和0.03。 <strong>设置超参数很棘手</strong>，需<strong>要通过反复试验进行调整</strong>。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lr = 0.03</span><br><span class="line">num_epochs = 3</span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># 小批量损失</span></span><br><span class="line">        <span class="comment">#l中的所有元素被加到一起，并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 经过一个周期的优化后的损失</span></span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.028825</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.000090</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.000047</span></span><br></pre></td></tr></table></figure><p>因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。 因此，我们可以通过[<strong>比较真实参数和通过训练学到的参数来评估训练的成功程度</strong>]。 事实上，真实参数和通过训练学到的参数确实非常接近。</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240806095801126.png" alt="image-20240806095801126"></p><h3 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h3><p>在过去的几年里，出于对深度学习强烈的兴趣， 许多公司、学者和业余爱好者开发了各种成熟的开源框架。 这些框架可以自动化基于梯度的学习算法中重复性的工作。 </p><p>我们只运用了： （1）通过张量来进行数据存储和线性代数； （2）通过自动微分来计算梯度。 实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。</p><p>对于标准深度学习模型，我们可以[<strong>使用框架的预定义好的层</strong>]。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量<code>net</code>，它是一个<code>Sequential</code>类的实例。 <code>Sequential</code>类将多个层串联在一起。 当给定输入数据时，<code>Sequential</code>实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要<code>Sequential</code>。 但是由于以后几乎所有的模型都是多层的，在这里使用<code>Sequential</code>会让你熟悉“标准的流水线”。</p><p>nn.Linear(2,1)的两个参数规定了输入&#x2F;输出张量的wei’du</p><p>输入张量（特征），形状为 <code>(batch_size, 2)</code> </p><p>输出张量（预测值&#x2F;标签），形状为 <code>(batch_size, 1)</code> </p><p>而参数（w&amp;b）则储存在该层神经网络的内部</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span>  torch.utils <span class="keyword">import</span>  data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">true_w= torch.tensor([<span class="number">2</span>,-<span class="number">3.4</span>])</span><br><span class="line">true_b= <span class="number">4.2</span></span><br><span class="line">featrues,labels = d2l.synthetic_data(true_w,true_b,<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = d2l.load_array((featrues,labels),batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn是神经网络的缩写</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 权重参数w</span></span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)           <span class="comment"># 偏置参数b</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment"># 定义优化算法</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.03</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X),y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step() <span class="comment"># 更新参数</span></span><br><span class="line">    l = loss(net(featrues),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.000330</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.000103</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.000103</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.grad)</span><br></pre></td></tr></table></figure><h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>回归和分类的区别</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814105001744.png" alt="image-20240814105001744"></p><p><strong>softmax运算获取一个向量并将它映射为概率</strong></p><p>假如我们有4个特征和3个可能的输出类别，我们为每个输入计算三个<em>未规范化的预测</em>（logit）：<br>$$<br>\begin{aligned}<br>o_1 &amp;&#x3D; x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\<br>o_2 &amp;&#x3D; x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\<br>o_3 &amp;&#x3D; x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.<br>\end{aligned}<br>$$<br>为了更简洁地表达模型，我们仍然使用线性代数符号。 通过向量形式表达为o&#x3D; Wx + b</p><p>而我们希望模型的输出可以代表对应类的概率，那么我们就需要softmax函数：</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814111909419.png" alt="image-20240814111909419"></p><p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个<em>线性模型</em>（linear model）</p><p><strong>损失函数：</strong></p><p><strong>交叉熵损失</strong>常用于衡量两个概率的区别<br>$$<br>l(\mathbf{y}, \hat{\mathbf{y}}) &#x3D; - \sum_{j&#x3D;1}^q y_j \log \hat{y}_j.<br>$$<br>结合softmax函数，可以得到</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814120039988.png" alt="image-20240814120039988"></p><p>即损失函数的导数是真实概率和预测概率的区别，这不是巧合，在任何指数族分布模型中 ， 对数似然的梯度正是由此得出的。 这使梯度计算在实践中变得容易很多。</p><h3 id="图像分类数据集"><a href="#图像分类数据集" class="headerlink" title="图像分类数据集"></a>图像分类数据集</h3><p>@save标记表明这个函数在d2l包中以及储存，这里只是展示具体实现</p><p><strong>MNIST</strong>数据集是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的<strong>Fashion-MNIST</strong>数据集</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line">d2l.use_svg_display()  <span class="comment"># 使用svg格式显示图片</span></span><br></pre></td></tr></table></figure><p>Fashion-MNIST由10个类别的图像组成， 每个类别由<em>训练数据集</em>（train dataset）中的6000张图像 和<em>测试数据集</em>（test dataset）中的1000张图像组成。 因此，训练集和测试集分别包含60000和10000张图像。 测试数据集不会用于训练，只用于评估模型性能&#x2F;。</p><p>在上一级目录中的data文件夹中下载Fashion-MNIST</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)s</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>每个输入图像的高度和宽度均为28像素。 数据集由灰度图像组成，其通道数为1</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br><span class="line"><span class="comment"># torch.Size([1, 28, 28])</span></span><br></pre></td></tr></table></figure><p>Fashion-MNIST中包含的10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。</p><p>这个函数的输入是一个列表，输出也是一个列表</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"><span class="comment"># eg：</span></span><br><span class="line"><span class="comment"># labels = [7, 2, 1, 5, 9]</span></span><br><span class="line"><span class="comment"># [&#x27;sneaker&#x27;, &#x27;trouser&#x27;, &#x27;t-shirt&#x27;, &#x27;sandal&#x27;, &#x27;ankle boot&#x27;]</span></span><br></pre></td></tr></table></figure><p>用于绘制图像的函数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    <span class="comment">#  _ 表示我们不使用 plt 返回的第一个对象（通常是图形对象）。</span></span><br><span class="line">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><p>读取小批量数据</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;同时使用4个进程来读取数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle=True表示每个epoch开始时，数据将被打乱</span></span><br><span class="line">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;4.79 sec &#x27;</span> <span class="comment"># 表示读取一遍数据需要4.79秒</span></span><br></pre></td></tr></table></figure><h4 id="整合组件"><a href="#整合组件" class="headerlink" title="整合组件"></a>整合组件</h4><p>load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。</p><p>这个函数返回训练集和验证集的数据迭代器。<br>此外，这个函数还接受一个可选参数<code>resize</code>，用来将图像大小调整为另一种形状。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure><p>读取一批训练集中的数据，X中的数据是32张1通道64X64像素的图片，而y中则是这张图片的标签，为32个0-9的数字，代表10个类别</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64</span></span><br></pre></td></tr></table></figure><h3 id="softmax回归从0开始实现"><a href="#softmax回归从0开始实现" class="headerlink" title="softmax回归从0开始实现"></a>softmax回归从0开始实现</h3><p>补充知识</p><p><strong>用序列和y同时索引y_hat：</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>,<span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]</span><br></pre></td></tr></table></figure><p>结果：tensor（[0.1000，0.5000]）</p><p>这个操作的意思是根据对应组别和分类标号获得对应的预测值，y_hat[[0, 1], y]代表获得第0组的第0类预测值和第1组的第2类预测值</p><p><strong>评估模式和训练模式</strong>：</p><p>在 PyTorch 中，模型有两种基本模式：训练模式（training mode）和评估模式（evaluation mode）</p><p>在 PyTorch 中，可以通过以下方式在模型的两种模式之间切换：</p><ul><li><code>net.train()</code>：将模型设置为训练模式。</li><li><code>net.eval()</code>：将模型设置为评估模式。</li></ul><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240814173404091.png" alt="image-20240814173404091"></p><p><strong>具体实现：</strong></p><p>注意：以下代码针对jupyter，与pycharm不完全兼容，因此可能出现动图不出现等情况</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">            self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line"></span><br><span class="line">        d2l.plt.draw()</span><br><span class="line">        d2l.plt.pause(<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815082838148.png" alt="image-20240815082838148"></p><p><strong>预测：</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815082853861.png" alt="image-20240815082853861"></p><h3 id="softmax回归的简洁实现"><a href="#softmax回归的简洁实现" class="headerlink" title="softmax回归的简洁实现"></a>softmax回归的简洁实现</h3><p>出现多线程问题的解决方案:</p><p><code>if __name__==&#39;__main__</code>的作用</p><blockquote><p>当.py文件被<strong>直接运行</strong>时，<code>if __name__ == &#39;__main__&#39;</code>之下的代码块将被运行；<br>当.py文件<strong>以模块形式被导入</strong>时，<code>if __name__ == &#39;__main__&#39;</code>之下的代码块不被运行。</p></blockquote><p><a href="https://blog.csdn.net/u013700358/article/details/82753019">pytorch使用出现”RuntimeError: An attempt has been made to start a new process before the…” 解决方法-CSDN博客</a>\</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">banch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(banch_size)</span><br><span class="line"><span class="comment">#print(len(train_iter))</span></span><br><span class="line"><span class="comment"># train_iter的长度是235，即234*256 加上一组不超过256的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义softmax回归模型</span></span><br><span class="line"><span class="comment"># Flatten展平层，用于调整输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="comment"># m-&gt;module 代表模型中的层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weight</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对net中的所有线性层进行权重初始化</span></span><br><span class="line">net.apply(init_weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化算法</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以下是d2l包缺少的train_ch3函数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在n个变量上累加</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n       <span class="comment"># 创建一个长度为 n 的列表，初始化所有元素为0.0。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):           <span class="comment"># 累加</span></span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):                <span class="comment"># 重置累加器的状态，将所有元素重置为0.0</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):     <span class="comment"># 获取所有数据</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算正确的数量</span></span><br><span class="line"><span class="string">    :param y_hat:</span></span><br><span class="line"><span class="string">    :param y:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)            <span class="comment"># 在每行中找到最大值的索引，以确定每个样本的预测类别</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算指定数据集的精度</span></span><br><span class="line"><span class="string">    :param net:</span></span><br><span class="line"><span class="string">    :param data_iter:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()                  <span class="comment"># 通常会关闭一些在训练时启用的行为</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在动画中绘制数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量的绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">            self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span><br><span class="line">        )</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        向图表中添加多个数据点</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :param y:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型一轮</span></span><br><span class="line"><span class="string">    :param net:是要训练的神经网络模型</span></span><br><span class="line"><span class="string">    :param train_iter:是训练数据的数据迭代器，用于遍历训练数据集</span></span><br><span class="line"><span class="string">    :param loss:是用于计算损失的损失函数</span></span><br><span class="line"><span class="string">    :param updater:是用于更新模型参数的优化器</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):  <span class="comment"># 用于检查一个对象是否属于指定的类（或类的子类）或数据类型。</span></span><br><span class="line">        net.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练损失总和， 训练准确总和， 样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:  <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):  <span class="comment"># 用于检查一个对象是否属于指定的类（或类的子类）或数据类型。</span></span><br><span class="line">            <span class="comment"># 使用pytorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()  <span class="comment"># 方法用于计算损失的平均值</span></span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制（自定义）的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape())</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型（）</span></span><br><span class="line"><span class="string">    :param net:</span></span><br><span class="line"><span class="string">    :param train_iter:</span></span><br><span class="line"><span class="string">    :param test_iter:</span></span><br><span class="line"><span class="string">    :param loss:</span></span><br><span class="line"><span class="string">    :param num_epochs:</span></span><br><span class="line"><span class="string">    :param updater:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        trans_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, trans_metrics + (test_acc,))</span><br><span class="line">        train_loss, train_acc = trans_metrics</span><br><span class="line">        <span class="built_in">print</span>(trans_metrics)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    进行预测</span></span><br><span class="line"><span class="string">    :param net:</span></span><br><span class="line"><span class="string">    :param test_iter:</span></span><br><span class="line"><span class="string">    :param n:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">global</span> X, y</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true + <span class="string">&quot;\n&quot;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n]</span><br><span class="line">    )</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><h1 id="ch03-多层感知机MLP"><a href="#ch03-多层感知机MLP" class="headerlink" title="ch03_多层感知机MLP"></a>ch03_多层感知机MLP</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>出现于上世纪60年代的算法</p><p>给定输入x，权重W，偏移b，感知机输出为 1&#x2F;0或者1&#x2F;-1，本质是一个二分类</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815205946737.png" alt="image-20240815205946737"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815205937769.png" alt="image-20240815205937769"></p><p>训练感知机</p><p>原理：如果y与&lt;W,x&gt;+b的积为负，说明y与后者异号，说明以W和b为参数的预测错误，则更新W和b</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815210402776.png" alt="image-20240815210402776"></p><p>这个损失函数的意思是，当y*… &lt;&#x3D; 0 的时候，才会有梯度，才需要更新</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815210616431.png" alt="image-20240815210616431"></p><p><strong>感知机的问题：</strong>只能产生线性分割面，无法拟合XOR函数</p><p>例如下图中，红色小球为一类，绿色小球为1类，无法通过感知器去切分</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815211427820.png" alt="image-20240815211427820"></p><p>思考如何解决XOR问题</p><p>同样四个小球，只要我们进行两次划分，最后结合两次的结果，就可以得到</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815212101754.png" alt="image-20240815212101754"></p><p>这就展现了多层感知机的基本思想</p><h3 id="单隐藏层"><a href="#单隐藏层" class="headerlink" title="单隐藏层"></a><strong>单隐藏层</strong></h3><h4 id="单类分类"><a href="#单类分类" class="headerlink" title="单类分类"></a><strong>单类分类</strong></h4><p>输入层：n维向量</p><p>隐藏层： W1是m x n矩阵，b是m维向量</p><p>输出层  W2是m维矩阵，b是标量</p><p>公式如下：式1中的西格玛函数为元素的激活函数、每个隐藏层都有自己的激活函数</p><p>W2转置的原因是其为列向量，而h也是一个长度为m的列向量</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815213229995.png" alt="image-20240815213229995"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815221307798.png" alt="image-20240815221307798"></p><p>激活函数必须为<strong>非线性</strong>的，分析：</p><p>如果激活函数为线性的，那么隐藏层和输出层的操作相当于对特征做线性变化，这样和用一个简单的线性层去完成没区别、</p><h4 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h4><p>softmax函数的主要作用是对数据进行规范化</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815220502134.png" alt="image-20240815220502134"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815210905106.png" alt="image-20240815210905106"></p><p>在上图中，从输入层到隐藏层的箭头错综复杂，这是因为任意一个特征值对向量h中每一个值都有影响</p><p>而上图隐藏层小球的个数，其实就代表隐藏层的大小，隐藏层将给定数目的特征映射到<strong>人为设定大小</strong>（<strong>超参数</strong>）的向量上</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>在感知机一章中，用到的这个二分类函数，其实也算一个激活函数，但是这个函数的变化太硬了<br>$$<br>f(x) &#x3D;<br>\begin{cases}<br>  1 &amp; \text{if } x &gt; 0, \<br>  \text{0} &amp; \text{otherwise}.<br>\end{cases}<br>$$</p><p>sigmoid函数：将输入投影到(0,1)<br>$$<br>\text{sigmoid}(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$<br>Tanh函数：讲述人投影到（-1，1）<br>$$<br>\tanh(x) &#x3D; \frac{1 - e^{-2x}}{1 + e^{-2x}}<br>$$<br>ReLU函数:<br>$$<br>\text{ReLU}(x) &#x3D; \max(x, 0)<br>$$</p><h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><p>就是将每一个隐藏层的输出（向量h）作为下一个隐藏层的输入</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240815221948725.png" alt="image-20240815221948725"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="单隐藏层多分类"><a href="#单隐藏层多分类" class="headerlink" title="单隐藏层多分类"></a>单隐藏层多分类</h4><p>注意训练函数还是用的train_ch3，再次回顾训练过程：遍历训练集数据，用net模型计算后，利用交叉熵损失函数计算损失（该函数包含<strong>softmax</strong>函数，会对net计算的结果进行规范），接着利用损失计算梯度，更新参数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">banch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(banch_size)</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">num_hiddens = <span class="number">256</span> <span class="comment"># 隐藏层大小 单隐藏层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐藏层参数设置</span></span><br><span class="line">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 输出层参数</span></span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X @ W1 + b1)  <span class="comment"># 这里的@相当于 torch.matmul</span></span><br><span class="line">    <span class="keyword">return</span> (H @ W2 + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，相比于只使用线性模型，加上隐藏层后，损失率明显下降，准确率略微提升</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817104034302.png" alt="image-20240817104034302"></p><h4 id="多隐藏层多分类"><a href="#多隐藏层多分类" class="headerlink" title="多隐藏层多分类"></a>多隐藏层多分类</h4><p>增加层数，记得上一个隐藏层的输出是下一个隐藏层的输入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">128</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">64</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>实验结果发现，在同样的10次循环的情况下，使用三个隐藏层的效果不如单隐藏层</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817111452520.png" alt="image-20240817111452520"></p><p>可以看到训练误差的表现很好，但是泛化误差的表现并不好，这可能是发生了<strong>过拟合</strong></p><h4 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h4><p>主要区别是net的定义直接用神经网络实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只有linear层有参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure><p>Flatten展平层：将输入的多维数据展平为一维数据</p><p>Linear全连接层：将输入的784个特征映射到256个特征</p><p>ReLU：激活函数</p><p>Linear全连接层：将256个特征映射到10个特征</p><h2 id="模型选择-过拟合与欠拟合"><a href="#模型选择-过拟合与欠拟合" class="headerlink" title="模型选择-过拟合与欠拟合"></a>模型选择-过拟合与欠拟合</h2><p><strong>误差</strong></p><p>训练误差：模型在训练数据上的误差 【模拟考】</p><p>泛化误差：模型在新数据上的误差  【高考】【最关心】</p><p><strong>数据集</strong></p><p>验证数据集：评估模型好坏的数据集</p><p>测试数据集：只用一次的数据集，相当于结果，不能用这个数据集去调参</p><p>数据集不够的时候的<strong>验证方式</strong> </p><p>数据集分成k快，跑k次</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817113014971.png" alt="image-20240817113014971"></p><p><strong>过拟合和欠拟合</strong></p><p>过拟合：相当于把每一个样本都记住了，过拟合发生在模型在训练数据上表现得很好，但在新的、未见过的数据上表现很差的情况下。</p><p>欠拟合：模型太简单，特征太多，拟合效果不好</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817114250534.png" alt="image-20240817114250534"></p><p><strong>模型容量</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817114846859.png" alt="image-20240817114846859"></p><p><strong>模型容量的影响</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817114940284.png" alt="image-20240817114940284"></p><p>根本上，我们需要的还是将泛化误差往下降</p><p><strong>估计模型容量</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817115702547.png" alt="image-20240817115702547"></p><p><strong>VC维</strong></p><p>对于一个分类模型，VC等于一个最大的数据集的大小。</p><p>例如: 2维输入的感知机，VC&#x3D;3 （最多分类3个点）</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817120211849.png" alt="image-20240817120211849"></p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817120433723.png" alt="image-20240817120433723"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line">max_degree = <span class="number">20</span> <span class="comment"># 从0开始，最高到19次幂</span></span><br><span class="line">n_train,n_test = <span class="number">100</span>,<span class="number">100</span> <span class="comment"># 数据集大小</span></span><br><span class="line">true_w = np.zeros(max_degree) <span class="comment"># 分配空间</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>]= np.array([<span class="number">5</span>,<span class="number">1.2</span>,-<span class="number">3.4</span>,<span class="number">5.6</span>]) <span class="comment"># 这里最高只需要到三阶</span></span><br><span class="line"></span><br><span class="line">features = np.random.normal(size = (n_train+n_test,<span class="number">1</span>))</span><br><span class="line">np.random.shuffle(features) <span class="comment"># 打乱数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算各个特征值的0到max_degree-1次幂，相当于features的行数不变，列数增加</span></span><br><span class="line">poly_features = np.power(features,np.arange(max_degree).reshape(<span class="number">1</span>,-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (max_degree):</span><br><span class="line">    <span class="comment">#  math.gamma(i+1) ： i的阶乘</span></span><br><span class="line">    poly_features[:,i]  /= math.gamma(i+<span class="number">1</span>)  <span class="comment"># x的n次方除以n的阶乘</span></span><br><span class="line"></span><br><span class="line">labels = np.dot(poly_features,true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>,size=labels.shape) <span class="comment"># 加上一点噪声</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy ndarray转换为tensor</span></span><br><span class="line">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class="line">    torch.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估损失</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features,test_features,train_lables,test_lables,</span></span><br><span class="line"><span class="params">          num_epochs = <span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>) <span class="comment"># 均方误差损失函数</span></span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 不设置偏置</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape,<span class="number">1</span>,bias=<span class="literal">False</span>))</span><br><span class="line">    banch_size = <span class="built_in">min</span>(<span class="number">10</span>,train_lables.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features,train_lables.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                                banch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features,test_lables.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                              banch_size,is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.01</span>) <span class="comment"># 优化函数</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每20轮看一次结果</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss <span class="subst">&#123;evaluate_loss(net,test_iter,loss):f&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure><p><strong>训练：</strong></p><p><strong>1、使用三阶多项式去拟合</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><p>最终结果：</p><p>epoch 400,loss 0.009408<br>weight: [[ 4.9953513  1.1967007 -3.3998032  5.5985456]]</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817130615698.png" alt="image-20240817130615698"></p><p><strong>2、使用线性函数去拟合（欠拟合）</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><p>epoch 400,loss 7.965484<br>weight: [[3.5459476 3.5083208]]</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817130607110.png" alt="image-20240817130607110"></p><p><strong>3、使用19阶函数去拟合（过拟合）</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在1500轮过后，拟合结果似乎还不错</p><p>epoch 1500,loss 0.011771<br>weight: [[ 4.9755540e+00  1.2619114e+00 -3.3151577e+00  5.2241902e+00<br>  -3.6112809e-01  1.1908938e+00  3.0346635e-01  2.6848632e-01<br>   1.5733847e-01 -7.8884050e-02  4.8131108e-02  2.5263547e-03<br>   2.0420229e-01 -2.0368829e-01  6.8688519e-02 -1.2085145e-01<br>   7.9493999e-02 -5.8916792e-02  7.3894918e-02  9.7813845e-02]]</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817130548690.png" alt="image-20240817130548690"></p><p>改变了一下true_w,观察到了<strong>过拟合现象</strong></p><p>true_w[0:4] &#x3D; np.array([77, 22.4, -221, 22.5])</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817131309496.png" alt="image-20240817131309496"></p><h2 id="权重衰退"><a href="#权重衰退" class="headerlink" title="权重衰退"></a>权重衰退</h2><p>weight decay 通常也被称为：L2正则化</p><p>最常见的处理过拟合的方法</p><ul><li>通过限制参数值的选择范围</li><li>通常不限制偏移b</li></ul><p>使用均方范数作为硬性限制,subject to 表示受限于<br>$$<br>\begin{align*}<br>\min_  \quad l(\mathbf{w}, b)<br>\text{subject to}  |\mathbf{w}|^2\leq 0<br>\end{align*}<br>$$<br>使用均方范数作为柔软性限制,lambda作为超参数，控制了这个正则项的重要程度<br>$$<br>\begin{align*}<br>\min_  \quad l(\mathbf{w}, b) + \frac{\lambda}{2} |\mathbf{w}|^2<br>\end{align*}<br>$$<br><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817194335164.png" alt="image-20240817194335164"></p><p><strong>原理</strong>：<br>过拟合的本质是因为有的参数其实我们并不需要&#x2F;有的参数数值过分偏离，应该让这些参数趋近于0，而损失函数中新的一项的加入，就起到这个作用</p><p>参数更新法则：</p><p>其实就相当于loss函数由两项构成，区别就是求梯度的时候会多减去一项</p><p>在每次更新参数的时候，会先将权重放小，这就是<strong>权重衰退</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817195728302.png" alt="image-20240817195728302"></p><h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><p>生成数据集用到的公式</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240817200749539.png" alt="image-20240817200749539"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span>  torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line"><span class="comment"># 这里的数据集很小/简单 但是特征值却有200个，所以很容易发生过拟合</span></span><br><span class="line">n_train,n_test,num_inputs,batch_size = <span class="number">20</span>,<span class="number">100</span>,<span class="number">200</span>,<span class="number">5</span></span><br><span class="line">true_w,true_b = torch.ones(num_inputs,<span class="number">1</span>)*<span class="number">0.01</span>,<span class="number">0.05</span> <span class="comment"># 权重是列向量</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w,true_b,n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data,batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w,true_b,n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data,batch_size,is_train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(num_inputs,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w,b]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义L2范式惩罚，这里话没有引入lamda</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L2_penalty</span>(<span class="params">W</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(W.<span class="built_in">pow</span>(<span class="number">2</span>))/<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w,b = init_params()</span><br><span class="line">    net = <span class="keyword">lambda</span> X:d2l.linreg(X,w,b) <span class="comment"># 之前定义的线性回归 lambda是匿名函数关键字 X是输入</span></span><br><span class="line">    loss = d2l.squared_loss</span><br><span class="line">    num_epoch,lr =<span class="number">100</span>,<span class="number">0.003</span></span><br><span class="line">    trainer = torch.optim.SGD([w,b],lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            <span class="comment"># 损失函数加上L2范式惩罚</span></span><br><span class="line">            l = loss(net(X),y) + lambd*L2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span>(epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>,train_loss<span class="subst">&#123;d2l.evaluate_loss(net,train_iter,loss):f&#125;</span>&#x27;</span></span><br><span class="line">                  <span class="string">f&#x27;test_loss<span class="subst">&#123;d2l.evaluate_loss(net,test_iter,loss):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w的L2范数是： &quot;</span>,torch.norm(w).item())</span><br><span class="line">train(lambd=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------&quot;</span>)</span><br><span class="line">train(lambd=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#明显过拟合</span></span><br><span class="line">结果：epoch100,train_loss0<span class="number">.000000</span>test_loss74<span class="number">.905751</span></span><br><span class="line">w的L2范数是：  <span class="number">12.894696235656738</span></span><br><span class="line"><span class="comment">#训练误差增加，但是测试误差减小，这就是我们想要的效果</span></span><br><span class="line">epoch100,train_loss0<span class="number">.000719</span>test_loss0<span class="number">.012278</span></span><br><span class="line">w的L2范数是：  <span class="number">0.04233487695455551</span></span><br></pre></td></tr></table></figure><p><strong>简洁实现</strong></p><p>主要的区别在于优化器的定义：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = torch.optim.SGD([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], </span><br><span class="line">    lr=lr)</span><br><span class="line"><span class="comment">#&#x27;weight_decay&#x27;是绝大部分框架的优化算法里面都会提供的选项</span></span><br></pre></td></tr></table></figure><h2 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h2><p>dropout</p><p>一个好的模型需要对输入数据的扰动鲁棒</p><ul><li>使用有噪音的数据等价于Tikhonov正则</li><li>丢弃法：在层之间加入噪音</li></ul><p>X’是加入噪音项后的x，但是我们希望期望不变<br>$$<br>E[X’] &#x3D; x<br>$$<br>dropout的定义：每个元素有p概率变为0，1-p概率变大，这样期望不变<br>$$<br>X’ &#x3D;<br>\begin{cases}<br>  0 &amp; \text{with probability } p, \<br>  \frac{x_i}{1-p} &amp; \text{otherwise}.<br>\end{cases}<br>$$<br>dropout通常作用在隐藏全连接层的输出上<br>$$<br>h &#x3D; \sigma(\mathbf{W_1} \mathbf{x} + b_1) \<br>h’ &#x3D; \text{dropout}(h) \<br>\hat{\mathbf{y}} &#x3D; \mathbf{W}_2 h’ + b_2 \<br>y &#x3D; \text{softmax}(\hat{\mathbf{y}})<br>$$<br><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818104912646.png" alt="image-20240818104912646"></p><p><strong>正则项只在训练中使用，用于参数的更新</strong></p><p><strong>但是在推理&#x2F;预测的时候，我们正常计算就可以，也就是说此时丢弃法直接返回输入</strong><br>$$<br>\mathbf{h} &#x3D; \text{dropout}(\mathbf{h})<br>$$<br>dropout的诞生理念是用多个子神经网络取平均，但是在实际实验过程中，其效果和正则项差不多，所以现在主流认为dropout是一个正则项</p><h3 id="代码实现-3"><a href="#代码实现-3" class="headerlink" title="代码实现"></a>代码实现</h3><p>softmax函数封装在CrossEntropyLoss函数内部</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X,dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span>&lt;=dropout&lt;=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    <span class="comment"># 随机生成向量(rand：0-1的正态分布），大于dropout为1，小于dropout为0</span></span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask*X/(<span class="number">1.0</span>-dropout)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;test&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># X = torch.arange(16,dtype=torch.float32).reshape((4,4))</span></span><br><span class="line"><span class="comment"># print(X)</span></span><br><span class="line"><span class="comment"># print(dropout_layer(X,1))</span></span><br><span class="line"><span class="comment"># print(dropout_layer(X,0))</span></span><br><span class="line"><span class="comment"># print(dropout_layer(X,0.5))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型 有两个隐藏层的多层感知机</span></span><br><span class="line">num_inputs,num_outputs,num_hiddens1,num_hiddens2 = <span class="number">784</span>,<span class="number">10</span>,<span class="number">256</span>,<span class="number">256</span></span><br><span class="line">dropout1 = <span class="number">0.2</span></span><br><span class="line">dropout2 = <span class="number">0.5</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span> (nn.Module): <span class="comment">#括号内表示net继承自nn.Module</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_inputs,num_outputs,num_hiddens1,num_hiddens2,</span></span><br><span class="line"><span class="params">              is_training = <span class="literal">True</span></span>):</span><br><span class="line">       <span class="comment"># 为net类的self对象调用父类的_init_函数</span></span><br><span class="line">       <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">       self.num_inputs = num_inputs</span><br><span class="line">       self.training = is_training</span><br><span class="line">       self.lin1 = nn.Linear(num_inputs,num_hiddens1)</span><br><span class="line">       self.lin2 = nn.Linear(num_hiddens1,num_hiddens2)</span><br><span class="line">       self.lin3 = nn.Linear(num_hiddens2,num_outputs)</span><br><span class="line">       self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward 是一个定义模型前向传播的具体行为的方法</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">       H1 = self.relu(self.lin1(X.reshape(-<span class="number">1</span>, self.num_inputs)))</span><br><span class="line">       <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">           H1 = dropout_layer(H1, dropout1)</span><br><span class="line">       H2 = self.relu(self.lin2(H1))</span><br><span class="line">       <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">           H2 = dropout_layer(H2, dropout2)</span><br><span class="line">       out = self.lin3(H2)</span><br><span class="line">       <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br><span class="line">num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;---concise----------------------------------------------------------------------------------------&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 主要区别还是在模型的定义上</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818114334999.png" alt="image-20240818114334999"></p><p>如果不用dropout，出现了过拟合的情况</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818115008315.png" alt="image-20240818115008315"></p><h2 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h2><p>当模型的深度比较大时，求梯度的两个问题<br>$$<br>1.5^{100} &#x3D; 4 * 10^{17}\<br>0.8^{100} &#x3D; 2*10^{-10}<br>$$</p><ul><li><p>梯度爆炸</p><ul><li>对学习率敏感<ul><li>lr太大-&gt;大参数值-&gt;大梯度</li><li>lr太小-&gt;训练无进展</li><li>可能需要在训练过程中不断调整学习率</li></ul></li></ul></li><li><p>梯度消失 </p><ul><li>梯度值变为0 <ul><li>不管学习率怎么取，都没有进展</li><li>对底部尤为严重，仅仅顶部层训练的好，无法让神经网络更深</li></ul></li></ul></li></ul><p><strong>让训练更加稳定</strong></p><p>目标：控制梯度值得方位</p><ul><li>将乘法变加法 ResNet ，LSTM</li><li>归一化  梯度归一化，梯度裁剪</li><li>合理的权重初始和激活函数</li></ul><p>这一节主要展示第三点：<strong>模型初始化</strong></p><p>让每一层的输出方差都是一个常数，这是我们的<strong>设计目标</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818132235329.png" alt="image-20240818132235329"></p><p> <strong>权重初始化</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818132525410.png" alt="image-20240818132525410"></p><p>以MLP为例，如果我们希望每一层的输出的方差相同，需要满足<br>$$<br>\mathbf{n_{t-1}}\mathbf{\sigma} &#x3D; 1\<br>\mathbf{n_{t}}\mathbf{\sigma} &#x3D; 1<br>$$<br>其中n_t表示第t层的输出的维度，sigma是每一层权重的方差</p><p><strong>公式推导：</strong></p><p>输出的方差相同</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818133105087.png" alt="image-20240818133105087"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818133114054.png" alt="image-20240818133114054"></p><p>梯度的方差相同</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818133513812.png" alt="image-20240818133513812"></p><h3 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h3><p>$$<br>要同时满足\mathbf{n_{in}}\mathbf{\sigma} &#x3D; 1 和<br>\mathbf{n_{out}}\mathbf{\sigma} &#x3D; 1是困难的<br>$$</p><p>因此，我们做出一定的权衡：<br>$$<br>\frac{1}{2}(\mathbf{n_{in}}+\mathbf{n_{out}})\mathbf{\sigma}^2 &#x3D;1<br>$$<br>从而，我们可以得到<strong>初始化权重的方法</strong>：<br>$$<br>Xavier初始化从均值为零，方差\<br>\sigma^2 &#x3D; \frac{2}{n_\mathrm{in} + n_\mathrm{out}}<br>的高斯分布中采样权重。<br>$$<br>注意：每一层的初始化函数都不相同，只要满足这个条件，就可以保证每一层的输出的方差和每一层的梯度的方差大致相同</p><p>同理，<strong>激活函数设置的原则</strong>：尽量减小对输出值方差、梯度方差的影响<br>$$<br>\mathbf{f}(\mathbf{x}) &#x3D; \mathbf{x}的影响就是最小的<br>$$<br>所以，我们可以在0附近采取泰勒展开等方式，让激活函数向f函数靠拢</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240818140735616.png" alt="image-20240818140735616"></p><p>最后，别忘了，我们限制权重方位，调整优化函数的目的是：</p><p><strong>保持数值稳定性</strong></p><h1 id="实战：Kaggle房价预测"><a href="#实战：Kaggle房价预测" class="headerlink" title="实战：Kaggle房价预测"></a>实战：Kaggle房价预测</h1><p>比赛链接</p><p><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a></p><p>比赛的形式是，Kaggle给出训练数据集和测试数据集</p><p><strong>测试数据集</strong>没有标签，用于最后的评估</p><p>我们需要自行将训练数据集进行分割，一部分用作<strong>验证数据集</strong></p><h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><p>使用api接口下载数据集的方法：<a href="https://blog.csdn.net/weixin_41847262/article/details/126154548">kaggle注册以及数据集下载全流程_kaggle数据集下载-CSDN博客</a></p><p>这里我们直接下载压缩包，用绝对路径访问</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">r&#x27;C:\Users\86181\Desktop\project\deeplearning_code\data\kaggle_house_price\train.csv&#x27;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">r&#x27;C:\Users\86181\Desktop\project\deeplearning_code\data\kaggle_house_price\test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_data.iloc[<span class="number">0</span>:<span class="number">5</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(test_data.iloc[<span class="number">0</span>:<span class="number">5</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>(1460, 81)<br>(1459, 80)<br>   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice<br>0   1          60       RL         65.0       WD        Normal     208500<br>1   2          20       RL         80.0       WD        Normal     181500<br>2   3          60       RL         68.0       WD        Normal     223500<br>3   4          70       RL         60.0       WD       Abnorml     140000<br>4   5          60       RL         84.0       WD        Normal     250000<br>     Id  MSSubClass MSZoning  LotFrontage  YrSold SaleType SaleCondition<br>0  1461          20       RH         80.0    2010       WD        Normal<br>1  1462          20       RL         81.0    2010       WD        Normal<br>2  1463          60       RL         74.0    2010       WD        Normal<br>3  1464          60       RL         78.0    2010       WD        Normal<br>4  1465         120       RL         43.0    2010       WD        Normal</p><p>可见训练数据集有1460个数据，79个特征，1个标签</p><p>测试数据集没有标签</p><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>我们采用来标准化数据<br>$$<br>\mathbf{x} \leftarrow \frac{\mathbf{x}-\mathbf{u}}{\sigma}<br>$$<br>在不指定axis的情况下，.mean和.std函数沿着0轴进行</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">all_features = pd.concat((train_data.iloc[:,<span class="number">1</span>:-<span class="number">1</span>],test_data.iloc[:,<span class="number">1</span>:-<span class="number">1</span>])) <span class="comment"># id不是特征值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;print(all_features.shape)&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 提取数值特征列的索引，也就是列名</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;print(numeric_features[:10])&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std()))</span><br><span class="line"><span class="comment"># 在标准化数据之后，均值为0，因此我们可以将缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>处理离散值（字符串）采用独热编码表示</p><p>采用独热编码表示后，特征会增加</p><p>此时all_features的形状为(2919,300)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在标准化数据之后，均值为0，因此我们可以将缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><strong>通过Values属性,从<code>pandas</code>格式中提取NumPy格式，并将其转换为张量表示</strong></p><p>pandas老版本的独热编码只有0和1，而新版本则改成true和false</p><p>所以要加上all_features &#x3D; all_features * 1，转成0和1</p><p>或者将独热编码写成all_features &#x3D; pd.get_<a href="https://search.bilibili.com/all?from_source=webcommentline_search&keyword=dummies&seid=14398139424397262989">dummies</a>(all_features, dummy_na&#x3D;True,dtype&#x3D;int)，强制转化后的数据类型是int型</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features = all_features * <span class="number">1</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line"><span class="comment"># 测试集的特征</span></span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(</span><br><span class="line">    train_data.SalePrice.values.reshape(-<span class="number">1</span>, <span class="number">1</span>), dtype=torch.float32)</span><br></pre></td></tr></table></figure><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>注意，我们这里用于评估准确度的是房价的误差<br>$$<br>\frac{y-\hat{y}}{y}<br>$$<br><strong>一种方法是用价格预测的对数来衡量差异,这也是比赛官方给出的衡量误差情况的方法</strong><br>$$<br>\sqrt{\frac{1}{n}\sum_{n} \left( \log(y_i) - \log(g_i) \right)^2}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">in_features = all_features.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单层线性回归</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Linear(in_features,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 衡量误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_rmse</span>(<span class="params">net,features,lables</span>):</span><br><span class="line">    clipped_preds = torch.clamp(net(features),<span class="number">1</span>,<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">    rmse = torch.sqrt(loss(clipped_preds.log(),lables.log()))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net,train_features,train_labels,test_features,test_labels,</span></span><br><span class="line"><span class="params">          num_epochs,learning_rate,weight_decay,batch_size</span>):</span><br><span class="line">    train_ls,test_ls = [],[]</span><br><span class="line">    train_iter = d2l.load_array((train_features,train_labels),batch_size)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(),lr = learning_rate,</span><br><span class="line">                                weight_decay = weight_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net,train_features,train_labels))</span><br><span class="line">        <span class="keyword">if</span>(test_labels) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net,test_features,test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls,test_ls</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>一开始写这段代码的时候，return写在了for X，y循环的内部</p><p>导致返回的是第一轮的第一次训练的损失，特别大</p><p><strong>k折交叉验证</strong></p><p>X_part，y_part就是数据集其中的一折</p><p>如果不用作验证集，就拼接到训练集上</p><p>idx是slice对象，如果idx是（0，20）就从0开始索引，到20结束（不包括20）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># K折交叉验证</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_k_fold_data</span>(<span class="params">k,i,X,y</span>):</span><br><span class="line">    <span class="comment"># i是当前被用作验证集的折数的编号</span></span><br><span class="line">    <span class="keyword">assert</span> k &gt;<span class="number">1</span></span><br><span class="line">    <span class="comment"># 每一折的大小 101//5 = 20  注意，多余的部分例如101多出来的那个1，不会当作一折</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># slice 对象，用于索引</span></span><br><span class="line">        idx = <span class="built_in">slice</span>(j * fold_size, (j + <span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part,y_part = X[idx,:],y[idx] <span class="comment"># 获取当前折的数据</span></span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid,y_valid = X_part,y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">    X_train = torch.cat([X_train,X[k*fold_size:]], <span class="number">0</span>)</span><br><span class="line">    y_train = torch.cat([y_train,y[k*fold_size:]], <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 多余的数据拼到训练集里</span></span><br><span class="line">    X_train = torch.cat(X_train,X[k*fold_size:], <span class="number">0</span>)</span><br><span class="line">    y_train = torch.cat(y_train,y[k*fold_size:], <span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure><p>注意：k次训练之间是独立的，重复k次训练只是为了获取更为<strong>客观的误差</strong>，方便后续调参</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">k_fold</span>(<span class="params">k,X_train,y_train,num_epochs,learning_rate,weight_decay,batch_size</span>):</span><br><span class="line">    train_l_sum,valid_l_sum = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="comment"># 每一次拿到第i折的数据，拿去训练</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># 每一次训练都是独立的，因此要重新生成一个net</span></span><br><span class="line">        data = get_k_fold_data(k,i,X_train,y_train)</span><br><span class="line">        net = get_net()</span><br><span class="line">        train_ls,valid_ls = train(net,*data,num_epochs,learning_rate,</span><br><span class="line">                                 weight_decay,batch_size)</span><br><span class="line">        train_l_sum += train_ls[-<span class="number">1</span>] <span class="comment"># 累积误差</span></span><br><span class="line">        valid_l_sum += valid_ls[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 每次训练的误差</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;折<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>，训练log rmse<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;验证log rmse<span class="subst">&#123;<span class="built_in">float</span>(valid_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># k次训练的平均误差</span></span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>定义各种超参数，并进行训练</p><p>这一步可以<strong>反复尝试进行调参</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,</span><br><span class="line">                          weight_decay, batch_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>-折验证: 平均训练log rmse: <span class="subst">&#123;<span class="built_in">float</span>(train_l):f&#125;</span>, &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;平均验证log rmse: <span class="subst">&#123;<span class="built_in">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>折1，训练log rmse0.131836, 验证log rmse0.144131<br>折2，训练log rmse0.129436, 验证log rmse0.146554<br>折3，训练log rmse0.128286, 验证log rmse0.142680<br>折4，训练log rmse0.133345, 验证log rmse0.135618<br>折5，训练log rmse0.126239, 验证log rmse0.166833<br>5-折验证: 平均训练log rmse: 0.129828, <strong>平均验证log rmse: 0.147163</strong></p><p>把num_epochs改成2000-&gt;过拟合</p><p>给net加入一个隐藏层-&gt;过拟合</p><p>5-折验证: 平均训练log rmse: 0.098884, <strong>平均验证log rmse: 0.205867</strong></p><h3 id="提交Kaggle预测"><a href="#提交Kaggle预测" class="headerlink" title="提交Kaggle预测"></a>提交Kaggle预测</h3><p>在调好各种超参数以后，我们在完整的训练集上训练最后一次，然后对测试集进行预测，将结果存放在,csv文件中</p><p>submission.csv文件会放在同一个目录下</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data,</span></span><br><span class="line"><span class="params">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class="line">    net = get_net()</span><br><span class="line">    <span class="comment"># 这次训练用的是完整的train_features，同时不需要验证</span></span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练log rmse：<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将网络应用于测试集。</span></span><br><span class="line">    <span class="comment"># .detach() 方法在PyTorch中用于从当前的计算图中分离出张量，使得这个张量不会参与梯度计算</span></span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    <span class="comment"># 将其重新格式化以导出到Kaggle</span></span><br><span class="line">    test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, -<span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># submission只有两列</span></span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_and_pred(train_features, test_features, train_labels, test_data,</span><br><span class="line">                num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure><p>在kaggle网站提交后，得分0.35246，排名4191</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240819135000707.png" alt="image-20240819135000707"></p><p><strong>kaggle网站上还有别人的优秀代码可以看</strong></p><h1 id="ch04-深度学习计算"><a href="#ch04-深度学习计算" class="headerlink" title="ch04_深度学习计算"></a>ch04_深度学习计算</h1><p>神经网络看起来很复杂，节点很多，层数多，参数更多。但核心部分或组件不多，把这些组件确定后，这个神经网络基本就确定了。这些核心组件包括：<br>（1）层：神经网络的基本结构，将输入张量转换为输出张量。<br>（2）模型：层构成的网络。<br>（3）损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。<br>（4）优化器：如何是损失函数最小，这就涉及到优化器。</p><h2 id="模型构造"><a href="#模型构造" class="headerlink" title="模型构造"></a>模型构造</h2><p>在PyTorch中，<code>Module</code>是所有神经网络组件的基类，它提供了一个框架来构建具有层次结构的模型</p><p>回顾多层感知机的模型定义：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>,<span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>Sequential本质上是一个继承自Module的类，按照顺序执行输入的层</p><p>想直观地了解块是如何工作的，最简单的方法就是自己实现一个</p><p><strong>顺序块：</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment">#这里的 args是我们传入的层</span></span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module <span class="comment"># 按照顺序存放层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p><strong>自定义块：</strong></p><p>当然，我们并不总是只用到顺序快，而只需要修改上述两个函数，我们就可以自定义一个符合我们需求的块</p><p>__inti__函数定义了我们需要的层、参数,__forward__函数定义了前向计算方式</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure><h2 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h2><h3 id="访问参数"><a href="#访问参数" class="headerlink" title="访问参数"></a>访问参数</h3><p>同样回顾多层感知机的模型</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>,<span class="number">8</span>), <span class="comment"># net[0]</span></span><br><span class="line">                    nn.ReLU(),<span class="comment">#net[1]</span></span><br><span class="line">                    nn.Linear(<span class="number">8</span>,<span class="number">1</span>)) <span class="comment"># net[2]</span></span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(net(X))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">OrderedDict([(&#x27;weight&#x27;, tensor([[-0.3272,  0.2718,  0.0853, -0.1085,  0.3155, -0.2714, -0.0330, -0.2700]])), (&#x27;bias&#x27;, tensor([-0.3074]))])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 访问某个数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="comment"># 访问全部参数</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span></span><br><span class="line"><span class="string">(&#x27;0.weight&#x27;, torch.Size([8, 4])) (&#x27;0.bias&#x27;, torch.Size([8])) (&#x27;2.weight&#x27;, torch.Size([1, 8])) (&#x27;2.bias&#x27;, torch.Size([1]))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><code>state_dict()</code>方法用于返回模块（<code>nn.Module</code>）的所有参数和缓存的字典</p><p>如果我们将多个块相互嵌套，参数命名约定是如何工作的?</p><p>注意理解最后输出的三个Sequential的嵌套关系</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (0): Sequential(</span></span><br><span class="line"><span class="string">    (block 0): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    (block 1): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    (block 2): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    (block 3): Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line"><span class="string">      (3): ReLU()</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (1): Linear(in_features=4, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>我们也可以像通过嵌套列表索引一样访问它们</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].bias.data</span><br></pre></td></tr></table></figure><h3 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h3><p><strong>内置初始化：</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net[<span class="number">0</span>].apply(init_normal) <span class="comment"># 对指定层应用</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_constant)</span><br></pre></td></tr></table></figure><p><strong>自定义初始化：</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240823133403145.png" alt="image-20240823133403145"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br></pre></td></tr></table></figure><p><strong>参数绑定</strong></p><p>有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>这个例子表明第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。</p><p>由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</p><h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2><p>其实自定义层和自定义网络没有本质区别，我们只需继承基础层类并实现前向传播功能。</p><p>不带参数的层：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br></pre></td></tr></table></figure><p>带参数的层</p><p><code>in_units</code>和<code>units</code>，分别表示输入数和输出数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line">    </span><br><span class="line"> linear = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。 因此，现在是时候学习如何加载和存储权重向量和整个模型了。</p><h3 id="read-and-write-张量"><a href="#read-and-write-张量" class="headerlink" title="read_and_write_张量"></a>read_and_write_张量</h3><p>对于单个张量，我们可以直接调用<code>load</code>和<code>save</code>函数分别读写它们。 这两个函数都要求我们提供一个名称，<code>save</code>要求将要保存的变量作为输入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>存储一个张量列表</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y],<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="read-and-write-模型"><a href="#read-and-write-模型" class="headerlink" title="read_and_write_模型"></a>read_and_write_模型</h3><p>深度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。</p><p>为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br><span class="line"><span class="comment"># 保存参数</span></span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br><span class="line"><span class="comment"># 恢复模型</span></span><br><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><h2 id="GPU使用"><a href="#GPU使用" class="headerlink" title="GPU使用"></a>GPU使用</h2><p>默认情况下，张量创建在CPU上</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;device(type=&#x27;cpu&#x27;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>使用GPU创建</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>)) <span class="comment"># 在第二块GPU上创建</span></span><br></pre></td></tr></table></figure><p>如果我们希望用X+Y，但二者创建在不同的GPU上，那么可以将<code>X</code>传输到第二个GPU并在那里执行操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意，这里Z 和X 共享相同的数据和梯度</span></span><br><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># </span></span><br><span class="line">Z = X.cuda(<span class="number">1</span>).detach()</span><br><span class="line">Y + Z</span><br></pre></td></tr></table></figure><p>类似地，神经网络模型可以指定设备。 下面的代码将模型参数放在GPU上</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure><h1 id="ch05-卷积神经网络"><a href="#ch05-卷积神经网络" class="headerlink" title="ch05_卷积神经网络"></a>ch05_卷积神经网络</h1><p>“Convolutional Neural Network ——CNN</p><p>适合于计算机视觉的神经网络架构。</p><ol><li><p><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</p></li><li><p><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li></ol><p>简单来说，神经网络通过先看图像的小部分（局部性），然后不管这些小部分在图像中怎么移动（平移不变性），都能把它们认出来。</p><h2 id="从全连接层到卷积"><a href="#从全连接层到卷积" class="headerlink" title="从全连接层到卷积"></a>从全连接层到卷积</h2><ul><li><p>将输入和输出从向量变形为矩阵</p></li><li><p>将权重变形为4-D的张量</p><p>为什么权重变形为4维呢？因为在w_{i,j,k,l}中，i,j代表输出的点在输出矩阵中的位置，k,l代表输入点在输入的图（或者矩阵）中的位置，要想完全记录所有的权重，需要一个4维张量。</p></li><li><p>V是对W的重新索引<br>$$<br>\mathbf{v_{i,j,a,b}} &#x3D; \mathbf{w_{i,j,i+a,j+b}}<br>$$</p></li></ul><p>现在，我们可以将全连接层表示为：其中U表示偏置<br>$$<br>\begin{aligned} \left[\mathbf{H}\right]<em>{i, j} &amp;&#x3D; [\mathbf{U}]</em>{i, j} + \sum_k \sum_l[\mathsf{W}]<em>{i, j, k, l}  [\mathbf{X}]</em>{k, l}\ &amp;&#x3D;  [\mathbf{U}]<em>{i, j} +<br>\sum_a \sum_b [\mathsf{V}]</em>{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}<br>$$<br><strong>#原则1 平移不变性</strong></p><p>假设我们目标的特征值在输入中发生了平移，就像小猫从图片的左下角移动到左上角，我们希望着仅导致隐藏表示H的平移，也就是说<strong>V和U实际上不依赖于（i，j）的数值</strong>，即：<br>$$<br>解决方案：  \mathbf{v_{i,j,a,b}} &#x3D;  \mathbf{v_{a,b}}\<br>即H定义为<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum_a\sum_b [\mathbf{V}]</em>{a, b} [\mathbf{X}]_{i+a, j+b}<br>$$<br><strong>这就是交叉相换</strong>&#x2F;<strong>二维卷积</strong></p><p><strong>#原则2 局部性</strong><br>$$<br>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum_a\sum_b [\mathbf{V}]</em>{a, b} [\mathbf{X}]_{i+a, j+b}<br>$$<br>评估H时，我们不应该用远离x_i,j的参数，相当于我们只考虑附近的元素</p><p>解决方案：<br>$$<br>|a|&gt; \Delta或|b| &gt; \Delta的范围之外，我们可以设置[\mathbf{V}]<em>{a, b} &#x3D; 0。\<br>因此，我们可以将[\mathbf{H}]</em>{i, j}重写为\</p><p>[\mathbf{H}]<em>{i, j} &#x3D; u + \sum</em>{a &#x3D; -\Delta}^{\Delta} \sum_{b &#x3D; -\Delta}^{\Delta} [\mathbf{V}]<em>{a, b}  [\mathbf{X}]</em>{i+a, j+b}<br>$$</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824104448021.png" alt="image-20240824104448021"></p><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>什么是卷积？</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110256292.png" alt="image-20240824110256292"></p><p>用一个卷积核不断地去扫描输入矩阵，点乘后求和映射到输出矩阵</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110015411.png" alt="image-20240824110015411"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110621791.png" alt="image-20240824110621791"></p><p>交叉相关和二维卷积在实际使用中没有区别，只是两个不同的数学概念，我们用的其实是交叉相关，不过习惯叫成卷积层</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824110939967.png" alt="image-20240824110939967"></p><p>一维&#x2F;三维地数据也可用卷积层</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824111106025.png" alt="image-20240824111106025"></p><p><strong>总结：</strong></p><ul><li><strong>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出</strong></li><li><strong>核矩阵和偏移是需要学习的参数</strong></li><li><strong>核矩阵的大小是超参数</strong></li></ul><h3 id="代码实现-4"><a href="#代码实现-4" class="headerlink" title="代码实现"></a>代码实现</h3><p>定义互相关运算</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240824111807444.png" alt="image-20240824111807444"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 互相关运算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X,K</span>):</span><br><span class="line">    h,w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>]-h+<span class="number">1</span>,X.shape[<span class="number">1</span>]-w+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i,j] = (X[i:i+h,j:j+w]*K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>自定义二维卷积层</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 二维卷积层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x,self.weight) + self.bias</span><br></pre></td></tr></table></figure><p>接下来，我们进行一个简单的应用：检测图片中不同颜色的边缘</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X</span></span><br><span class="line"><span class="string">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>如果我们构造卷积核：K &#x3D; torch.tensor([[1.0, -1.0]])</p><p>这个卷积核可以对<strong>垂直边缘</strong>进行检测，水平边缘则不行</p><p><strong>学习卷积核</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，输入通道：1，输出通道：1，卷积核形状：（1，2）</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练结果：</p><p>epoch 2, loss 5.172<br>epoch 4, loss 0.976<br>epoch 6, loss 0.208<br>epoch 8, loss 0.053<br>epoch 10, loss 0.016<br>tensor([[[[-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232],<br>          [-0.0232,  0.9771,  0.0000,  0.0000,  0.0000, -1.0003, -0.0232]]]],<br>       grad_fn&#x3D;<ConvolutionBackward0>)</ConvolutionBackward0></p><h2 id="卷积层的填充和步幅"><a href="#卷积层的填充和步幅" class="headerlink" title="卷积层的填充和步幅"></a>卷积层的填充和步幅</h2><p>给定32X32的输入，应用7层5X5的卷积核，最后输出的大小为4X4，也就是说<strong>最多使用7层卷积层</strong></p><p>在深度学习中，我们希望增加神经网络层数</p><p><strong>填充</strong></p><p>在输入数据周围填上一圈数据，输出的行数+2，列数+2</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825093104898.png" alt="image-20240825093104898"></p><p><strong>步幅</strong></p><p>给定输入大小为224X224，在使用5X5的卷积核的情况下，需要55层才将输出降低到4X4，因此需要大量的计算</p><p>相当于让我们的窗口以更大的幅度移动，这会减小输出的大小</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825094000404.png" alt="image-20240825094000404"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825094230288.png" alt="image-20240825094230288"></p><p><strong>填充和步幅是卷积层的超参数</strong>，填充使输出线性扩大，步幅使输出成倍减小</p><h3 id="代码实现-5"><a href="#代码实现-5" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d,X</span>):</span><br><span class="line">    <span class="comment">#  将输入数据X重塑为适合卷积操作的形状。</span></span><br><span class="line">    <span class="comment">#  这里X原本的形状是(8,8)，重塑后变为(1,1,8,8)</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>,<span class="number">1</span>)+X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3X3的卷积核，填充为1【上下左右各填充1行】</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;torch.Size([8, 8])，可以看到经过一层卷积层后形状不变&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># padding的第一个参数表示填充的行数</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=(<span class="number">5</span>,<span class="number">3</span>),padding=(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#步幅------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 注意到8可以被2整除，会变成4X4矩阵</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (8-3+0+3)/3 = 2  (8-5+2+4)/4 = 2 [向下取整]</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,kernel_size=(<span class="number">3</span>,<span class="number">5</span>),padding=(<span class="number">0</span>,<span class="number">1</span>),stride=(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d,X).shape)</span><br></pre></td></tr></table></figure><h2 id="输入和输出通道"><a href="#输入和输出通道" class="headerlink" title="输入和输出通道"></a>输入和输出通道</h2><p>彩色图片有RGB三个通道，灰度图片有黑白灰三种颜色，一个通道</p><p>如果输入有多个通道，每个通道有都有一个卷积核，结果是所有通道卷积结果的和</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825102123898.png" alt="image-20240825102123898"><br>$$<br>Y &#x3D; \sum_{i&#x3D;0}^{c_{i}} (X_{i,:,:}^{\prime}) \star W_{i,:,:}<br>$$<br>如果我们需要多个输出通道，那么就需要多个<strong>三维卷积核</strong>，<strong>每个三维卷积核生成一个通道</strong><br>$$<br>c_{i}表示输入通道数,c_{o}表示输出通道数\<br>输入 X：c_{i}\times n_{h} \times n_{w} \<br>核W：c_{o} \times c_{i}\times k_{h} \times k_{w}\<br>输出Y :c_{o} \times m_{h} \times m_{w}\<br>Y_{i,:,:} &#x3D; X\star W_{i,:,:,:} ; \text{for i in 1,…}c_{o}<br>$$</p><ul><li><strong>每一个输出通道可以识别特定模式</strong></li><li>三维卷积核识别并组合输入中的模式</li></ul><p>通俗理解，假如我们希望识别图片中的猫，一个三维卷积核识别轮廓，一个识别猫头，一个识别猫毛……最后组合起来，就能将猫识别出来</p><p><strong>1X1卷积层</strong></p><p>卷积核的形状为（1.1），这样的卷积核不识别空间模式，只是融合通道</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825103801816.png" alt="image-20240825103801816"><br>$$<br>相当于输入形状为n_{h}n_{w}\times c_{i} \text{， }权重为c_{i} \times c_{o}的全连接层<br>$$<br><strong>总结</strong></p><p>输出通道数是超参数</p><p>每个输出通道对应一个独立的三维卷积核</p><h3 id="代码实现-6"><a href="#代码实现-6" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多通道的互相关运算</span></span><br><span class="line"><span class="comment"># K是三维卷积核</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X,K</span>):</span><br><span class="line">    <span class="comment"># zip(X,K) 函数将 X 和 K 中的元素配对，形成一系列的元组，每个元组包含一个输入张量和一个卷积核</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x,k) <span class="keyword">for</span> x,k <span class="keyword">in</span> <span class="built_in">zip</span>(X,K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># K是四维卷积核</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X,K</span>):</span><br><span class="line">    <span class="comment"># torch.stack将多个张量沿着一个新的维度合并成一个更大的张量。</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X,k) <span class="keyword">for</span> k <span class="keyword">in</span> K],<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2,3,3)</span></span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"><span class="comment"># (3,2,2,2)</span></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(corr2d_multi_in_out(X, K))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证1X1卷积层等价于全连接层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X,K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h*w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    Y = torch.matmul(K,X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line">X = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X,K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X,K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>((Y1-Y2).<span class="built_in">sum</span>())) &lt; <span class="number">1e-6</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化操作（如最大池化）有助于增加网络对小的平移的不变性。即使特征在池化窗口内发生小的平移，池化层仍然可以捕捉到最重要的特征（例如最大值），也就是说，<strong>使用了池化层后，即便输入发生了一定的平移，输出的结果可以保持不变</strong></p><p>二维最大池化：返回滑动窗口中的最大值</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825121547242.png" alt="image-20240825121547242"></p><p>池化层则对卷积层的输出进行下采样</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825122540693.png" alt="image-20240825122540693"></p><p><strong>池化层并不能像卷积层一样融合多个输入通道的结果，池化层每一个通道都有一个独立的输出</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240825122941515.png" alt="image-20240825122941515"></p><p><strong>总结</strong></p><ul><li>池化层返回窗口的最大值或平均值</li><li>作用是缓解卷积层对位置的敏感性</li><li>同样有窗口大小、填充、步幅作为超参数</li></ul><h3 id="代码实现-7"><a href="#代码实现-7" class="headerlink" title="代码实现"></a>代码实现</h3><p>深度学习框架中的步幅与池化窗口的大小相同，意义是两个窗口之间是没有重叠的</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line"><span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充和步幅</span></span><br><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>) <span class="comment"># 3X3的窗口</span></span><br><span class="line"><span class="comment"># 深度学习框架中的步幅与池化窗口的大小相同，意义是两个窗口之间是没有重叠的</span></span><br><span class="line"><span class="built_in">print</span>(pool2d(X))</span><br><span class="line"><span class="comment"># 手动设定填充和步幅</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>) <span class="comment"># 4-3+1*2+2/2=2</span></span><br><span class="line"><span class="built_in">print</span>(pool2d(X))</span><br><span class="line"></span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(pool2d(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多通道</span></span><br><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>) <span class="comment"># 注意这里沿着0/1拼接的区别</span></span><br><span class="line"><span class="built_in">print</span>(X.shape) <span class="comment"># 1,2,4,4</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(pool2d(X)) <span class="comment"># 可以看到每个通道之间的输出是独立的</span></span><br></pre></td></tr></table></figure><h2 id="卷积神经网络-LeNet"><a href="#卷积神经网络-LeNet" class="headerlink" title="卷积神经网络_LeNet"></a>卷积神经网络_LeNet</h2><p>设计之初是为了实现一个手写数字识别的应用</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240826074439501.png" alt="image-20240826074439501"></p><p>LeNet的输入是的图片，然后放到一个5X5的<strong>卷积层</strong>中，接着采用2X2的<strong>池化层[<strong>步幅与池化层形状相同]，再用5X5的</strong>卷积层</strong>，最后用2X2的<strong>池化层</strong>处理后拉成一个向量，输入到<strong>全连接层</strong>，再输入到<strong>全连接层</strong>，最后输入到<strong>高斯层</strong>【类似于一个softmax】得到输出结果</p><p>注意这个过程中的通道数目变化，只有卷积层会影响通道数目</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240826075326758.png" alt="image-20240826075326758"></p><h3 id="代码实现-8"><a href="#代码实现-8" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(</span><br><span class="line">    Reshape(), <span class="comment"># 将输入图片转化为28X28</span></span><br><span class="line">    <span class="comment"># 28-5+4+1 = 28</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    <span class="comment"># 28-2+2/2 = 14</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 14-5+1 = 10</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,kernel_size=<span class="number">5</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    <span class="comment"># 10-2+2/2 =5</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 16X5X5</span></span><br><span class="line">    <span class="comment"># (1,16,5,5)展平到(1,400)</span></span><br><span class="line">    nn.Flatten(), <span class="comment"># 从第一维展平到最后一维，第0维通常是批量大小</span></span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>,<span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>,<span class="number">10</span>),</span><br><span class="line">    )</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X = torch.rand(size=(1,1,28,28),dtype = torch.float32)</span></span><br><span class="line"><span class="string">for layer in net:</span></span><br><span class="line"><span class="string">    X = layer(X)</span></span><br><span class="line"><span class="string">    print(layer.__class__.__name__,&#x27;output shape:\t&#x27;,X.shape)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">banch_size = <span class="number">256</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size=banch_size)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net,data_iter,device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net,torch.nn.Module): <span class="comment"># 检查net是否为torch.nn.Module类/子类</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            <span class="comment"># 获取参数列表的第一个参数的设备</span></span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X,<span class="built_in">list</span>):</span><br><span class="line">            X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 张量的转化方法</span></span><br><span class="line">            X = X.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        metric.add(d2l.accuracy(net(X),y),y.numel())</span><br><span class="line">        <span class="keyword">return</span> metric[<span class="number">0</span>]/metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net,train_iter,test_iter,num_epochs,lr,device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>,device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>,xlim=[<span class="number">1</span>,num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>,<span class="string">&#x27;train acc&#x27;</span>,<span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer = d2l.Timer()</span><br><span class="line">    <span class="comment"># len(train_iter)是训练集的批次数</span></span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enumerate(train_iter) 会将 train_iter 中的每个元素与其索引（从0开始）封装成元组 (index, (X, y))。</span></span><br><span class="line"><span class="string">        for 循环中的 i 就是这个索引，它表示当前是第几个批次。i 是一个整数，每次循环迭代时递增。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i,(X,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            y=y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat,y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            metric.add(l*X.shape[<span class="number">0</span>],d2l.accuracy(y_hat,y),X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>]/metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>]/metric[<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># 每 num_batches // 5 个批次打印一次，最后一个数据打印一次，相当于打印五次或六次</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="comment"># 训练结束</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">            <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>训练结果：</p><p>loss 0.487, train acc 0.816, test acc 0.840<br>66594.0 examples&#x2F;sec on cuda:0</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240828081313830.png" alt="image-20240828081313830"></p><h1 id="ch06-现代卷积神经网络"><a href="#ch06-现代卷积神经网络" class="headerlink" title="ch06_现代卷积神经网络"></a>ch06_现代卷积神经网络</h1><h2 id="深度卷积神经网络（AlexNet）"><a href="#深度卷积神经网络（AlexNet）" class="headerlink" title="深度卷积神经网络（AlexNet）"></a>深度卷积神经网络（AlexNet）</h2><p>在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。</p><p>另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，认为<strong>特征本身应该被学习</strong>。此外，他们还认为，在合理地复杂性前提下，<strong>特征应该由多个共同学习的神经网络层</strong>组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体<em>AlexNet</em>。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，</p><p>深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素：</p><ul><li><p><strong>数据</strong>，在2009年以前，研究使用的数据集都是非常小的，2009年，<strong>ImageNet</strong>数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。</p></li><li><p><strong>硬件</strong>，图形处理器<em>（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的4×4矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为</em>通用GPU（general-purpose GPUs，GPGPU）来销售。</p><p>当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上<strong>并行化</strong>的操作。 于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新<a href="https://code.google.com/archive/p/cuda-convnet/">cuda-convnet</a>几年来它一直是行业标准，并推动了深度学习热潮。</p></li></ul><p>AlexNet首次证明了<strong>学习到的特征可以超越手工设计的特征</strong></p><p>下图左侧是LeNet的架构，右侧是AlexNet的简化版</p><p>可以看到相比于LeNet，AlexNet的窗口更大，增加了三个卷积层</p><p>更多细节：</p><ul><li>采用ReLU作为激活函数（减缓梯度消失）</li><li>隐藏全连接层后加入了丢弃层</li><li>数据增强（例如会将图片进行随机截取、随机调整亮度、色温等等，可以理解为扩充了数据集）</li></ul><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240828181032751.png" alt="image-20240828181032751"></p><h3 id="代码实现-9"><a href="#代码实现-9" class="headerlink" title="代码实现"></a>代码实现</h3><p>注意，我们这里使用的是Fashion-MNIST数据集，这是因为如果使用ImageNet，会需要数个小时的时间</p><p>Fashion-MNIST中的图片是28X28，我们将它们增加到224X224，通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用AlexNet架构</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span></span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 另外，输出通道的数目远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>), <span class="comment"># 加入噪音，防止过拟合</span></span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察形状</span></span><br><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">num_epochs=<span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240828221115006.png" alt="image-20240828221115006"></p><h2 id="使用块的网络（VGG）"><a href="#使用块的网络（VGG）" class="headerlink" title="使用块的网络（VGG）"></a>使用块的网络（VGG）</h2><p>基本思想：用可重复使用的卷积快来构建深度神经网络</p><p>VGG块：</p><ul><li>3X3卷积核（padding 1）</li><li>2X2最大池化层（stride 2）</li></ul><p>VGG架构：多个VGG块后连接全连接层，不同次数的重复块得到不同的架构：VGG-16,VGG-19</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829073757999.png" alt="image-20240829073757999"></p><p>相较于AlexNet,VGG块使用的窗口更小，同时网络更深</p><h3 id="代码实现-10"><a href="#代码实现-10" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_convs表示一个vgg块中卷积层的数量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs,in_channels,out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels,out_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># convolutional architecture</span></span><br><span class="line">conv_arch = ((<span class="number">1</span>,<span class="number">64</span>),(<span class="number">1</span>,<span class="number">128</span>),(<span class="number">2</span>,<span class="number">256</span>),(<span class="number">2</span>,<span class="number">512</span>),(<span class="number">2</span>,<span class="number">512</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs,out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        <span class="comment"># 注意这里是append的是一个线性层，也就是说每一块之间是隔开的</span></span><br><span class="line">        conv_blks.append(vgg_block(num_convs,in_channels,out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*conv_blks,nn.Flatten(),</span><br><span class="line">                         <span class="comment"># 这里没有泛化，因为输入是224X224才这样写</span></span><br><span class="line">                         nn.Linear(out_channels*<span class="number">7</span>*<span class="number">7</span>,<span class="number">4096</span>),nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                         nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>),nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                         nn.Linear(<span class="number">4096</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次经过vgg块处理后，高宽都除以2，经过5个vgg块后，高宽变为7X7</span></span><br><span class="line"><span class="comment"># 注意到每次高宽减半，通道数都翻倍，这是以这种经典的设计模式</span></span><br><span class="line">net = vgg(conv_arch)</span><br><span class="line">X = torch.randn(size=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算量太大了，所以这里构造小一点的通道数</span></span><br><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829081350419.png" alt="image-20240829081350419"></p><h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><p>Network in Network</p><p>之前的神经网络中，都在卷积层的最后加入了全连接层</p><p>以AlexNet为例，第一个全连接层的参数个数为：256X5X5X4096&#x3D; 26M【将（256，5，5）拉平后银蛇到4096】，这个参数个数是很大的</p><p>NIN的思想就用卷积层代替全连接层，这样能够有更少的参数个数</p><p> <strong>NiN块</strong></p><ul><li>卷积层+两个1X1卷积层，1x1卷积层在保持空间维度不变的情况下，实现跨通道的特征融合，这相当于对每个像素点进行全连接操作，从而在不损失分辨率的前提下增加非线性特性</li><li>起到全连接层的作用</li></ul><p><strong>NiN架构</strong> </p><ul><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层，逐步较小高宽比和增大通道数</li><li>最后使用平均池化层得到输出</li></ul><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829164933918.png" alt="image-20240829164933918"></p><h3 id="代码实现-11"><a href="#代码实现-11" class="headerlink" title="代码实现"></a>代码实现</h3><p><code>nn.AdaptiveAvgPool2d((1,1))</code> 是 PyTorch 库中的一个自适应平均池化层，其作用是将输入的多通道二维特征图转换为具有固定尺寸的特征图，具体来说，这里的输出尺寸被指定为 1x1。参数 <code>(1,1)</code> 指定了输出特征图的高度和宽度，即输出特征图的每个通道都会被压缩成一个单一的数值，这个数值是对应输入通道上所有像素值的平均值</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#  定义NiN块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels,out_channels,kernel_size,strides,padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 参数个数： out_channels*out_channels*1*1</span></span><br><span class="line">        nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 定义NiN模型</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>,<span class="number">96</span>,kernel_size=<span class="number">11</span>,strides=<span class="number">4</span>,padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>,<span class="number">256</span>,kernel_size=<span class="number">5</span>,strides=<span class="number">1</span>,padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>,<span class="number">384</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nin_block(<span class="number">384</span>,<span class="number">10</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># 全局平均池化层，对每个通道内的二维矩阵分别求平均值</span></span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">    nn.Flatten()</span><br><span class="line"> )</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829171427362.png" alt="image-20240829171427362"></p><h2 id="含并行连结的网络（GoogLeNet）"><a href="#含并行连结的网络（GoogLeNet）" class="headerlink" title="含并行连结的网络（GoogLeNet）"></a>含并行连结的网络（GoogLeNet）</h2><p>Google做的，只是致敬LeNet</p><p>Inception块：四个路径从<strong>不同层面抽取信息</strong>，然后在输出通道维合并</p><p>高宽都不变，只有通道数改变</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829172033156.png" alt="image-20240829172033156"></p><p>以输入192X28X28的输入，观察通道数的变化，蓝色块抽取信息，白色块融合通道信息</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829172331822.png" alt="image-20240829172331822"></p><p>跟单3X3&#x2F;5X5卷积层相比，Inception块的参数更少</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829172903646.png" alt="image-20240829172903646"></p><p><strong>GoogLeNet架构</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829173050742.png" alt="image-20240829173050742"></p><p><strong>Inception的后续变种</strong></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829173953903.png" alt="image-20240829173953903"></p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829174424705.png" alt="image-20240829174424705"></p><p>当然这里并不是纯深度的上百层，这里有一点广度学习的思想</p><h3 id="代码实现-12"><a href="#代码实现-12" class="headerlink" title="代码实现"></a>代码实现</h3><p>注意到我们在前面NiN的模型中，使用函数去定义块，这里使用的是类，这是因为这里的块含有<strong>并行结构</strong>，需要我们<strong>定义新的forward函数</strong></p><p>在 Python 类的构造函数 <code>__init__</code> 中，<code>**kwargs</code> 表示“关键字参数”（keyword arguments），它允许你将不定数量的命名参数传递给函数。这些命名参数在函数内部以字典形式存在。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inception</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels,c1,c2,c3,c4,**kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1 1X1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels,c1,kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2 1X1卷积层+3X3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels,c2[<span class="number">0</span>],kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>],c2[<span class="number">1</span>],kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3 1X1卷积层+5X5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels,c3[<span class="number">0</span>],kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>],c3[<span class="number">1</span>],kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4 3X3最大池化层+1X1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels,c4,kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连接输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1,p2,p3,p4),dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># stage 1</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># stage 2</span></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>,<span class="number">192</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># stage 3</span></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>,<span class="number">64</span>,(<span class="number">96</span>,<span class="number">128</span>),(<span class="number">16</span>,<span class="number">32</span>),<span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>,<span class="number">128</span>,(<span class="number">128</span>,<span class="number">192</span>),(<span class="number">32</span>,<span class="number">96</span>),<span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># stage 4</span></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>,<span class="number">192</span>,(<span class="number">96</span>,<span class="number">208</span>),(<span class="number">16</span>,<span class="number">48</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>,<span class="number">160</span>,(<span class="number">112</span>,<span class="number">224</span>),(<span class="number">24</span>,<span class="number">64</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>,<span class="number">128</span>,(<span class="number">128</span>,<span class="number">256</span>),(<span class="number">24</span>,<span class="number">64</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>,<span class="number">112</span>,(<span class="number">144</span>,<span class="number">288</span>),(<span class="number">32</span>,<span class="number">64</span>),<span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>,<span class="number">256</span>,(<span class="number">160</span>,<span class="number">320</span>),(<span class="number">32</span>,<span class="number">128</span>),<span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># stage 5</span></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>,<span class="number">256</span>,(<span class="number">160</span>,<span class="number">320</span>),(<span class="number">32</span>,<span class="number">128</span>),<span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>,<span class="number">384</span>,(<span class="number">192</span>,<span class="number">384</span>),(<span class="number">48</span>,<span class="number">128</span>),<span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整个模型：</span></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="comment"># 展示每一块的形状</span></span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829205212799.png" alt="image-20240829205212799"></p><h2 id="批量归一化（batch-norm"><a href="#批量归一化（batch-norm" class="headerlink" title="批量归一化（batch_norm)"></a>批量归一化（batch_norm)</h2><p>对于一个较深的神经网络</p><ul><li>上层梯度大，底层梯度小</li><li>数据在最底部</li><li>底层的训练较慢</li><li>底层一变化，所有都得跟着变</li><li>这导致顶部的层需要重新学习多次</li><li>导致收敛变慢</li></ul><p>批量归一化的核心思想是规范化（normalization）网络中的激活输出，使其具有固定的均值和方差。<br>$$<br>具体而言，批量归一化层计算这个批量的平均值\mathbf{u}和方差\mathbf{\sigma}\<br>然后对每一个输出进行规范化 \mathbf{x} &#x3D; \lambda\frac{\mathbf{x}-\mathbf{u}}{\sigma}+\mathbf{B}<br>$$<br>不考虑两个参数，归一化后，这一组数据的平均值为0，方差为1</p><p>式子中的lambda和B是可以学习的参数</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829211511069.png" alt="image-20240829211511069"></p><p>作用在特征维的意思是将对每一个特征（每一列）求平均值和方差</p><p>作用在通道维的意思是对每个通道的数据进行归一化，使得每个通道的数据的均值为0，方差为1，相当于一个通道看作一个特征</p><p><strong>批量归一化在做什么？</strong></p><ul><li><p>最初论文是想用它来减少内部协变量转移</p></li><li><p>后续有论文指出它的作用其实是可能是通过在每个小批量里加入随机噪音来控制模型复杂度</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240829212429850.png" alt="image-20240829212429850"></p></li><li><p>因此没有必要和丢弃法混合使用</p></li></ul><p><strong>作用</strong>：可以允许较大的学习率，可<strong>加速收敛速度</strong>，一般不会改变模型的精度</p><h3 id="代码实现-13"><a href="#代码实现-13" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># eps被加到方差的计算中，以确保分母不会变得太小</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">banch_norm</span>(<span class="params">X,gamma,beta,moving_mean,moving_var,eps,momentum</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 预测模式</span></span><br><span class="line">        X_hat = (X-moving_mean)/torch.sqrt(moving_var+eps)</span><br><span class="line">    <span class="keyword">else</span> : <span class="comment"># 训练模式</span></span><br><span class="line">        <span class="comment"># 判断X是否为二维/四维数组</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 二维数组</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X-mean)**<span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 数据有四个维度0，1，2，3，dim=(0,2,3)的意思就是把第一三四维降到1，只保留第二维（核的输入通道）</span></span><br><span class="line">            <span class="comment"># 注意，这里只有mean和var的形状改变为（1，？,1,1)，最后Y的形状和输入的形状是相同的，只是进行了归一化</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X-mean)**<span class="number">2</span>).mean(dim=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdim=<span class="literal">True</span>)</span><br><span class="line">        X_hat = (X-mean)/torch.sqrt(var+eps)</span><br><span class="line">        <span class="comment"># 将当前批量的均值以一定的权重融入到了移动平均均值中。</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma*X_hat + beta</span><br><span class="line">    <span class="keyword">return</span> Y,moving_mean.data,moving_var.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># BatchNorm层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_features,num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>,num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>,num_features,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        Y,self.moving_mean,self.moving_var = banch_norm(X,self.gamma,self.beta,self.moving_mean,self.moving_var,eps=<span class="number">1e-5</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用到LeNet</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>), BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">X = torch.rand(size=(1,1,28,28),dtype = torch.float32)</span></span><br><span class="line"><span class="string">for layer in net:</span></span><br><span class="line"><span class="string">    X = layer(X)</span></span><br><span class="line"><span class="string">    print(layer.__class__.__name__,&#x27;output shape:\t&#x27;,X.shape)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902205208733.png" alt="image-20240902205208733"></p><h2 id="残差网络-ResNet"><a href="#残差网络-ResNet" class="headerlink" title="残差网络(ResNet)"></a>残差网络(ResNet)</h2><p>在下图中，星星是我们想要学习到的最优点，每一块区域是我们可以学到的模型的范围</p><p>范围越大，模型越复杂</p><p>可以看到，在左图中，随着模型越来越复杂，可学习到的最优点离星星的距离并不会更近，而右图则越来越近</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902210226767.png" alt="image-20240902210226767"></p><p>ResNet的核心思想是，将原先的小网络作为新网络的子集，即<br>$$<br>\mathbf{f}(x) &#x3D; \mathbf{g}(x)+x \<br>x代表原先小网络的输出结果<br>$$<br>这种架构称为跳跃连接</p><p>ResNet引入了“残差学习”的概念，即网络学习的是输入和输出之间的残差（或差异），而不是直接学习输出。这意味着网络的每个层不仅尝试映射输入到输出，而且尝试映射输入到输出与输入之间的差异。</p><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902210631163.png" alt="image-20240902210631163"></p><p>ResNet块</p><ul><li>第一块高宽减半</li><li>后接多个高宽不变的ResNet块</li><li>每一块都有一个跳跃连接</li></ul><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902212124537.png" alt="image-20240902212124537"></p><h3 id="代码实现-14"><a href="#代码实现-14" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,input_channels,num_channels,use_1x1conv=<span class="literal">False</span>,stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels,num_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>,stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels,num_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels,num_channels,kernel_size=<span class="number">1</span>,stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ResNet 模型</span></span><br><span class="line"><span class="comment"># 前两层比较常规</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span><br><span class="line"><span class="params">                 first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="comment"># 对于b3,b4，b5的第一个ResNet块，我们希望它具有高宽减半的效果</span></span><br><span class="line">        <span class="comment"># 对于b2，由于前面两层已经采取了高宽减半，所以这里做特殊处理</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/2024/07/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Users/86181/Desktop/project/学习笔记/深度学习/动手学习深度学习/image-20240902214550047.png" alt="image-20240902214550047"></p><p><strong>为什么ResNet可以训练上千层的模型？</strong></p><ul><li>乘法变加法，可以缓解梯度消失。在f(X)+X中，梯度消失的一方会被另一方取代。</li><li><strong>恒等快捷连接</strong>：ResNet通过引入恒等快捷连接（Identity Shortcut Connections），即在网络中添加跳过一层或多层的连接，使得梯度可以直接传播到前面的层。这种设计有效地解决了梯度消失问题，因为它允许梯度在反向传播过程中直接流向网络的较早层级。</li><li>加深模型可以退化为浅层模型</li><li>梯度高速通道，因为有高速通道的存在，底层拿到的梯度也会比较大，收敛速度会更快</li></ul><h1 id="实战：Kaggle图片分类"><a href="#实战：Kaggle图片分类" class="headerlink" title="实战：Kaggle图片分类"></a>实战：Kaggle图片分类</h1><p><a href="https://www.kaggle.com/c/classify-leaves">https://www.kaggle.com/c/classify-leaves</a></p><p>1、关于pd.read_csv函数中header参数的解释：</p><p>默认为0，表示第0行为表头，这样read_csv以后，DataFrame数据中仍然有表头，但是用iloc索引第0行得到的就是数据了</p><p>如果设定header&#x3D;None，表示没有表头，这样得到的数据中第一行为表头</p><p>2、用DataFrame.iloc索引数据的时候，第0列就是数据列，不会包含id列</p><p>3、保存模型的时候，有两种后缀</p><ul><li>.pth: PyTorch 中最常见的模型文件后缀之一，它通常用来保存模型的权重参数（<code>state_dict</code>）。这种方式只保存模型的参数，不保存模型的结构</li><li><code>.ckpt</code> 后缀通常用于保存模型的检查点（checkpoint），它不仅包含模型的权重参数，还可能包含优化器状态、当前epoch、训练历史等信息。这种文件通常用于训练过程中的断点续训，以便在训练过程中断时可以从最近的检查点恢复训练。</li></ul><p>4、..argmax(dim&#x3D;-1)可以返回每一行数值最大处的列索引</p><p>5、.append 函数和.extend函数的区别：</p><ul><li>append：如果添加的是一个列表，那么这个列表会被作为一个整体添加为一个元素</li><li>extend：如果可迭代对象是一个列表，那么它的所有元素将被展开并添加到原列表中</li></ul><h1 id="ch07-硬件介绍"><a href="#ch07-硬件介绍" class="headerlink" title="ch07_硬件介绍"></a>ch07_硬件介绍</h1><p>参考《动手学深度学习》p31-p35</p><h1 id="ch08-计算机视觉"><a href="#ch08-计算机视觉" class="headerlink" title="ch08_计算机视觉"></a>ch08_计算机视觉</h1><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构（更新完毕）</title>
      <link href="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
      <url>/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>教程来源 ：油管搜索：my code school</p><h2 id="什么是数据结构"><a href="#什么是数据结构" class="headerlink" title="什么是数据结构"></a>什么是数据结构</h2><p>数据结构定义：</p><p>a data structure is a way to store and organize data in a computer, </p><p>so that it can be used efficiently.</p><p>我们常说的“数据结构”其实分为两种概念：</p><p>1)ADT:  abstract data types –定义我们想要实现的功能，不讨论实现细节</p><p>2)Implementation（实现）–设计程序实现功能</p><h2 id="链表linked-list"><a href="#链表linked-list" class="headerlink" title="链表linked list"></a>链表linked list</h2><h3 id="为什么需要链表？"><a href="#为什么需要链表？" class="headerlink" title="为什么需要链表？"></a>为什么需要链表？</h3><p>在使用数组申请内存空间的时候，数组的大小往往是限定好的，例如: int A[4];这样就申请了4*4&#x3D;16个字节的空间</p><p>当你想要插入第五个元素的时候，由于这块空间附近的空间可能已经被内存管理器分配给了其他变量，这导致数组不具有扩展性。</p><p>想要扩展，只能重新创建一个数组，然后把原来的数据复制进去，由于需要遍历所有元素，因此时间复杂度为O(n)，空间复杂度也为O(n)</p><p>因此，我们需要一个<strong>更具有扩展性的数据结构</strong>——<strong>链表</strong></p><h3 id="链表的扩展性"><a href="#链表的扩展性" class="headerlink" title="链表的扩展性"></a>链表的扩展性</h3><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240709104317964.png" alt="image-20240709104317964"></p><p> 数组的特点是申请一整块连续的内存块</p><p>而链表可扩展性的秘诀在于为每一个数据申请单独的内存块（分散）</p><p>对于数组，我们可以通过内存块的首地址+数组的位置计算出某一元素所在的位置，例如数组A的首地址为200，那么A[4]就是200+4*4&#x3D;216</p><p>但是对于链表，由于内存块是分散的，如何找到对应的地址就是一个新的问题</p><p>解决方案是，在一个块中，既存放数据，又存放下一个元素（内存块）的地址</p><p>也就是说一个内存块存放两个变量，一个是<u>特定类型的元素</u>，另一个是<u>下一个元素的数据类型对应的指针</u></p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240709104501024.png" alt="image-20240709104501024"></p><h3 id="节点-node"><a href="#节点-node" class="headerlink" title="节点-node"></a>节点-node</h3><p>把每一个内存块（包括元素和指针）看成一个整体，这样的整体称为：node 节点</p><p>在C&#x2F;C++中，用结构体<strong>struct</strong>定义这样一个节点</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">    <span class="type">int</span> data; <span class="comment">// 存储数据  4字节</span></span><br><span class="line">    Node* next; <span class="comment">// 指向下一个节点的指针  4字节</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>注意这里的指针类型为 Node* ，表明这里存放的是下一个节点的地址</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240709110040745.png" alt="image-20240709110040745"></p><h3 id="时间-空间复杂度"><a href="#时间-空间复杂度" class="headerlink" title="时间&#x2F;空间复杂度"></a>时间&#x2F;空间复杂度</h3><p>1、在链表中，插入一个数据需要创建节点-找到插入的位置-连接</p><p>因此对应的时间复杂度为O(n），空间复杂度为O(1)</p><p>2、删除数据需要找到删除的位置-删除节点-<a href>重新连接</a></p><p>因此对应的时间复杂度为O(n），空间复杂度为O(1)</p><h3 id="C-实现-单向链表"><a href="#C-实现-单向链表" class="headerlink" title="C++实现-单向链表"></a>C++实现-单向链表</h3><p>单向链表：single linked list </p><h4 id="定义节点"><a href="#定义节点" class="headerlink" title="定义节点"></a>定义节点</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span>&#123;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">    Node *next;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span> * head;<span class="comment">//全局变量或者写在main函数里面都可以</span></span><br></pre></td></tr></table></figure><p>接口：</p><h4 id="1、从头-尾插入"><a href="#1、从头-尾插入" class="headerlink" title="1、从头&#x2F;尾插入"></a>1、从头&#x2F;尾插入</h4><p>注意节点必须创建在堆区，如果创建在栈区，会在函数调用借结束后释放内存。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Insert</span> <span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    Node *temp = <span class="keyword">new</span> <span class="built_in">Node</span>();<span class="comment">//创建节点</span></span><br><span class="line">    temp-&gt;data = x;</span><br><span class="line">    temp-&gt;next = head;<span class="comment">//连接</span></span><br><span class="line">    head = temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Insert_end</span><span class="params">(<span class="type">int</span>  x)</span></span>&#123;</span><br><span class="line">    Node * new_node = <span class="keyword">new</span> <span class="built_in">Node</span>();</span><br><span class="line">    new_node-&gt;data= x;</span><br><span class="line">    new_node-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">    Node* temp = head;</span><br><span class="line">    <span class="comment">//定位    </span></span><br><span class="line">    <span class="keyword">while</span>(temp-&gt;next!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">        temp = temp-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    temp-&gt;next=new_node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2、在任意位置插入"><a href="#2、在任意位置插入" class="headerlink" title="2、在任意位置插入"></a>2、在任意位置插入</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Insert</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> index)</span></span>&#123;</span><br><span class="line">    Node* new_node =<span class="keyword">new</span> <span class="built_in">Node</span>(); </span><br><span class="line">    new_node-&gt;data = x;</span><br><span class="line">    new_node-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">    <span class="comment">//在这里1代表初始位置</span></span><br><span class="line">    <span class="keyword">if</span>( index == <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        new_node-&gt;next = head;</span><br><span class="line">        head = new_node;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定位    </span></span><br><span class="line">    Node *temp= head;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;index<span class="number">-2</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>( temp != <span class="literal">NULL</span>)</span><br><span class="line">        temp=temp-&gt;next;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;输入的位置无效！&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    new_node-&gt;next=temp-&gt;next;</span><br><span class="line">    temp-&gt;next=new_node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3、输出链表"><a href="#3、输出链表" class="headerlink" title="3、输出链表"></a>3、输出链表</h4><p>a.迭代方式实现</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//这里的head是局部变量的head，不是全局变量，所以不会影响全局变量head</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Output</span><span class="params">( Node * head)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(head!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">        cout&lt;&lt;head-&gt;data&lt;&lt;  <span class="string">&quot; -&gt; &quot;</span>   &lt;&lt;endl;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>b.递归方式实现</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//递归方式输出链表</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Print</span><span class="params">( Node * p)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(p==<span class="literal">NULL</span>) <span class="comment">// 退出条件</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; p-&gt;data &lt;&lt; <span class="string">&quot;-&gt; &quot;</span>;</span><br><span class="line">    <span class="built_in">Print</span>(p-&gt;next);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//反向输出</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Print</span><span class="params">( Node * p)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(p==<span class="literal">NULL</span>) <span class="comment">// 退出条件</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">Print</span>(p-&gt;next);</span><br><span class="line">    cout &lt;&lt; p-&gt;data &lt;&lt; <span class="string">&quot;&lt;- &quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4、在任意位置删除节点"><a href="#4、在任意位置删除节点" class="headerlink" title="4、在任意位置删除节点"></a>4、在任意位置删除节点</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Delete</span><span class="params">( <span class="type">int</span> index)</span></span>&#123;</span><br><span class="line">    Node * temp = head;</span><br><span class="line">    <span class="comment">//如果要删去开头节点</span></span><br><span class="line">    <span class="keyword">if</span> ( index == <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        head = temp-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> temp;</span><br><span class="line">        <span class="keyword">return</span>; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//定位到前一个节点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i =<span class="number">0</span> ;i &lt; index<span class="number">-2</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>( temp-&gt;next != <span class="literal">NULL</span>)</span><br><span class="line">        temp=temp-&gt;next;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;输入的位置无效！&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    Node * temp1 = temp-&gt;next;</span><br><span class="line">    temp-&gt;next = temp1-&gt;next;</span><br><span class="line">    <span class="keyword">delete</span> temp1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5、链表反转"><a href="#5、链表反转" class="headerlink" title="5、链表反转"></a>5、链表反转</h4><p>a.迭代方式实现</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Node ** 是因为要传入的是 &amp;head</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Reverse</span><span class="params">(Node ** head)</span></span>&#123;</span><br><span class="line">    Node* prev = <span class="literal">NULL</span>;</span><br><span class="line">    Node* current = *head;</span><br><span class="line">    Node* next = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (current != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        next = current-&gt;next; <span class="comment">// 保存下一个节点</span></span><br><span class="line">        current-&gt;next = prev; <span class="comment">// 反转当前节点的指针</span></span><br><span class="line">        prev = current;       <span class="comment">// 移动 prev 到当前节点</span></span><br><span class="line">        current = next;       <span class="comment">// 移动 current 到下一个节点</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    *head = prev; <span class="comment">// 更新头指针</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>b.递归方式实现</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这里的head是全局变量</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">REverse</span><span class="params">(Node  * p)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( p-&gt;next==<span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        head=p;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">REverse</span>( p-&gt;next );</span><br><span class="line">    p-&gt;next-&gt;next = p;</span><br><span class="line">    p-&gt;next = <span class="literal">NULL</span> ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//假如head是局部变量</span></span><br><span class="line"><span class="comment">//REverse函数需要返回第一个节点的地址</span></span><br><span class="line"><span class="function">Node* <span class="title">REverse</span> <span class="params">( Node * p)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( p-&gt;next==<span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">       <span class="keyword">return</span> p;</span><br><span class="line">    &#125;</span><br><span class="line">    Node * head = <span class="built_in">REverse</span>( p-&gt;next );</span><br><span class="line">    p-&gt;next-&gt;next = p;</span><br><span class="line">    p-&gt;next = <span class="literal">NULL</span> ;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="C-实现-双向链表"><a href="#C-实现-双向链表" class="headerlink" title="C++实现-双向链表"></a>C++实现-双向链表</h3><p>doubly linked list</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240712172237302.png" alt="image-20240712172237302"></p><h4 id="定义节点-1"><a href="#定义节点-1" class="headerlink" title="定义节点"></a>定义节点</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span>  <span class="title class_">Node</span> &#123;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">    Node * next;</span><br><span class="line">    Node * prev;<span class="comment">// previous</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1、从头-尾插入-1"><a href="#1、从头-尾插入-1" class="headerlink" title="1、从头&#x2F;尾插入"></a>1、从头&#x2F;尾插入</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">InsertAtHead</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node * temp = <span class="keyword">new</span> <span class="built_in">Node</span>();</span><br><span class="line">    temp-&gt;data=x;</span><br><span class="line">    temp-&gt;prev=<span class="literal">NULL</span>;</span><br><span class="line">    temp-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">if</span>(head == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        head=temp;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    temp-&gt;next=head;</span><br><span class="line">    head-&gt;prev=temp;</span><br><span class="line">    head=temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">InsertAtTail</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    Node * NewNode = <span class="keyword">new</span> <span class="built_in">Node</span>();</span><br><span class="line">    NewNode-&gt;data=x;</span><br><span class="line">    NewNode-&gt;prev=<span class="literal">NULL</span>;</span><br><span class="line">    NewNode-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    Node * temp = head;</span><br><span class="line">    <span class="keyword">while</span>(temp-&gt;next != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        temp = temp-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    NewNode-&gt;prev=temp;</span><br><span class="line">    temp-&gt;next=NewNode;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2、在任意位置插入-1"><a href="#2、在任意位置插入-1" class="headerlink" title="2、在任意位置插入"></a>2、在任意位置插入</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">ReversePrint</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Node * temp = head;</span><br><span class="line">    <span class="keyword">if</span>(temp == <span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">while</span>(temp-&gt;next != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        temp=temp-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(temp != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        cout&lt;&lt;temp-&gt;data&lt;&lt;<span class="string">&quot;&lt;-&gt;&quot;</span>;</span><br><span class="line">        temp=temp-&gt;prev;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span>  <span class="title">Insert</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> index)</span></span>&#123;</span><br><span class="line">    Node * NewNode = <span class="keyword">new</span> <span class="built_in">Node</span>( );</span><br><span class="line">    NewNode-&gt;data = x;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (index == <span class="number">1</span>)&#123;<span class="comment">//从头插入</span></span><br><span class="line">        NewNode-&gt;next=head;</span><br><span class="line">        head-&gt;prev=NewNode;</span><br><span class="line">        head=NewNode;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Node * temp= head;</span><br><span class="line">    <span class="comment">//定位到目标位置前一位</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;index<span class="number">-2</span>;i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (temp != <span class="literal">NULL</span>)</span><br><span class="line">        temp = temp-&gt;next;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;输入的位置无效&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    Node * NextNode = temp-&gt;next;</span><br><span class="line"></span><br><span class="line">    NewNode-&gt;next=NextNode;</span><br><span class="line">    <span class="keyword">if</span>(NextNode != <span class="literal">NULL</span>)</span><br><span class="line">    NextNode-&gt;prev=NewNode;</span><br><span class="line">    temp-&gt;next=NewNode;</span><br><span class="line">    NewNode-&gt;prev=temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3、输出链表-1"><a href="#3、输出链表-1" class="headerlink" title="3、输出链表"></a>3、输出链表</h4><p>迭代方式和递归方式的实现和单链表相同，不再赘述</p><p>反向输出：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">ReversePrint</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Node * temp = head;</span><br><span class="line">    <span class="keyword">if</span>(temp == <span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">while</span>(temp-&gt;next != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        temp=temp-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(temp != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        cout&lt;&lt;temp-&gt;data&lt;&lt;<span class="string">&quot;&lt;-&gt;&quot;</span>;</span><br><span class="line">        temp=temp-&gt;prev;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="栈stack"><a href="#栈stack" class="headerlink" title="栈stack"></a>栈stack</h2><h3 id="ADT"><a href="#ADT" class="headerlink" title="ADT"></a>ADT</h3><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240717121055963.png" alt="image-20240717121055963"></p><p>只能访问栈顶,**last-in-first-out(LIFO)**最后加入的元素最先出去</p><p>定义：a list with the restriction that insertion and deletion can be performed only from one end , called the top.</p><p>operations:</p><p>Push(添加元素)、Pop(移出元素)、Top（返回栈顶的元素）、IsEmpty()这些操作的时间复杂度都是O(1) </p><p><u>在很多软件中，ctrl+z撤回的操作就是用栈来实现的</u></p><h3 id="数组实现"><a href="#数组实现" class="headerlink" title="数组实现"></a>数组实现</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MAX_SIZE 101</span></span><br><span class="line"><span class="type">int</span> A[MAX_SIZE];</span><br><span class="line"><span class="type">int</span> top = <span class="number">-1</span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Push</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( top == MAX_SIZE<span class="number">-1</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;ERROR: stack overflow&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// top++;</span></span><br><span class="line">    <span class="comment">// A[top]=x;</span></span><br><span class="line">    A[++top] = x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Pop</span> <span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(top == <span class="number">-1</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;ERROR: No element to pop&quot;</span>&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    top--;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Top</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(top == <span class="number">-1</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;ERROR: No element&quot;</span>&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> A[top];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsEmpty</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(top==<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br></pre></td></tr></table></figure><p>时间复杂度：</p><p>如果采用<u>动态数组</u>（当数组满了以后创建一个大小为原来的两倍的数组）</p><p>一次push的最好时间复杂度为O(1),最差为O(n)</p><p>n次push的时间复杂度为O(n),平均每次为O(1)</p><h3 id="链表实现"><a href="#链表实现" class="headerlink" title="链表实现"></a>链表实现</h3><p>相比数组实现，链表实现不需要考虑oveflow(溢出)的问题</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span>&#123;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">    Node * next;</span><br><span class="line">&#125;;</span><br><span class="line">Node* top = <span class="literal">NULL</span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Push</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    Node * temp = <span class="keyword">new</span> <span class="built_in">Node</span>();</span><br><span class="line">    temp-&gt;data=x;</span><br><span class="line">    temp-&gt;next=top;</span><br><span class="line">    top = temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Pop</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(top == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;Error: Stack is empty&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    Node * temp;</span><br><span class="line">    temp = top;</span><br><span class="line">    top = top-&gt;next;</span><br><span class="line">    <span class="keyword">delete</span> temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">TopElement</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(top == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;Error: Stack is empty&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> top-&gt;data;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsEmpty</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(top==<span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="栈的应用"><a href="#栈的应用" class="headerlink" title="栈的应用"></a>栈的应用</h3><h4 id="1、反转字符串"><a href="#1、反转字符串" class="headerlink" title="1、反转字符串"></a>1、反转字符串</h4><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240717143514898.png" alt="image-20240717143514898"></p><p>在C++的标准库STL（standard template library)中已经有一个stack库</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stack&gt;</span><span class="comment">//stack 库</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">//函数的时间复杂度为O(n)</span></span><br><span class="line"><span class="comment">//函数的空间复杂度为O(n)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Reverse</span><span class="params">(<span class="type">char</span> C[],<span class="type">int</span> n )</span></span>&#123;</span><br><span class="line">    stack&lt;<span class="type">char</span>&gt; S;<span class="comment">//创建一个char类型的栈S</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        S.<span class="built_in">push</span>(C[i]);</span><br><span class="line">    &#125;<span class="comment">//时间复杂度O(n)</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        cout&lt;&lt;S.<span class="built_in">top</span>();</span><br><span class="line">        S.<span class="built_in">pop</span>();</span><br><span class="line">    &#125;<span class="comment">//时间复杂度O(n)</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">char</span> C[<span class="number">51</span>];</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;Enter a string:  &quot;</span>;</span><br><span class="line">    cin.<span class="built_in">getline</span>(C,<span class="number">51</span>);</span><br><span class="line">    cout&lt;&lt;<span class="built_in">strlen</span>(C);</span><br><span class="line">    <span class="built_in">Reverse</span>(C,<span class="built_in">strlen</span>(C));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然，在反转字符的时候，这并不是最好的算法</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Reverse</span><span class="params">(<span class="type">char</span> C[],<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> j = n<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(;i&lt;j;i++,j--)&#123;</span><br><span class="line">        <span class="type">char</span> temp = C[i];</span><br><span class="line">        C[i]= C[j];</span><br><span class="line">        C[j]= temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">char</span> C[<span class="number">51</span>];</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;Enter a string:  &quot;</span>;</span><br><span class="line">    cin.<span class="built_in">getline</span>(C,<span class="number">51</span>);</span><br><span class="line">    cout&lt;&lt;<span class="built_in">strlen</span>(C);</span><br><span class="line">    <span class="built_in">Reverse</span>(C,<span class="built_in">strlen</span>(C));</span><br><span class="line">    cout&lt;&lt;C;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个算法的时间复杂度为O(n)空间复杂度为O(1);</p><h4 id="2、反转链表"><a href="#2、反转链表" class="headerlink" title="2、反转链表"></a>2、反转链表</h4><p>实际上，使用递归的方法反转一个链表就使用了隐式的栈(implicit stack)</p><p>下面是使用显式的栈反转链表的实现：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="type">void</span> <span class="title">Reverse</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( head == <span class="literal">NULL</span> )<span class="keyword">return</span>;</span><br><span class="line">    stack&lt;Node*&gt; S;<span class="comment">//构建栈</span></span><br><span class="line">    Node * temp = head;</span><br><span class="line">    <span class="keyword">while</span>(temp != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        S.<span class="built_in">push</span>(temp);</span><br><span class="line">        temp = temp-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    temp = S.<span class="built_in">top</span>();</span><br><span class="line">    head = temp;</span><br><span class="line">    S.<span class="built_in">pop</span>();</span><br><span class="line">    <span class="keyword">while</span>(!S.<span class="built_in">empty</span>())&#123;</span><br><span class="line">        temp-&gt;next = S.<span class="built_in">top</span>();</span><br><span class="line">        S.<span class="built_in">pop</span>();</span><br><span class="line">        temp=temp-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3、检查括号匹配性"><a href="#3、检查括号匹配性" class="headerlink" title="3、检查括号匹配性"></a>3、检查括号匹配性</h4><p>记得有一次上机测试我用计算规定当出现（时候+1，当出现）时候-1，同时限制数字之和不为负数【防止出现“)(”的情况】这种方法解出了题目</p><p>但是我的这种方法无法判定（【）】这种式子是否正确</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240717211428517.png" alt="image-20240717211428517"></p><p>那么，如何解决这个问题呢？</p><p>把左括号逐个丢入栈中，每次遇到右括号，都先判断一下其与相邻的括号是否配对，如果配对，就将相邻的括号pop掉（有点像消消乐），如果不配对，说明括号不配对</p><p>这里运用到了<strong>last in firtst out</strong> 的思想，具体表现为<strong>最后进入的最早闭合</strong></p><p>有的编译器&#x2F;编辑器（如vscode）可以检测出代码括号不配对这种错误，其实就是用栈实现的</p><p>具体实现：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">CheckBalanceParenthesis</span><span class="params">(<span class="type">char</span> A[])</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n = <span class="built_in">strlen</span>(A);</span><br><span class="line">    stack&lt;<span class="type">char</span>&gt; s;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(A[i] == <span class="string">&#x27;[&#x27;</span>||A[i] == <span class="string">&#x27;(&#x27;</span> || A[i] == <span class="string">&#x27;&#123;&#x27;</span>)</span><br><span class="line">            s.<span class="built_in">push</span>(A[i]);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(A[i] == <span class="string">&#x27;]&#x27;</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s.<span class="built_in">empty</span>() || s.<span class="built_in">top</span>() != <span class="string">&#x27;[&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                s.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(A[i] == <span class="string">&#x27;)&#x27;</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s.<span class="built_in">empty</span>() || s.<span class="built_in">top</span>() != <span class="string">&#x27;(&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                s.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(A[i] == <span class="string">&#x27;&#125;&#x27;</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s.<span class="built_in">empty</span>() || s.<span class="built_in">top</span>() != <span class="string">&#x27;&#123;&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                s.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> s.<span class="built_in">empty</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//main函数（测试用）</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="type">char</span> A[<span class="number">10</span>];</span><br><span class="line">        cin.<span class="built_in">getline</span>(A,<span class="number">10</span>);</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">CheckBalanceParenthesis</span>(A))</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;括号配对&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">else</span> cout&lt;&lt;<span class="string">&quot;括号不配对&quot;</span>&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="前缀、中缀、后缀"><a href="#前缀、中缀、后缀" class="headerlink" title="前缀、中缀、后缀"></a>前缀、中缀、后缀</h3><p>prefix infix postfix</p><p>操作数：operand</p><p>操作符：operator</p><p>对于一个表达式，我们可以有中缀表达式、前缀表达式、后缀表达式</p><p>它们的含义是相同的，我们日常中使用比较频繁的是中缀表达式</p><p>但是，中缀表达式存在歧义问题，例如3+5*2的计算，是通过规定 * 的优先级在+之上才有了唯一的结果，除此之外，中缀表达式还通过规定括号的优先级，多个连续的幂运算的优先级来避免歧义</p><p>而逻辑学家和数学家则找到了<strong>能够有效避免歧义的表示方法</strong>，即前缀表达式(波兰表达式)和后缀表达式(逆波兰表达式 )，这两种表达式<strong>不需要规定运算符的优先级和结合性，更不用使用括号</strong></p><p>从编程的角度而言，后缀表达式是<u>最容易解析、求值的时间和内存代价最小</u>的</p><p>中缀表达式不需要空格，但后缀和前缀表达式需要空格区分，例如2333*意义不明，22 33 * 和 223 3 *则明确</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240717220508557.png" alt="image-20240717220508557"></p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240718095209122.png" alt="image-20240718095209122"></p><h4 id="后缀表达式的计算："><a href="#后缀表达式的计算：" class="headerlink" title="后缀表达式的计算："></a>后缀表达式的计算：</h4><p>注意到这些运算符都是二元运算符，每个运算符要找到它的两个操作数（<strong>从左往右</strong>）</p><p>例如：2 3 * 5 4 * + 9 - ， 第一个* 的操作数是2 3 ，第二个为 5 和 4，即化为 6 20 + 9 -，+的操作数为6 20 即：26 9 - ，-的操作数为 26 9，即17</p><p><strong>这个计算过程可以用栈很好地进行</strong></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">EvaluatePostfix</span><span class="params">(<span class="type">char</span> A[])</span> </span>&#123;</span><br><span class="line">    stack&lt;<span class="type">int</span>&gt; s;</span><br><span class="line">    string numStr = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; A[i] != <span class="string">&#x27;\0&#x27;</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isdigit</span>(A[i])) &#123;</span><br><span class="line">            numStr += A[i];</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(A[i] == <span class="string">&#x27; &#x27;</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span> (!numStr.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">                    <span class="comment">//例如当出现 23 5...的时候，push的是23而不是2和3</span></span><br><span class="line">                    s.<span class="built_in">push</span>(<span class="built_in">stoi</span>(numStr));</span><br><span class="line">                    numStr = <span class="string">&quot;&quot;</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">int</span> op2=s.<span class="built_in">top</span>();</span><br><span class="line">                s.<span class="built_in">pop</span>();</span><br><span class="line">                <span class="type">int</span> op1=s.<span class="built_in">top</span>();</span><br><span class="line">                s.<span class="built_in">pop</span>();</span><br><span class="line">                <span class="keyword">switch</span>(A[i])&#123;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">&#x27;+&#x27;</span>:</span><br><span class="line">                        s.<span class="built_in">push</span>(op1+op2);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">&#x27;-&#x27;</span>:</span><br><span class="line">                        s.<span class="built_in">push</span>(op1-op2);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">                        s.<span class="built_in">push</span>(op1*op2);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">&#x27;/&#x27;</span>:</span><br><span class="line">                        s.<span class="built_in">push</span>(op1/op2);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> s.<span class="built_in">top</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>前缀表达式的计算同理</p><p>用代码计算前缀表达式的时候，其实只需要改动一步：从后往前遍历，这样就可以像计算后缀表达式一样利用栈来计算了</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 从后向前遍历字符数组 A</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="built_in">strlen</span>(A) - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">if</span> (A[i] &gt;= <span class="string">&#x27;0&#x27;</span> &amp;&amp; A[i] &lt;= <span class="string">&#x27;9&#x27;</span>) &#123;</span><br><span class="line">            numStr += A[i];</span><br><span class="line">        &#125; </span><br></pre></td></tr></table></figure><h4 id="表达式的转换"><a href="#表达式的转换" class="headerlink" title="表达式的转换"></a>表达式的转换</h4><p>中缀-&gt;后缀</p><p>注意到中缀转后缀的过程中，操作符的次序可能变化，但是操作数的次序不变</p><p>因此读取到操作数时，可以直接将其放入表达式中</p><p>那么如何确定运算符的次序呢？</p><p>有一种算法是：</p><p>当新的操作符的优先级比栈顶操作符的优先级高时，压入栈</p><p>反之，栈顶元素弹出，直到将新元素压入栈为止</p><p>括号处理：（直接压入栈中，出现），就使栈顶元素弹出，直到”（“弹出，注意弹出的（不需要加入表达式中。</p><p>在这个过程中，如果遇到操作数，直接加入表达式即可，同时最后如果栈中还有操作符，依次弹出即可</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240718150806487.png" alt="image-20240718150806487"></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//判断优先级</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">precedence</span><span class="params">(<span class="type">char</span> op)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (op == <span class="string">&#x27;+&#x27;</span> || op == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (op == <span class="string">&#x27;*&#x27;</span> || op == <span class="string">&#x27;/&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">string <span class="title">InfixToPostfix</span><span class="params">(string infix)</span> </span>&#123;</span><br><span class="line">    stack&lt;<span class="type">char</span>&gt; operators;</span><br><span class="line">    string postfix = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; infix.<span class="built_in">length</span>(); i++)&#123;</span><br><span class="line">        <span class="type">char</span> c = infix[i];</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isdigit</span>(c)) &#123;</span><br><span class="line">            <span class="comment">// 将多位数作为一个整体处理</span></span><br><span class="line">            <span class="keyword">while</span> (i &lt; infix.<span class="built_in">length</span>() &amp;&amp; <span class="built_in">isdigit</span>(infix[i])) &#123;</span><br><span class="line">                postfix += infix[i];</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            i--; <span class="comment">// 回退一步，因为外层循环还会再增加一次</span></span><br><span class="line">            postfix += <span class="string">&#x27; &#x27;</span>; <span class="comment">// 在数字后面添加空格，以分隔数字</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//正常情况下中缀表达式不需要使用括号，出现了就忽略即可 </span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;(&#x27;</span>) &#123;</span><br><span class="line">            operators.<span class="built_in">push</span>(c);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;)&#x27;</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span> (!operators.<span class="built_in">empty</span>() &amp;&amp; operators.<span class="built_in">top</span>() != <span class="string">&#x27;(&#x27;</span>) &#123;</span><br><span class="line">                postfix += operators.<span class="built_in">top</span>();</span><br><span class="line">                operators.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            operators.<span class="built_in">pop</span>(); <span class="comment">// 弹出 &#x27;(&#x27;</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//新操作符的优先级小于等于栈顶操作符-&gt;栈顶弹出</span></span><br><span class="line">            <span class="comment">//新操作符的优先级大于栈顶操作符    -&gt;压入</span></span><br><span class="line">            <span class="keyword">while</span> (!operators.<span class="built_in">empty</span>() &amp;&amp; operators.<span class="built_in">top</span>() != <span class="string">&#x27;(&#x27;</span> &amp;&amp; <span class="built_in">precedence</span>(c) &lt;= <span class="built_in">precedence</span>(operators.<span class="built_in">top</span>())) &#123;</span><br><span class="line">                postfix += operators.<span class="built_in">top</span>();</span><br><span class="line">                operators.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            operators.<span class="built_in">push</span>(c);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (!operators.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        postfix += operators.<span class="built_in">top</span>();</span><br><span class="line">        operators.<span class="built_in">pop</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> postfix;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>中缀-&gt;前缀</p><p>具体流程和上文中缀-&gt;后缀的算法类似，只需要保证在弹出操作符的时候，将操作符加在表达式的最前方</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">string <span class="title">InfixToPrefix</span><span class="params">(string infix)</span></span>&#123;</span><br><span class="line">    stack&lt;<span class="type">char</span>&gt;operators;</span><br><span class="line">    string prefix = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;infix.<span class="built_in">length</span>();i++)&#123;</span><br><span class="line">        <span class="type">char</span> c = infix[i];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">isdigit</span>(c))&#123;</span><br><span class="line">            <span class="keyword">while</span>(i&lt;infix.<span class="built_in">length</span>() &amp;&amp; <span class="built_in">isdigit</span>(infix[i]))&#123;</span><br><span class="line">                prefix +=infix[i];</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            i--;</span><br><span class="line">            <span class="comment">//单引号，因为要添加是字符不是字符串</span></span><br><span class="line">            prefix +=<span class="string">&#x27; &#x27;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(c == <span class="string">&#x27; &#x27;</span>)&#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(c == <span class="string">&#x27;(&#x27;</span>)&#123;</span><br><span class="line">            operators.<span class="built_in">push</span>(c);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(c == <span class="string">&#x27;)&#x27;</span>)&#123;</span><br><span class="line">            <span class="keyword">while</span>(operators.<span class="built_in">top</span>()!=<span class="string">&#x27;(&#x27;</span> &amp;&amp; !operators.<span class="built_in">empty</span>())&#123;</span><br><span class="line">                prefix = operators.<span class="built_in">top</span>()+prefix;<span class="comment">//接在前面</span></span><br><span class="line">                operators.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            operators.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//弹出直到能够压入</span></span><br><span class="line">            <span class="keyword">while</span>(!operators.<span class="built_in">empty</span>()&amp;&amp; <span class="built_in">precedence</span>(operators.<span class="built_in">top</span>())&gt;=<span class="built_in">precedence</span>(c))&#123;</span><br><span class="line">                prefix = operators.<span class="built_in">top</span>()+prefix;<span class="comment">//接在前面</span></span><br><span class="line">                operators.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            operators.<span class="built_in">push</span>(c);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(!operators.<span class="built_in">empty</span>())&#123;</span><br><span class="line">        prefix = operators.<span class="built_in">top</span>()+prefix;<span class="comment">//接在前面</span></span><br><span class="line">        operators.<span class="built_in">pop</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> prefix;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="队列Queues"><a href="#队列Queues" class="headerlink" title="队列Queues"></a>队列Queues</h2><h3 id="ADT-1"><a href="#ADT-1" class="headerlink" title="ADT"></a>ADT</h3><p>First-In-First-Out</p><p>定义：A list or collection with the restriction that isertion can be performed at one end (rear) and deletion can be performed at other end (front)</p><p>队尾进，队头出</p><p>operations:</p><p>EnQueue&#x2F;push :从队尾插入元素</p><p>DeQueue&#x2F;pop : 从队首移出元素</p><p>front&#x2F;peek: 返回队首元素</p><p>IsEmpty：查看是否为空</p><p>时间复杂度都是O（1）</p><p><u>栈是一段开口的容器，队列就是两端开口的容器</u></p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240719090231956.png" alt="image-20240719090231956"></p><p>下面将给出两种简单的实现方式</p><p>ps：C++标准库中的<code>queue</code>模板类通常使用双端队列（<code>deque</code>）作为其底层容器，而不是数组或链表。</p><h3 id="数组实现-1"><a href="#数组实现-1" class="headerlink" title="数组实现"></a>数组实现</h3><p>规定一个数组的某一边为front，另一边为rear</p><p>用变量记录这两个位置的索引，黄色的范围就是队列的范围，其他部分备用</p><p>用数组实现队列有一个有趣的现象，如果规定左侧为front，右侧为rear，可以发现队列会一直向右侧移动</p><p>如果想要最大化使用数组的内存，可以利用<strong>循环数组</strong>的概念</p><p>此时current position &#x3D; i, next position &#x3D;(i+1)%N, prv position &#x3D; (i+N-1)%N,这样一直增加i，就可以不断循环遍历数组的每一个位置，如果使用<strong>循环数组</strong>，那么队列满的条件是：</p><p>(rear+1)%N&#x3D;front，即使rear的下一位就是front</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240719104359815.png" alt="image-20240719104359815"></p><p>以下代码没有使用循环数组</p><p>但是利用了动态数组，在队列满了以后，会创建一个两倍原大小的新数组用于存放队列</p><p>使用先需要先调用init 函数进行初始化</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Queues</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> * A;</span><br><span class="line">    <span class="type">int</span> size;</span><br><span class="line">    <span class="type">int</span> front =<span class="number">-1</span>;</span><br><span class="line">    <span class="type">int</span> rear  =<span class="number">-1</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">//初始化函数</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        A = <span class="keyword">new</span> <span class="type">int</span>[x];</span><br><span class="line">        size = x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">IsEmpty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (front == <span class="number">-1</span> &amp;&amp; rear == <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//空</span></span><br><span class="line">        <span class="keyword">if</span> (front == <span class="number">-1</span> &amp;&amp; rear == <span class="number">-1</span>) &#123;</span><br><span class="line">            front = rear = <span class="number">0</span>;</span><br><span class="line">            A[rear] = x;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//满</span></span><br><span class="line">        <span class="keyword">if</span> (rear == size) &#123;</span><br><span class="line">            <span class="built_in">QueueIsFull</span>();</span><br><span class="line">            rear++;</span><br><span class="line">            A[rear] = x;</span><br><span class="line">            <span class="keyword">return</span> ; </span><br><span class="line">        &#125;</span><br><span class="line">        rear++;</span><br><span class="line">        A[rear] = x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (front == <span class="number">-1</span> &amp;&amp; rear == <span class="number">-1</span>) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Error: Queue is empty&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (front == rear) &#123;</span><br><span class="line">            front = rear = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        front++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//放回队首元素</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">Front</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (front == <span class="number">-1</span> &amp;&amp; rear == <span class="number">-1</span>) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Error: Queue is empty&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> A[front];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//返回队尾元素</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">back</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (front == <span class="number">-1</span> &amp;&amp; rear == <span class="number">-1</span>) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Error: Queue is empty&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> A[rear];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//处理数组满了的情况</span></span><br><span class="line">    <span class="comment">//无需手动调用</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">QueueIsFull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//最坏的情况是原数组的所有位置都填满了</span></span><br><span class="line">        <span class="type">int</span> newSize = <span class="number">2</span> * (size);</span><br><span class="line">        <span class="type">int</span>* B = <span class="keyword">new</span> <span class="type">int</span>[newSize];</span><br><span class="line">        <span class="type">int</span> temp = front;</span><br><span class="line">        <span class="type">int</span> lengh = rear - front;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= lengh; i++) &#123;</span><br><span class="line">            B[i] = A[temp];</span><br><span class="line">            temp++;</span><br><span class="line">        &#125;</span><br><span class="line">        A = B;</span><br><span class="line">        front = <span class="number">0</span>;</span><br><span class="line">        rear = lengh;</span><br><span class="line">        size = newSize;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = front; i &lt;= rear; i++) &#123;</span><br><span class="line">            cout &lt;&lt; A[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//测试案例</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Queues q1;</span><br><span class="line">    q1.<span class="built_in">init</span>(<span class="number">5</span>);</span><br><span class="line">    q1.<span class="built_in">push</span>(<span class="number">1</span>);</span><br><span class="line">    q1.<span class="built_in">push</span>(<span class="number">2</span>);</span><br><span class="line">    q1.<span class="built_in">push</span>(<span class="number">3</span>);</span><br><span class="line">    q1.<span class="built_in">push</span>(<span class="number">4</span>);</span><br><span class="line">    q1.<span class="built_in">push</span>(<span class="number">5</span>);</span><br><span class="line">    q1.<span class="built_in">pop</span>();</span><br><span class="line">    q1.<span class="built_in">Print</span>();<span class="comment">// 2 3 4 5 </span></span><br><span class="line">    cout&lt;&lt;endl;</span><br><span class="line"></span><br><span class="line">    q1.<span class="built_in">push</span>(<span class="number">121</span>);</span><br><span class="line">    q1.<span class="built_in">push</span>(<span class="number">50000</span>);<span class="comment">//满了</span></span><br><span class="line">    q1.<span class="built_in">Print</span>();</span><br><span class="line">    <span class="comment">//2 3 4 5 121 50000</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="链表实现-1"><a href="#链表实现-1" class="headerlink" title="链表实现"></a>链表实现</h3><p>正常的链表只需要一个head记录头节点的地址就可以</p><p>这里需要再加上一个rear指针记录最后一个节点的地址</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240719131429378.png" alt="image-20240719131429378"></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Queues</span>&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">Node</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> data;</span><br><span class="line">        Node * next;</span><br><span class="line">    &#125;;</span><br><span class="line">    Node* front = <span class="literal">NULL</span>;</span><br><span class="line">    Node * rear = <span class="literal">NULL</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">        Node * temp = <span class="keyword">new</span> <span class="built_in">Node</span>();</span><br><span class="line">        temp-&gt;data = x;</span><br><span class="line">        temp-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">if</span>(front == <span class="literal">NULL</span> &amp;&amp; rear == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            front = rear = temp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//队尾进</span></span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            rear-&gt;next = temp;</span><br><span class="line">            rear = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//队头出</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Pop</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(front == <span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">if</span>(front == rear)&#123;</span><br><span class="line">            <span class="keyword">delete</span> front;</span><br><span class="line">            front = rear = <span class="literal">NULL</span>;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Node * temp = front;</span><br><span class="line">        front = front-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">IsEmpty</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(front == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get_front</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(front == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;Error: queues is empty&quot;</span>&lt;&lt;endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> front-&gt;data; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get_back</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(rear == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;Error: queues is empty&quot;</span>&lt;&lt;endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> rear-&gt;data;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Print</span><span class="params">()</span></span>&#123;</span><br><span class="line">        Node * temp = front;</span><br><span class="line">        <span class="keyword">while</span>(temp != <span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;temp-&gt;data&lt;&lt;<span class="string">&quot;-&gt;&quot;</span>;</span><br><span class="line">            temp = temp-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="树tree"><a href="#树tree" class="headerlink" title="树tree"></a>树tree</h2><h3 id="ADT-2"><a href="#ADT-2" class="headerlink" title="ADT"></a>ADT</h3><p>是一种层级结构（非线性）</p><p>是多个节点的集合，最高的节点称之为<strong>根</strong>（root）</p><p>各节点的关系：</p><p><strong>parent</strong> <strong>children</strong> <strong>sibling</strong><strong>ancenstor</strong>(例如节点1是所有节点的祖先) <strong>descendent</strong> <strong>cousin</strong></p><p>没有子节点的节点称为<strong>叶</strong>（leaf）</p><p>N个节点的树，有N-1条边（edge）因为每个节点（除了root）都只有一条传入的边</p><p><strong>depth</strong>：length of path from root to x 例如节点5的深度为2</p><p><strong>层级</strong>：深度为2，可以称为level-2(L-2），root 为level-0（L-2）</p><p><strong>Height</strong>：No. of edges in longest path from x to a leaf例如节点3的高度是2，leaf的高度是0</p><p>height of tree：就是root的height，下面这棵树的高度是3</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240719134607138.png" alt="image-20240719134607138"></p><p><strong>Binary Tree</strong>：树中的每个节点至多有两条传出的边</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240719135927990.png" alt="image-20240719135927990"></p><p>应用：</p><p>1)天然具备层级的数据，例如磁盘驱动器的文件系统</p><p>2)快速查找、插入、删除</p><p>3)动态的拼写检查</p><p>4)网络路由算法</p><h3 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h3><p>每个节点最多有两个孩子，一个称为left-child 另一个称为right-child</p><p><strong>严格二叉树</strong>strict&#x2F;proper binary tree：每个节点要么有两个孩子，要么有0个孩子</p><p><strong>完全二叉树</strong>Complete Binary tree: 叶子结点只能出现在最下层和次下层，且最下层的叶子结点集中在树的左部。（<strong>满二叉树</strong>perfect Binary tree是特殊的完全二叉树）</p><p>第i层的最大节点数为2^i, 总最大节点数为2^(h+1)-1,其中h是二叉树的深度&#x2F;高度</p><p>换句话说,完全二叉树深度 h&#x3D;log2 (n+1)-1 &#x3D; log2(n) 向下取整</p><p><strong>平衡二叉树</strong> Balanced binary tree ：每一个节点的左子树和右子树的高度差不超过K（mostly 1)</p><p>定义平衡二叉树是因为有时我们不希望一棵树的高度太高</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>1、结构体实现：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span>&#123;</span><br><span class="line"><span class="type">int</span> data;</span><br><span class="line">Node * left;</span><br><span class="line">Node * right;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>2、数组实现</p><p>这种方法<strong>只对完全二叉树成立</strong></p><p>将节点编号后存放入数组（编号就是在数组中的索引）</p><p>对于索引为i的节点，其左孩为的索引为：2i+1,右孩为2i+2</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240720122748165.png" alt="image-20240720122748165"></p><h3 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h3><h4 id="ADT-3"><a href="#ADT-3" class="headerlink" title="ADT"></a>ADT</h4><p>适用于数据的快速搜索与更新</p><p>如果我们使用数组，查找的时间复杂度：O(n)，插入O(1),删除O(n),同时如果插入的数据超过了数组的容量，需要重新创建一个数组，成本是O(n)</p><p>如果使用链表：查找：O(n),插入（假设在链表头插入）O(1),删除O(n)</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240720124148440.png" alt="image-20240720124148440"></p><p>利用数组<strong>二分查找</strong>的时间复杂度： O（logn）</p><p>解释：</p><p>进行k次查找操作后，搜索范围将缩小到原来的一半的k次方，即n&#x2F;2^k。当n&#x2F;2 ^k小于等于1时，即k大于等于log2(n)，搜索范围将缩小到1，即找到了目标元素或搜索范围为空。所以，二分查找的时间复杂度为O(log n)，其中n是数组的长度。</p><p>但是这要求每次插入数据都要保证顺序，也就是说倒是Insert的时间复杂度（如果insert也采用二分查找）<strong>找到插入位置</strong>的时间复杂度为O(logn)，而<strong>插入数据</strong>的时间复杂度为O(n)</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240720125308635.png" alt="image-20240720125308635"></p><p>假如每秒可以进行100万次计算，即1次比较耗时：10^-6s （1微秒）</p><p>那么对于2^31个长度的数据，查找一个数据只需要 31微秒</p><p>如果用普通的数组&#x2F;链表去查询，查找会可能花费超过2000秒</p><p><strong>BST——Binary Search Tree</strong></p><p>BST的三个操作的时间成本都是O(log n)</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240720130041411.png" alt="image-20240720130041411"></p><p><strong>BST的定义：</strong></p><p>任一节点均不小于其左<strong>后代</strong>，且不大于其右<strong>后代</strong></p><p>注意这里的要求是“<strong>后代</strong>”而<strong>不是</strong>孩子</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/v2-50d1b50e9048325020d17ae6b990f61a_r.jpg" alt="img"></p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240720130327841.png" alt="image-20240720130327841"></p><p>如果将上图的12改为16，那么对于value为15的根节点来说，其有一个左后代比它大，这就不是一颗二叉搜索树</p><p><strong>查找过程（search）：</strong></p><p>先比较目标数据与root的大小，如果小，就到左子树中查找，大就到右子树</p><p>重复这个过程，直到找到与目标数据相等的节点</p><p>每次查找，都会丢弃一半的查找范围，n-&gt;n&#x2F;2-&gt;n&#x2F;4……</p><p>n&#x2F;2^k &lt;&#x3D; 1 时， k&gt;&#x3D;log2(n),即该操作的<strong>时间复杂度为O(logn)</strong></p><p>其实这也是一种二分查找，当然，这要求二分搜索树是<strong>平衡</strong>的</p><p><strong>插入过程（insert）：</strong></p><p>重复查找过程，直到某节点的左孩&#x2F;右孩有空位</p><p>查找过程耗费**O(log n)**，创建节点并连接只耗费常数时间</p><p><strong>删除过程（remove）：</strong></p><p>同样的，查找到位置**O(log n)**，删除节点并重新连接只耗费常数时间</p><p><strong>上述三个过程中的递归调用次数与深度h正相关，也就是说空间复杂度为O(h)</strong></p><p><strong>最差情况下的空间复杂度为O(n)</strong></p><p><strong>平均情况&#x2F;最好情况下的空间复杂度为O(log(n))</strong></p><p><strong>C++的STL的set库&#x2F;map库的底层是自平衡的二叉搜索树（红黑树）</strong></p><p>红黑树是一种自平衡的二叉搜索树，它通过旋转和重新着色等操作来保持树的平衡，从而保证最坏情况下的时间复杂度为O(log n)。而BST可能是不平衡的，在最坏情况下，时间复杂度为O(n)。</p><h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><p>以下代码中的check是为了在第一次调用递归函数的时候将局部变量的数值赋值为树的根，这样写是因为我将root和各种接口都封装在了一个类中，这种写法并不常见</p><p>更常见的写法是在调用函数的时候直接传入root，即Search(100,root)，这样就不需要利用check赋初值</p><h5 id="Search-Insert-Remove"><a href="#Search-Insert-Remove" class="headerlink" title="Search&#x2F;Insert&#x2F;Remove"></a>Search&#x2F;Insert&#x2F;Remove</h5><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BST</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">        <span class="type">int</span> data;</span><br><span class="line">        Node* left;</span><br><span class="line">        Node* right;</span><br><span class="line">    &#125;;</span><br><span class="line">    Node* root = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">int</span> check = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">//查找函数</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">Search</span><span class="params">(<span class="type">int</span> x, Node* root = <span class="literal">NULL</span>)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//赋初值</span></span><br><span class="line">        <span class="keyword">if</span> (check == <span class="number">0</span>) &#123;</span><br><span class="line">            root = <span class="keyword">this</span>-&gt;root;</span><br><span class="line">            check++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            check--; <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (x == root-&gt;data) &#123;</span><br><span class="line">            check--; <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (x &lt; root-&gt;data) &#123;</span><br><span class="line">             <span class="keyword">return</span> <span class="built_in">Search</span>(x, root-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (x &gt; root-&gt;data) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">Search</span>(x, root-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//插入函数</span></span><br><span class="line">    <span class="function">Node* <span class="title">Insert</span><span class="params">(<span class="type">int</span> x, Node* root = <span class="literal">NULL</span>)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>-&gt;root = <span class="built_in">GetNewNode</span>(x);</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//赋初值</span></span><br><span class="line">        <span class="keyword">if</span> (check == <span class="number">0</span>) &#123;</span><br><span class="line">            root = <span class="keyword">this</span>-&gt;root;</span><br><span class="line">            check++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//退出条件</span></span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            root = <span class="built_in">GetNewNode</span>(x);</span><br><span class="line">            check--; <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (x &lt;= root-&gt;data) &#123;</span><br><span class="line">            root-&gt;left = <span class="built_in">Insert</span>(x, root-&gt;left);</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (x &gt; root-&gt;data) &#123;</span><br><span class="line">            root-&gt;right = <span class="built_in">Insert</span>(x,root-&gt;right);</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//删除函数</span></span><br><span class="line">    <span class="function">Node*  <span class="title">Remove</span><span class="params">(<span class="type">int</span> x, Node* root = <span class="literal">NULL</span>)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//赋初值</span></span><br><span class="line">        <span class="keyword">if</span> (check == <span class="number">0</span>) &#123;</span><br><span class="line">            root = <span class="keyword">this</span>-&gt;root;</span><br><span class="line">            check++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//没找到</span></span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            check--; <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//找的过程</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (x &lt; root-&gt;data) &#123;</span><br><span class="line">            root-&gt;left=<span class="built_in">Remove</span>(x, root-&gt;left);</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (x &gt; root-&gt;data) &#123;</span><br><span class="line">            root-&gt;right=<span class="built_in">Remove</span>(x, root-&gt;right);</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//Remove函数容易存在的问题：</span></span><br><span class="line">        <span class="comment">//用指针A将目标对应的内存释放掉了，但是node-&gt;left却成为了野指针</span></span><br><span class="line">        <span class="comment">//因此将NULL放回给上一个node的对应left/right指针是很有必要的</span></span><br><span class="line">        <span class="comment">// 也就是所Remove函数的放回类型应该是Node*而不是void </span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//找到了</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//case 1: no child</span></span><br><span class="line">            <span class="keyword">if</span> (root-&gt;left == <span class="literal">NULL</span> &amp;&amp; root-&gt;right == <span class="literal">NULL</span>) &#123;</span><br><span class="line">                <span class="keyword">delete</span> root;</span><br><span class="line">                root = <span class="literal">NULL</span>;</span><br><span class="line">                check--; <span class="keyword">return</span> root;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//case 2: one child</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (root-&gt;left == <span class="literal">NULL</span>) &#123;</span><br><span class="line">                Node* temp = root;</span><br><span class="line">                root = root-&gt;right;</span><br><span class="line">                <span class="keyword">delete</span> temp;</span><br><span class="line">                check--; <span class="keyword">return</span> root;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (root-&gt;right == <span class="literal">NULL</span>) &#123;</span><br><span class="line">                Node* temp = root;</span><br><span class="line">                root = root-&gt;left;</span><br><span class="line">                <span class="keyword">delete</span> temp;</span><br><span class="line">                check--; <span class="keyword">return</span> root;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//case 3: two child</span></span><br><span class="line">           <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//将右子树中最小的数值作为root的新数值（因为这个数字满足BST需要的条件）   </span></span><br><span class="line">                <span class="comment">//使用左子树的最大值也可以</span></span><br><span class="line">                root-&gt;data = <span class="built_in">FindMin</span>(root-&gt;right);</span><br><span class="line">                root-&gt;right = <span class="built_in">Remove</span>(<span class="built_in">FindMin</span>(root-&gt;right), root-&gt;right);</span><br><span class="line">                check--; <span class="keyword">return</span> root;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//查找一颗树的最小值的函数</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">FindMin</span><span class="params">(Node* temp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;Empty tree&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (temp-&gt;left != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            temp = temp-&gt;left;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> temp-&gt;data;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">Node* <span class="title">GetNewNode</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        Node* temp = <span class="keyword">new</span> <span class="built_in">Node</span>();</span><br><span class="line">        temp-&gt;data = x;</span><br><span class="line">        temp-&gt;left = temp-&gt;right = <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">return</span> temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//show 第二层的右节点（测试用）</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">ShowL2_right</span><span class="params">()</span></span>&#123;</span><br><span class="line">        Node * temp =<span class="keyword">this</span>-&gt;root;</span><br><span class="line">        temp = temp-&gt;right;</span><br><span class="line">        <span class="keyword">return</span> temp-&gt;data;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    BST bst;</span><br><span class="line">    bst.<span class="built_in">Insert</span>(<span class="number">5</span>);</span><br><span class="line">    bst.<span class="built_in">Insert</span>(<span class="number">100</span>);</span><br><span class="line">    bst.<span class="built_in">Insert</span>(<span class="number">66</span>);</span><br><span class="line">    bst.<span class="built_in">Insert</span>(<span class="number">330</span>);</span><br><span class="line">    bst.<span class="built_in">Insert</span>(<span class="number">120</span>);</span><br><span class="line">    cout &lt;&lt; bst.<span class="built_in">Search</span>(<span class="number">120</span>) &lt;&lt; endl;<span class="comment">//1</span></span><br><span class="line">    cout &lt;&lt; bst.<span class="built_in">Search</span>(<span class="number">66</span>) &lt;&lt; endl;<span class="comment">//1</span></span><br><span class="line">    cout &lt;&lt; bst.<span class="built_in">Search</span>(<span class="number">330</span>) &lt;&lt; endl;<span class="comment">//1</span></span><br><span class="line">    cout &lt;&lt; bst.<span class="built_in">Search</span>(<span class="number">121</span>) &lt;&lt; endl;<span class="comment">//0</span></span><br><span class="line">    <span class="comment">//          5</span></span><br><span class="line">    <span class="comment">//         &lt;--&gt;100</span></span><br><span class="line">    <span class="comment">//          66 &lt;-&gt;330</span></span><br><span class="line">    <span class="comment">//             120&lt;-    </span></span><br><span class="line">    bst.<span class="built_in">Remove</span>(<span class="number">100</span>);</span><br><span class="line">    cout &lt;&lt; bst.<span class="built_in">Search</span>(<span class="number">100</span>) &lt;&lt; endl;<span class="comment">//0</span></span><br><span class="line">    cout&lt;&lt;bst.<span class="built_in">ShowL2_right</span>()&lt;&lt;endl;<span class="comment">//120</span></span><br><span class="line">&#125;</span><br><span class="line">    <span class="comment">//          5</span></span><br><span class="line">    <span class="comment">//         &lt;--&gt;120</span></span><br><span class="line">    <span class="comment">//          66 &lt;-&gt;330</span></span><br></pre></td></tr></table></figure><h5 id="FindMin"><a href="#FindMin" class="headerlink" title="FindMin"></a>FindMin</h5><p>函数的递归写法：</p><p>时间复杂度O(log n)</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">Find_Min</span><span class="params">(Node * temp)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(temp == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;Empty tree&quot;</span>&lt;&lt;endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//退出条件</span></span><br><span class="line">        <span class="keyword">if</span>(temp-&gt;left == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> root-&gt;data;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">Find_Min</span>(root-&gt;left);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h5 id="FindHeight"><a href="#FindHeight" class="headerlink" title="FindHeight"></a>FindHeight</h5><p>获得二叉树的高度</p><p>算法：一颗树的高度&#x3D;MAX（左右子树的高度）+1</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//每个点都遍历一次，时间复杂度O(n)</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">FindHeight</span><span class="params">(Node * root)</span></span>&#123;</span><br><span class="line">    <span class="comment">//退出条件</span></span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;<span class="comment">//empty的高度为-1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> LeftHeight = <span class="built_in">FindHeight</span>(root-&gt;left);</span><br><span class="line">    <span class="type">int</span> rightHeight = <span class="built_in">FindHeight</span>(root-&gt;right);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(LeftHeight,rightHeight)+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="IsBinarySearchTree"><a href="#IsBinarySearchTree" class="headerlink" title="IsBinarySearchTree"></a>IsBinarySearchTree</h5><p>判断是否为二叉搜索树</p><p>算法1（不推荐）：</p><p>根据定义，要求对于每一个节点，其大于等于自己左子树中的最大点，小于等于自己在右子树中的最小点，同时其左子树和右子树也是二叉搜索树</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsBinarySearchTree</span><span class="params">(BST::Node *root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span> ) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="comment">//前序遍历</span></span><br><span class="line">    <span class="keyword">if</span>( </span><br><span class="line">       ( root-&gt;left == <span class="literal">NULL</span> || <span class="built_in">find_max</span>(root-&gt;left)&lt;= root-&gt;data )</span><br><span class="line">       &amp;&amp;</span><br><span class="line">       ( root-&gt;right == <span class="literal">NULL</span> || <span class="built_in">find_min</span>(root-&gt;right)&gt;= root-&gt;data )</span><br><span class="line">       &amp;&amp;</span><br><span class="line">       <span class="built_in">IsBinarySearchTree</span>(root-&gt;left)</span><br><span class="line">       &amp;&amp;</span><br><span class="line">       <span class="built_in">IsBinarySearchTree</span>(root-&gt;right)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>算法2：</p><p>根据父节点的数值确定子节点数值的方位，逐一判断</p><p>这个方法，每个节点只需要遍历1次，时间复杂度O(n)</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240721210802341.png" alt="image-20240721210802341"></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsBinarySearchTree</span><span class="params">(BST::Node * root,<span class="type">int</span> minValue=INT_MIN,<span class="type">int</span> maxValue=INT_MAX)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//DLR</span></span><br><span class="line">    <span class="keyword">if</span>(root-&gt;data &gt;=minValue</span><br><span class="line">        &amp;&amp;</span><br><span class="line">        root-&gt;data&lt;=maxValue</span><br><span class="line">        &amp;&amp;</span><br><span class="line">        <span class="built_in">IsBinarySearchTree</span>(root-&gt;left,minValue,root-&gt;data)</span><br><span class="line">        &amp;&amp;</span><br><span class="line">        <span class="built_in">IsBinarySearchTree</span>(root-&gt;right,root-&gt;data,maxValue)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中INT_MAX是最大的int类型数据，而INT_MIN则是最小的int类型数据</p><p>效果展示：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//          5</span></span><br><span class="line"> <span class="comment">//         &lt;--&gt;120</span></span><br><span class="line"> <span class="comment">//          66 &lt;-&gt;330</span></span><br><span class="line"> bst.root-&gt;left-&gt;data=<span class="number">81</span>;</span><br><span class="line"> cout&lt;&lt;<span class="built_in">IsBinarySearchTree</span>(bst.root);<span class="comment">//0</span></span><br></pre></td></tr></table></figure><h3 id="Tree-Traversal遍历"><a href="#Tree-Traversal遍历" class="headerlink" title="Tree Traversal遍历"></a>Tree Traversal遍历</h3><p>process or visiting each node in the tree exactly once in some order</p><p><strong>breadth-first 广度优先</strong></p><p>每一层的节点都遍历完了以后，再到下一个节点</p><p>这种遍历称为<strong>层次遍历</strong></p><p><strong>Depth-first 深度优先</strong></p><p><strong>将一个子树全部遍历完以后，再到下一棵子树</strong>，这个特点可以用递归很好地实现</p><p>根据遍历的相对顺序的不同，可以分为前序&#x2F;中序&#x2F;后序遍历</p><p>例如前序遍历（preorder)就是的访问顺序为：root left right</p><p>也可以称为<strong>DLR</strong>，D就是data，LR表示方向</p><h4 id="二叉树层次遍历-代码实现"><a href="#二叉树层次遍历-代码实现" class="headerlink" title="二叉树层次遍历-代码实现"></a>二叉树层次遍历-代码实现</h4><p>算法：利用队列FIFO(first in first out)的性质<br>让root进入队列，然后当root出队列时，依次让其左孩和右孩进入队列</p><p>而当每个节点出队列时，执行相同操作，这样就按照从左往右的顺序实现了一次遍历</p><p>每个节点都被访问了一次，<strong>时间复杂度O(n)</strong></p><p>如果是满二叉树,<strong>空间复杂度O(n)</strong> &#x2F;&#x2F;最下层的节点数约等于n&#x2F;2</p><p>这也是平均场景下的空间复杂度</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Levelorder</span><span class="params">(BST::Node * root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( root == <span class="literal">NULL</span>) <span class="keyword">return</span> ;</span><br><span class="line">    queue&lt;BST::Node*&gt; Q;</span><br><span class="line">    Q.<span class="built_in">push</span>(root);</span><br><span class="line">    <span class="keyword">while</span>(!Q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">        BST::Node * temp = Q.<span class="built_in">front</span>();</span><br><span class="line">        Q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span>( temp-&gt;left != <span class="literal">NULL</span>) Q.<span class="built_in">push</span>(temp-&gt;left);</span><br><span class="line">        <span class="keyword">if</span>( temp-&gt;right != <span class="literal">NULL</span>) Q.<span class="built_in">push</span>(temp-&gt;right);</span><br><span class="line">        cout&lt;&lt;temp-&gt;data&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//          5</span></span><br><span class="line"><span class="comment">//        4&lt;--&gt;100</span></span><br><span class="line"><span class="comment">//      3&lt;-  66 &lt;-&gt;330</span></span><br><span class="line"><span class="comment">//             120&lt;-</span></span><br><span class="line"><span class="comment">//          101&lt;-  </span></span><br></pre></td></tr></table></figure><p>的遍历结果为：</p><p>5 4 100 3 66 330 120 101 </p><h4 id="二叉树前序-中序-后序遍历-代码实现"><a href="#二叉树前序-中序-后序遍历-代码实现" class="headerlink" title="二叉树前序&#x2F;中序&#x2F;后序遍历-代码实现"></a>二叉树前序&#x2F;中序&#x2F;后序遍历-代码实现</h4><p>前序遍历DLR</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Preorder</span><span class="params">(BST::Node * root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;root-&gt;data&lt;&lt;endl;<span class="comment">//D</span></span><br><span class="line">    <span class="built_in">Preorder</span>(root-&gt;left);<span class="comment">//L</span></span><br><span class="line">    <span class="built_in">Preorder</span>(root-&gt;right);<span class="comment">//R</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>中序遍历LDR</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Inorder</span><span class="params">(BST::Node * root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">Inorder</span>(root-&gt;left);<span class="comment">//L</span></span><br><span class="line">    cout&lt;&lt;root-&gt;data&lt;&lt;endl;<span class="comment">//D</span></span><br><span class="line">    <span class="built_in">Inorder</span>(root-&gt;right);<span class="comment">//R</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>后序遍历LRD</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Postorder</span><span class="params">(BST::Node * root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">Postorder</span>(root-&gt;left);<span class="comment">//L</span></span><br><span class="line">    <span class="built_in">Postorder</span>(root-&gt;right);<span class="comment">//R</span></span><br><span class="line">        cout&lt;&lt;root-&gt;data&lt;&lt;endl;<span class="comment">//D</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如，对于二叉树：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//          5</span></span><br><span class="line"><span class="comment">//        4&lt;--&gt;100</span></span><br><span class="line"><span class="comment">//      3&lt;-  66 &lt;-&gt;330</span></span><br><span class="line"><span class="comment">//             120&lt;-</span></span><br><span class="line"><span class="comment">//          101&lt;-  </span></span><br></pre></td></tr></table></figure><p>三种遍历方式的结果为：</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240721114343583.png" alt="image-20240721114343583"></p><p>这三种算法的时间复杂度都是O(n)</p><p>空间复杂度O(h),其中h是树的高度</p><p>最差情况下的空间复杂度为O(n)&#x2F;&#x2F;类似于链表的树</p><p>最好情况&#x2F;平均情况的空间复杂度为O(log(n))</p><p>这里的空间复杂度是因为<strong>递归调用</strong>产生的，空间成本看的是<strong>最多同时占据栈区中的几个栈帧</strong></p><h4 id="中序后继节点"><a href="#中序后继节点" class="headerlink" title="中序后继节点"></a>中序后继节点</h4><p>Inorder Successor</p><p>中序遍历有个特点：</p><p>访问的数据严格按照从小到大排列</p><p>有时候我们需要找到树比某个数值大的下一个数据</p><p>也就是要找到这个节点的中序后继节点</p><p>如果我们直接采用中序遍历去找，当然可以找到，但是这个时间复杂度是O(n)</p><p>这个开销还是比较大的，因此我们需要更快捷的算法：</p><p><strong>case1</strong>: 当前节点有right child，中序后继节点就是findmin(right child)&#x2F;&#x2F;返回Node*</p><p><strong>case2</strong>:当前节点没有right child,</p><p>如果当前节点是其父节点的left child中序后继节点就是就是其父节点</p><p>如果当前节点是其父节点的right child,中序后继节点就是其最近的祖先，而且这个祖先还要满足：目标节点在其左子树中</p><p>也就是说，这两种情况可以统一为：*<strong>找到最近的将目标节点包含在左子树中的祖先</strong></p><p>代码实现：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">BST::Node * <span class="title">GetSuccessor</span><span class="params">(BST::Node*root,<span class="type">int</span> data)</span></span>&#123;</span><br><span class="line">    <span class="comment">//先确定当前节点的位置</span></span><br><span class="line">    BST::Node* current = <span class="built_in">Search</span>(data,root);</span><br><span class="line">    <span class="keyword">if</span>(current == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//确定最近的将目标节点包含在左子树中的祖先</span></span><br><span class="line">    <span class="comment">//case 1: has right subtree</span></span><br><span class="line">    <span class="keyword">if</span>(current-&gt;right !=<span class="literal">NULL</span>)&#123;</span><br><span class="line">        BST::Node* temp = current-&gt;right;</span><br><span class="line">        <span class="keyword">while</span>(temp-&gt;left != <span class="literal">NULL</span>) temp = temp-&gt;left;</span><br><span class="line">        <span class="keyword">return</span> temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//case 2: no right subtree</span></span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        BST::Node * successor = <span class="literal">NULL</span>;<span class="comment">//中序后继节点</span></span><br><span class="line">        BST::Node * ancestor = root;</span><br><span class="line">        <span class="keyword">while</span>(current-&gt;data != ancestor-&gt;data )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//当current 在ancestor的左子树中时</span></span><br><span class="line">            <span class="keyword">if</span>(current-&gt;data &lt; ancestor-&gt;data)&#123;</span><br><span class="line">                successor = ancestor;</span><br><span class="line">                ancestor = ancestor-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> ancestor = ancestor-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> successor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最差的情况下，进行两次时间复杂度为O(log n)的查找</p><p>也就是说，这个算法的时间复杂度为O(log n) </p><p>当n的数目极大的时候，这个算法会明显由于前文提到的直接用中序遍历寻找的方法</p><h2 id="图graphs"><a href="#图graphs" class="headerlink" title="图graphs"></a>图graphs</h2><h3 id="ADT："><a href="#ADT：" class="headerlink" title="ADT："></a>ADT：</h3><p>边可以以任意方式链接节点</p><p>定义: A graoh G is an ordered pair of a set V of vertices and a set E of edges </p><p>一个图G包含边集V和点集E，可以写成G(V,E)  &#x2F;&#x2F;(V,E)有序对</p><p><strong>边的表示</strong>：</p><p>有向(directed edge)： (u,v) !&#x3D; (v,u)</p><p>无向(undirected edge) {u,v} </p><p><strong>图的分类</strong>:所有边都有方向:a directed graph &#x2F; Digraph</p><p>所有边都没有方向: an undirected graph</p><p><strong>应用：</strong>好友关系可以用无向图，关注关系可以用有向图</p><p>链接的跳转可以用有向图， web-crawling（网络爬虫）本质上就是图的遍历（graph traversal)</p><p><strong>加权图</strong>：</p><p>每条edge都有数值，例如可以用数值表示道路的长度</p><h3 id="图的特性"><a href="#图的特性" class="headerlink" title="图的特性:"></a>图的特性:</h3><p><strong>自环(self-loop)</strong></p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240722100718879.png" alt="image-20240722100718879"></p><p><strong>多重边(multiedge)</strong></p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240722100934574.png" alt="image-20240722100934574"></p><p><strong>简单图(simple graph)</strong></p><p>没有多重边和自环的图</p><p>一个有向简单图的最大边数为 (n-1)*n</p><p>一个无向简单图的最大边数 (n-1)*n&#x2F;2</p><p><strong>Dense(稠密)-too many edges</strong></p><p><strong>Sparse(稀疏)-too few edges</strong></p><p><strong>路径(Path)</strong></p><p>简单路径(simple path)经过的节点和边都不重复</p><p>通常我们说path 的时候指的就是simple path</p><p>而常用<strong>walk</strong>指一般的路径</p><p>**强连接(strongly connected **</p><p>定义：if there is a path from any vertex to any vertex</p><p><strong>弱连接图(weakly connected)</strong></p><p>如果将一个非强连接图中的directed edge 改成 undirected edge 就变成强连接</p><p>那么这个图就是弱连接</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240722102612481.png" alt="image-20240722102612481"></p><p><strong>closed walk</strong></p><p>闭合路径，要求起点和重点是同一个vertex</p><p><strong>simple cycle</strong></p><p>起点和终点的顶点访问两次，其他顶点和边不重复访问的闭合路径</p><p>常用cycle 代表 simple cycle</p><p><strong>Acyclic graph(无环图)</strong></p><p>没有简单循环的图的图</p><h3 id="图的表示法"><a href="#图的表示法" class="headerlink" title="图的表示法"></a>图的表示法</h3><h4 id="边列表"><a href="#边列表" class="headerlink" title="边列表"></a>边列表</h4><p>可以用动态列表实现，例如C++中的Vector</p><p>一个list存放vertex，一个list存放edge</p><p>edge可以用struct定义</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Edge</span>&#123;</span><br><span class="line">    <span class="comment">//starVertex&amp;endVertex代表对应节点在vertex list中的索引</span></span><br><span class="line"><span class="type">int</span> starVertex;</span><br><span class="line"><span class="type">int</span> endVertex;</span><br><span class="line"><span class="type">int</span> weight;<span class="comment">//加权</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240722135215800.png" alt="image-20240722135215800"></p><p>从空间复杂度来看，边列表的开销不算大</p><p>对于一个图，最频繁的执行的操作是找到一个点的邻接点，这个操作的<strong>时间复杂度为O(|E|)</strong></p><p>另一个操作是查看给定的两个点是否相连,具体算法是从起点开始，让每一条walk走到底（直到没有点&#x2F;发生循环为止)这个操作的<strong>时间复杂度为O(|E|)</strong></p><p>对于一个有向简单图,E&lt;&#x3D;V*(V-1)&#x2F;2,这个时间复杂度的开销还是比较大的</p><p><strong>如果一个图是稀疏的，那么就适合用边列表</strong></p><h4 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h4><p>我们需要用一个二维矩阵代替原来的edge list </p><p>二维矩阵的一个元素可以表示为Aij，矩阵的数据类型是int&#x2F;bool</p><p>如果从点i到点j有edge，那么Aij就等于1，如果这条边是无向的，那么Aji也为1</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240722140801342.png" alt="image-20240722140801342"></p><p>空间复杂度O(|V|)、O（|V|*|V|)</p><p>时间复杂度：</p><p><strong>找到邻接点：</strong>首先在vertex list 中找到这个数据的索引，O（V）</p><p>然后在数组中以这个索引为横坐标&#x2F;纵坐标找到邻接点，O(V)+O(V)</p><p><strong>总时间复杂度O（|V|）</strong></p><p><strong>判断两个节点是否连接:</strong></p><p>先根据节点的名字找到节点的索引，耗费O(V)</p><p>然后看Aij和Aji即可，时间复杂度O(1),<strong>总时间复杂度O(V)</strong></p><p>如果使用一个<strong>哈希表</strong>存储点的名字和索引的话，这个时间复杂度可以降到O(1)</p><p>如果图有<strong>权重</strong>，就可以把数组存储的数值改成边的权重</p><p>同时将数值的默认值设置为一个不可能出现的数值</p><p><strong>如果一个图是稠密的，那么就适合用邻接矩阵</strong></p><p>例如对于facebook，假设其有10^9用户，每个用户的好友不超过1000，那么边数： 10 ^9 * 10^3 &#x2F;2 &#x3D;5*10^11 &lt;&lt; 10^18</p><p>也就是说这个好友关系图是稀疏的，不应该用邻接矩阵，否则<strong>内存消耗太大</strong>了</p><p>当然了，Facebook用的也不是边列表，用的可能是<strong>邻接列表（Adjacency List）</strong></p><h4 id="邻接表"><a href="#邻接表" class="headerlink" title="邻接表"></a>邻接表</h4><p>现实生活中的大多数表是稀疏的，如果我们像邻接矩阵一样用一块内存保存两个相连节点的”不相连“关系，未免太过奢侈了</p><p>邻接列表的基本思想是：</p><p><strong>对于图中的每个节点，都维护一个列表，该列表包含与该节点相邻的所有其他节点。</strong></p><p>如果是有向图，那么列表中存储的是以该节点为起点的邻接点</p><p>同样的，这个列表存储的是节点在vertex list 中的索引</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240722151022511.png" alt="image-20240722151022511"></p><p>在C++中，可以用指针数组实现一个这样的结构</p><p>相当于实现了一个每一行的大小都不一样的二维数组</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span> *&gt;v;<span class="comment">//整个列表</span></span><br><span class="line"><span class="type">int</span> * v0 = <span class="keyword">new</span> <span class="type">int</span> [<span class="number">3</span>];   </span><br><span class="line">v.<span class="built_in">push_back</span>(v0);</span><br><span class="line">v0[<span class="number">0</span>] = <span class="number">5</span>;</span><br><span class="line">v0[<span class="number">6</span>] = <span class="number">3</span>;</span><br><span class="line">v0[<span class="number">7</span>] = <span class="number">21</span>;</span><br></pre></td></tr></table></figure><p>空间复杂度O（V+E）,相比于邻接矩阵，空间耗费少了很多</p><p>判断两个点是否连接：时间复杂度O(d)，d代表节点的度数，假设已经通过键值对（哈希表）获得了两个节点的索引，只需要看第i行的数组是否有j以及第j行的数组中是否有i，这个<strong>时间复杂度是O(d)</strong>,D&lt;&#x3D;V</p><p>找到一个点的所有邻接节点：找到以该点为起点的邻接节点：O（d）</p><p>找到以该点为终点的邻接节点可能需要遍历邻接表中的所有元素，这个开销是很大的</p><p><strong>如果该图无向，时间复杂度为O(d)</strong></p><p>如果有向，我们得考虑让一个点维护两个数组，一个代表入度，一个代表出度，如果这个，<strong>时间复杂度仍为 O（d）</strong></p><p><strong>否则，时间复杂度为O(all)</strong>,all代表邻接表中的元素个数</p><p>相较于邻接矩阵，邻接表的最大优势是节省了大量空间，同时能够节省一部分时间，即使在时间复杂度上来看这并不明显，但是当V很大（例如10亿）时，如果用邻接表找邻接节点，需要扫描十亿次，而用邻接表，如果这个节点有1000个好友，那么也只需要扫描1000次</p><p>有时候，每个节点维护的列表不用动态数组实现，而用链表实现，这是因为链表在插入新数据的时候开销比数组小很多</p><p>在某些情况下，我们也会用二叉搜索树实现这个列表</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240722224608478.png" alt="image-20240722224608478"></p><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><p>ADT：用根据关键码的值而直接进行访问的数据结构</p><p>要求查询的时间复杂度为O(1)</p><p>哈希法<strong>牺牲了空间换取了时间</strong>，因为我们要使用额外的数组，set或者是map来存放数据，才能实现快速的查找</p><p>哈希表的最重要特征是：将数据（利用哈希函数）映射到索引</p><p>这个数据就是<strong>键</strong>，而映射出来的这个索引就叫做<strong>值</strong>，哈希表以值为索引存储有关键的信息，通常，我们只能通过键得到值，而不能通过值得到键（除非哈希函数特别简单）</p><p>如果我们希望通过值得到键，可以再建立一个哈希表，将值作为键，键作为值，这个就是反向哈希表。</p><p><strong>哈希函数</strong></p><p>例如，当我们需要查询一个学校中学生的名字时，我们通过<strong>哈希函数</strong>将学生的名字映射为数字，</p><p>如果得到的数值过大，会进行<strong>取模</strong>的操作</p><p><strong>哈希碰撞</strong></p><p>小李和小王都映射到了索引下标 1 的位置，<strong>这一现象叫做哈希碰撞</strong>。</p><p>一般哈希碰撞有两种解决方法， 拉链法和线性探测法。</p><p>哈希表大小:table_size  数据大小：data_size </p><p><strong>拉链法</strong></p><p>发生冲突的元素都被存储在链表中。 这样我们就可以通过索引找到小李和小王了</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/68747470733a2f2f636f64652d7468696e6b696e672d313235333835353039332e66696c652e6d7971636c6f75642e636f6d2f706963732f32303231303130343233353031353232362e706e67.png" alt="哈希表4"></p><p><strong>线性探测法</strong></p><p>使用线性探测法，一定要保证<strong>tableSize大于dataSize</strong>。 我们需要依靠哈希表中的空位来解决碰撞问题。</p><p>例如冲突的位置，放了小李，那么就向下找一个空位放置小王的信息。所以要求tableSize一定要大于dataSize ，要不然哈希表上就没有空置的位置来存放 冲突的数据了。如图所示：</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/68747470733a2f2f636f64652d7468696e6b696e672d313235333835353039332e66696c652e6d7971636c6f75642e636f6d2f706963732f32303231303130343233353130393935302e706e67.png" alt="哈希表5"></p><h3 id="三种实现方式"><a href="#三种实现方式" class="headerlink" title="三种实现方式"></a>三种实现方式</h3><ul><li>数组</li><li>集合set</li><li>映射map</li></ul><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240907165142539.png" alt="image-20240907165142539"></p><p>std::unordered_set底层实现为哈希表，std::set 和std::multiset 的底层实现是红黑树，红黑树是一种平衡二叉搜索树，所以key值是有序的，但key不可以修改，改动key值会导致整棵树的错乱，所以只能删除和增加。</p><p><img src="/2024/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20240907165148574.png" alt="image-20240907165148574"></p><p>std::unordered_map 底层实现为哈希表，std::map 和std::multimap 的底层实现是红黑树。同理，std::map 和std::multimap 的key也是有序的（这个问题也经常作为面试题，考察对语言容器底层的理解）。</p><p>当我们要使用集合来解决哈希问题的时候，优先使用unordered_set，因为它的查询和增删效率是最优的，如果需要集合是有序的，那么就用set，如果要求不仅有序还要有重复数据的话，那么就用multiset。</p><p>那么再来看一下map ，在map 是一个key value 的数据结构，map中，对key是有限制，对value没有限制的，因为key的存储方式使用红黑树实现的。</p><p>然std::set和std::multiset 的底层实现基于红黑树而非哈希表，它们通过红黑树来索引和存储数据。<strong>不过给我们的使用方式，还是哈希法的使用方式</strong>，即依靠键（key）来访问值（value）。所以使用这些数据结构来解决映射问题的方法，我们依然称之为哈希法。std::map也是一样的道理。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>乌萨奇通讯录项目报告</title>
      <link href="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/"/>
      <url>/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/</url>
      
        <content type="html"><![CDATA[<h1 id="乌萨奇通讯录项目报告"><a href="#乌萨奇通讯录项目报告" class="headerlink" title="乌萨奇通讯录项目报告"></a>乌萨奇通讯录项目报告</h1><h2 id="设计背景："><a href="#设计背景：" class="headerlink" title="设计背景："></a><strong>设计背景：</strong></h2><p>在聊天软件（如微信）日益发达的今天，我们通过打电话联系他人的频率越来越低，这导致了我们可能会经常性地忘记某个朋友的电话或是邮箱账号。</p><p>在移动端已经有了成熟的通讯录app的基础上，乌萨奇通讯录的设计目的是在PC端为用户提供方便快捷的录入联系人信息的功能，以及强大的生日查询和邮件发送功能，满足现代人日益发展的社交需求。</p><h2 id="程序结构"><a href="#程序结构" class="headerlink" title="程序结构"></a><strong>程序结构</strong></h2><p>本项目共有17个.h头文件，对应17个.cpp头文件以及14个.ui文件，由于界面文件较多，下面由界面的切换逻辑来对程序的结构进行讲解。</p><p>界面切换逻辑示意图：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617103858409.png" alt="image-20240617103858409"></p><p>图中，带有“切换“字样的双箭头表明两个界面之间是来回切换的关系，但一个页面显示，另一个页面则会进行隐藏。</p><p>而单箭头表明从箭头的起点对应的页面可以产生箭头指向的页面，两个页面将会同时显示在屏幕上。</p><p>而从Email_func到Email_func_birth、Email_func_normal、Email_send_func的切换逻辑比较特殊，这里在Email_func页面制作了一个侧边栏的设计，可以通过三个按钮在三个页面间来回切换。</p><p>除了图中14个包含.ui文件的文件，本项目中还有四个文件，他们的功能如下：</p><p>|——mysound.h &#x2F;&#x2F;实现各种音效</p><p>|——contact.h &#x2F;&#x2F;包含一个基类和五个派生类，实现快速添加联系人</p><p>|——db_operator.h &#x2F;&#x2F;连接数据库</p><p>|——email.h &#x2F;&#x2F;封装一个函数，用于发送邮件</p><h2 id="程序功能"><a href="#程序功能" class="headerlink" title="程序功能"></a><strong>程序功能</strong></h2><p>下面我将结合相应的界面对项目的功能进行说明，并会对该功能的实现方式进行简单的诠释。</p><p>主界面如下：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617103921604.png" alt="image-20240617103921604"></p><h3 id="1、登录-注册功能："><a href="#1、登录-注册功能：" class="headerlink" title="1、登录&#x2F;注册功能："></a><strong>1、登录&#x2F;注册功能：</strong></h3><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617103933469.png" alt="image-20240617103933469"></p><p>用户打开程序，最先显示的是左侧的登录界面，首先用户应该注册一个账户，注册界面如右图所示。</p><p>用户可以自由添加头像，图片中显示的是用户的默认头像，接着用户可以输入自己的用户名和密码（密码需要二次输入进行确认），点击“注册”后，程序会将用户的用户名和密码信息保存到数据库中，实现注册功能。</p><p>在具体的代码中，首先用</p><p>QString username &#x3D; ui-&gt;username_le-&gt;text();</p><p>QString pwd_1 &#x3D; ui-&gt;pwd_le1-&gt;text();</p><p>QString pwd_2 &#x3D; ui-&gt;pwd_le2-&gt;text();</p><p>获取用户输入的信息，接着用if语句用户输入是否为空以及两次输入的密码是否相同等内容。</p><p>重要的是会对数据库中是否已存在相同的用户名进行判定，判定代码如下：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617103948169.png" alt="image-20240617103948169"></p><p>若查询结果为空，表明不存在相同的用户名，下一步会进行图片的处理。</p><p>此处限定图片大小为15.9mb是因为数据库中用于存放图片数据的字段的数据类型为MEDIUNBLOB，其最大容量为15.9mb</p><p>此过程可以概括为将图片的二进制数据写入QByteArray 对象 photo_data后，用photo_data初始化QVariant对象 photo</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//图片处理</span></span><br><span class="line">              QByteArray photo_data;</span><br><span class="line">              <span class="function">QBuffer <span class="title">buffer</span><span class="params">(&amp;photo_data)</span></span>;</span><br><span class="line">              buffer.<span class="built_in">open</span>(QIODevice::WriteOnly);</span><br><span class="line">              <span class="keyword">this</span>-&gt;image.<span class="built_in">save</span>(&amp;buffer, <span class="string">&quot;JPEG&quot;</span>,<span class="number">100</span>); <span class="comment">// 将图片的二进制数据写入photo_data</span></span><br><span class="line">              <span class="comment">// 检查图片数据大小</span></span><br><span class="line">              <span class="type">int</span> quality=<span class="number">90</span>;</span><br><span class="line">              <span class="keyword">while</span> (photo_data.<span class="built_in">size</span>() &gt; <span class="number">15.9</span> * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line">              &#123;</span><br><span class="line">                  <span class="comment">// 图片数据超过了最大允许大小</span></span><br><span class="line">                  <span class="comment">// 清空buffer</span></span><br><span class="line">                  buffer.<span class="built_in">setData</span>(<span class="built_in">QByteArray</span>());</span><br><span class="line">                  <span class="comment">// 重新保存 quality  逐渐降低</span></span><br><span class="line">                  image.<span class="built_in">save</span>(&amp;buffer, <span class="string">&quot;JPEG&quot;</span>,quality);</span><br><span class="line">                  <span class="built_in">qDebug</span>()&lt;&lt; quality ;</span><br><span class="line">                  quality -= <span class="number">10</span>;</span><br><span class="line">              &#125;</span><br><span class="line">              <span class="function">QVariant <span class="title">photo</span><span class="params">(photo_data)</span></span>;</span><br></pre></td></tr></table></figure><p>在获取了用户的信息后，对用户信息进行插入操作，对应代码如下：c</p><p>（此处的memo字段用于存放“便利贴”中的内容，在之后的“便利贴”功能讲解中会说明，这里先插入一个空字符串）</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">QSqlQuery query;</span><br><span class="line">QString memo  = <span class="string">&quot;&quot;</span>;</span><br><span class="line">query.<span class="built_in">prepare</span>(<span class="string">&quot;INSERT INTO accounts (user_name, password, photo,memo) VALUES (:username, :password,:photo,:memo)&quot;</span>);</span><br><span class="line">query.<span class="built_in">bindValue</span>(<span class="string">&quot;:username&quot;</span>, username);</span><br><span class="line">query.<span class="built_in">bindValue</span>(<span class="string">&quot;:password&quot;</span>, pwd_1);</span><br><span class="line">query.<span class="built_in">bindValue</span>(<span class="string">&quot;:photo&quot;</span>, photo);</span><br><span class="line">query.<span class="built_in">bindValue</span>(<span class="string">&quot;:memo&quot;</span>, memo);</span><br><span class="line"><span class="keyword">if</span> (query.<span class="built_in">exec</span>())</span><br><span class="line">&#123;</span><br><span class="line">    sound.yaha.<span class="built_in">play</span>();</span><br><span class="line">    QMessageBox::<span class="built_in">information</span> (<span class="keyword">this</span>, <span class="string">&quot;注册&quot;</span>, <span class="string">&quot;注册成功！&quot;</span>);</span><br><span class="line">    <span class="function">emit <span class="title">regist_ready</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="keyword">this</span>-&gt;<span class="built_in">close</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在注册成功后，用户便能够进行登录操作</p><p>点击登录按钮对应的槽函数的部分代码如下：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104017986.png" alt="image-20240617104017986">这段代码在数据库中查询是否存在对应用户名和密码的数据，接着将数据库中accounts表中的id（primary key)存储到变量user_id中。</p><h3 id="2、添加-修改-删除联系人信息功能"><a href="#2、添加-修改-删除联系人信息功能" class="headerlink" title="2、添加&#x2F;修改&#x2F;删除联系人信息功能"></a><strong>2、添加&#x2F;修改&#x2F;删除联系人信息功能</strong></h3><p>添加页面：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104043299.png" alt="image-20240617104043299"></p><p>添加页面的comboBox提供了四个默认分组以及“无”和“其他”的选项</p><p>如果选择“其他”就会让用户填写自定义的分组。</p><p>这个小功能的实现是利用了on_comboBox_activated这一个槽函数，这个槽函数会在comboBox的索引被修改的时候自动调用，当索引为5（对应“其他”）的时候ui-&gt;newtype_lb-&gt;setVisible(true);ui-&gt;newtype_le-&gt;setVisible(true)；</p><p>点击“添加联系人”按钮以后，在槽函数中，会执行对用户输入内容的检测、图片处理等，这一部分和上文登录&#x2F;注册功能中展示的代码类似，不再进行展示。</p><p>随后，利用switch语句，根据用户在comboBox中选择的索引，进行对应对象的创建，其中new_contact是Contact*类型对象，其他类均为Contact类的子类</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104052407.png" alt="image-20240617104052407"></p><p>Other_cot类的add_contact函数实现如下：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104102613.png" alt="image-20240617104102613"></p><p>这样就实现了联系人信息的添加。</p><p>添加成功后，关闭页面，就可以看到新添加的联系人已经出现在了主界面上，这里重写了add_func类的关闭事件</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104110431.png" alt="image-20240617104110431"></p><p>关闭页面后发出close（）信号，其与main_func对象的on_show()槽函数连接</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104205576.png" alt="image-20240617104205576"></p><p>其中show_cot代码如下：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">QSqlQuery query;</span><br><span class="line">   query.<span class="built_in">prepare</span>(<span class="string">&quot;SELECT name, birth, phone, email, type, special_info FROM contacts WHERE user_id = :user_id&quot;</span>);</span><br><span class="line">   query.<span class="built_in">bindValue</span>(<span class="string">&quot;:user_id&quot;</span>, <span class="keyword">this</span>-&gt;user_id);</span><br><span class="line">   query.<span class="built_in">exec</span>();</span><br><span class="line">   <span class="comment">//qDebug()&lt;&lt; this-&gt;user_id;</span></span><br><span class="line">   <span class="keyword">this</span>-&gt;qmodel = <span class="keyword">new</span> <span class="built_in">QSqlQueryModel</span>(<span class="keyword">this</span>);</span><br><span class="line">   <span class="comment">//使用移动语义设置模型的数据</span></span><br><span class="line">   qmodel-&gt;<span class="built_in">setQuery</span>(std::<span class="built_in">move</span>(query));</span><br><span class="line">   <span class="comment">//ui-&gt;contact_table-&gt;setModel(qmodel);</span></span><br><span class="line"></span><br><span class="line">   ui-&gt;contact_table-&gt;<span class="built_in">setSortingEnabled</span>(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">   QSortFilterProxyModel *sqlproxy = <span class="keyword">new</span> <span class="built_in">QSortFilterProxyModel</span>(<span class="keyword">this</span>);</span><br><span class="line">   sqlproxy-&gt;<span class="built_in">setSourceModel</span>(qmodel);</span><br><span class="line"></span><br><span class="line">   sqlproxy-&gt;<span class="built_in">setHeaderData</span>(<span class="number">0</span>, Qt::Horizontal, <span class="string">&quot;姓名&quot;</span>);</span><br><span class="line">   sqlproxy-&gt;<span class="built_in">setHeaderData</span>(<span class="number">1</span>, Qt::Horizontal, <span class="string">&quot;生日&quot;</span>);</span><br><span class="line">   sqlproxy-&gt;<span class="built_in">setHeaderData</span>(<span class="number">2</span>, Qt::Horizontal, <span class="string">&quot;电话号码&quot;</span>);</span><br><span class="line">   sqlproxy-&gt;<span class="built_in">setHeaderData</span>(<span class="number">3</span>, Qt::Horizontal, <span class="string">&quot;邮箱&quot;</span>);</span><br><span class="line">   sqlproxy-&gt;<span class="built_in">setHeaderData</span>(<span class="number">4</span>, Qt::Horizontal, <span class="string">&quot;分组&quot;</span>);</span><br><span class="line">   sqlproxy-&gt;<span class="built_in">setHeaderData</span>(<span class="number">5</span>, Qt::Horizontal, <span class="string">&quot;备注&quot;</span>);</span><br><span class="line"></span><br><span class="line">   ui-&gt;contact_table-&gt;<span class="built_in">setModel</span>(sqlproxy);</span><br></pre></td></tr></table></figure><p>这样就实现了页面切换+自动刷新，之后的页面切换功能都是采用类似的做法，不会再次讲解。</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104521061.png" alt="image-20240617104521061"></p><p>在主界面，可以右键呼出菜单项，点击“查看”后，跳转到查看页面</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104527764.png" alt="image-20240617104527764"></p><p>在查看页面中，可以对输入框的内容进行修改，点击保存按钮后，即修改成功，保存按钮的槽函数和上文添加联系人功能的槽函数类似，都是检验格式+图片处理，只是第三步修改为了update语句，例如：UPDATE contacts SET name &#x3D; :name_change</p><p>这里不作详细展示。</p><p>有关利用菜单项切换页面的具体实现，会在“技术亮点”中进行讲解</p><p>删除功能：</p><p>实现流程是，根据索引值和model信息，获取鼠标点击位置的那一行的相关数据</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//数据初始化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Modify_func::data_init</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> row = index.<span class="built_in">row</span>();</span><br><span class="line">    name = model-&gt;<span class="built_in">data</span>(model-&gt;<span class="built_in">index</span>(row,<span class="number">0</span>)).<span class="built_in">toString</span>();</span><br><span class="line">    birth  = model-&gt;<span class="built_in">data</span>(model-&gt;<span class="built_in">index</span>(row,<span class="number">1</span>)).<span class="built_in">toDate</span>();</span><br><span class="line">    phone = model-&gt;<span class="built_in">data</span>(model-&gt;<span class="built_in">index</span>(row,<span class="number">2</span>)).<span class="built_in">toString</span>();</span><br><span class="line">    email = model-&gt;<span class="built_in">data</span>(model-&gt;<span class="built_in">index</span>(row,<span class="number">3</span>)).<span class="built_in">toString</span>();</span><br><span class="line">    type = model-&gt;<span class="built_in">data</span>(model-&gt;<span class="built_in">index</span>(row,<span class="number">4</span>)).<span class="built_in">toString</span>();</span><br><span class="line">    special_info=model-&gt;<span class="built_in">data</span>(model-&gt;<span class="built_in">index</span>(row,<span class="number">5</span>)).<span class="built_in">toString</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接着在数据库中进行查询</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">query.<span class="built_in">prepare</span>(<span class="string">&quot;SELECT photo &quot;</span></span><br><span class="line">                  <span class="string">&quot;FROM contacts&quot;</span></span><br><span class="line">                  <span class="string">&quot; WHERE  name=:name AND  birth=:birth AND  phone=:phone AND email=:email AND type=:type AND special_info=:special_info AND user_id=:user_id &quot;</span>);</span><br><span class="line"></span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:name&quot;</span>, name);</span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:birth&quot;</span>,  birth);</span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:phone&quot;</span>,  phone);</span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:email&quot;</span>,  email);</span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:type&quot;</span>,  type);</span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:special_info&quot;</span>, special_info);</span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:user_id&quot;</span>, <span class="keyword">this</span>-&gt;user_id);</span><br><span class="line">    query.<span class="built_in">exec</span>();</span><br><span class="line">    query.<span class="built_in">next</span>();</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>最后提取那一行的id（主键），然后用DELETE语句进行删除</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(query.<span class="built_in">next</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//获取contacts表中这一行数据的id</span></span><br><span class="line">        <span class="type">int</span> id = query.<span class="built_in">value</span>(<span class="number">0</span>).<span class="built_in">toInt</span>();</span><br><span class="line">        <span class="built_in">qDebug</span>()&lt;&lt;id;</span><br><span class="line">        <span class="comment">// 删除语句</span></span><br><span class="line">        QString deleteSql = <span class="built_in">QString</span>(<span class="string">&quot;DELETE FROM contacts WHERE id = %1&quot;</span>).<span class="built_in">arg</span>(id);</span><br><span class="line">        <span class="keyword">if</span>(query.<span class="built_in">exec</span>(deleteSql))<span class="comment">//删除</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">qDebug</span>()&lt;&lt;<span class="string">&quot;删除成功&quot;</span>;</span><br><span class="line">            emit <span class="keyword">this</span>-&gt;<span class="built_in">delete_ready</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="3、生日查询-祝福邮件发送功能"><a href="#3、生日查询-祝福邮件发送功能" class="headerlink" title="3、生日查询&#x2F;祝福邮件发送功能"></a><strong>3、生日查询&#x2F;祝福邮件发送功能</strong></h3><p>生日查询界面如下：</p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104718253.png" alt="image-20240617104718253" style="zoom: 67%;"><p>左侧是三个按钮，可以实现页面切换，这里侧边栏的实现是通过stackedWidget实现</p><p>具体而言步骤为：</p><p>1、为Email_func的stackedWidget对象添加界面对象的地址</p><p>2、为每个按钮设定索引值</p><p>3、连接信号，将点击按钮传递的信号（包含索引值）与stackedWidget的setCurrentIndex函数连接，实现页面切换</p><p>这里没有将buttonClicked信号与setCurrentIndex函数直接连接，buttonClicked会传递一个QAbstractButton *类型的参数，与setCurrentIndex需要的参数类型（int）不匹配，所以写了一个适配器函数onButtonClicked进行辅助连接。</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104739982.png" alt="image-20240617104739982"></p><p>适配器函数：<img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104748716.png" alt="image-20240617104748716"></p><p>年-月-日查询：</p><p>在最上方提供了三各QspinBox用于信息的输入，每个QspinBox都设定了范围</p><p>同时0作为一个默认值，代表所有，例如用户对0-0-0进行生日查询，会查询出所有联系人</p><p>如果对0-7-0进行查询，就可以查询任意年份+7月+任意日期过生日的联系人。</p><p>这个查询功能的实现方式将会在“难点攻破”中详细说明</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104759495.png" alt="image-20240617104759495"></p><p>近期生日查询：</p><p>在此页面中，同样用一个QspinBox进行信息的输入，范围设置为0-31</p><p>可以查询当天前后0-31天过生日的联系人，具体实现会在“技术亮点”中进行讲解</p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104815721.png" alt="image-20240617104815721" style="zoom:67%;"><p>邮件发送功能：</p><p>在输入框中输入收件人的邮箱（可以在前两个页面查询的结果中直接复制，十分方便）</p><p>会自动匹配邮箱名对应的联系人姓名，并且显示在上方，同时自动修改正文开头</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104832710.png" alt="image-20240617104832710"></p><p>自动匹配姓名的代码如下：</p><p>这段代码在on_lineEdit_textChanged槽函数中实现（在line_edit中的内容被更改的时候调用此函数）</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104844452.png" alt="image-20240617104844452"></p><p>点击“发送邮件”有，跳转到邮箱登录界面：</p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104854364.png" alt="image-20240617104854364" style="zoom:50%;"><p>这里要求用户填写邮箱的账号以及授权码（不是密码）需要用户去发送邮件的邮箱开通POP3&#x2F;SMTP服务</p><p>开通后，会得到一个授权码。</p><p>填写好邮箱和授权码，点击“登录此邮箱”就会进行一个邮件的发送。</p><p>点击按钮后，如果输入格式正确，email_login对象会发出一个email_login信号，这个信连接的槽函数的代码如下：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这里要小心多次连接问题</span></span><br><span class="line">   <span class="type">static</span> <span class="type">int</span> check=<span class="number">0</span>;</span><br><span class="line">   <span class="keyword">if</span>(check==<span class="number">0</span>)</span><br><span class="line">   &#123;</span><br><span class="line">       <span class="built_in">connect</span>(email_login_page,&amp;email_login::login_ready,[=]()</span><br><span class="line">               &#123;</span><br><span class="line">                   QString  SenderEmail= email_login_page-&gt;account;</span><br><span class="line">                   QString SenderPassword = email_login_page-&gt;password;</span><br><span class="line"></span><br><span class="line">                   QRegularExpression <span class="built_in">regex</span>(<span class="string">&quot;^(.+)@(.+)$&quot;</span>);</span><br><span class="line">                   QRegularExpressionMatch match = regex.<span class="built_in">match</span>(SenderEmail);</span><br><span class="line">                   QString domain = match.<span class="built_in">captured</span>(<span class="number">2</span>);</span><br><span class="line">                   QString SmtpServer = <span class="string">&quot;smtp.&quot;</span>  +  domain;</span><br><span class="line">                   <span class="built_in">qDebug</span>()&lt;&lt;SmtpServer;</span><br><span class="line">                   QString ReceiveEmail=ui-&gt;lineEdit-&gt;<span class="built_in">text</span>();</span><br><span class="line">                   <span class="type">int</span> SmtpPort =<span class="number">25</span>;</span><br><span class="line">                   QString SubjectTitle=<span class="string">&quot;生日快乐！&quot;</span>;</span><br><span class="line">                   QString Content=ui-&gt;plainTextEdit-&gt;<span class="built_in">toPlainText</span>();</span><br><span class="line"></span><br><span class="line">                   email.<span class="built_in">sendEmail</span>(  SmtpServer,  SmtpPort, SenderEmail,  SenderPassword,  ReceiveEmail,  SubjectTitle,  Content);</span><br><span class="line"></span><br><span class="line">               &#125;);</span><br><span class="line">       check++;</span><br><span class="line">   &#125;</span><br><span class="line">   email_login_page-&gt;<span class="built_in">show</span>();</span><br></pre></td></tr></table></figure><p>这里采用正则表达式获取用户邮箱的@后面内容，&lt;例如<a href="mailto:&#x78;&#120;&#64;&#113;&#113;&#x2e;&#99;&#111;&#109;">&#x78;&#120;&#64;&#113;&#113;&#x2e;&#99;&#111;&#109;</a>&gt;，会获取“qq.com”,接着在qq前加上smtp.前缀，就是qq的smtp服务器smtp.qq.com，端口一般时25，接着将用于发送邮件的邮箱的邮箱名、授权码、收件邮箱的邮箱名、邮件标题（这里默认为“生日快乐！”）、邮箱正文（用户之前编辑的内容）作为sendEmail函数的参数，在sendEmail函数中实现邮件发送。在sendEmail函数中，采取TCP连接方式，用一个QTcpSocket对象的write函数向服务器发送信息，用readAll函数接收服务器发送回来的信息，由于此代码长度较长，而且重复的内容较多（基本上就是依次发送上方的七个参数的内容给服务器）这里只展示一个小片段：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104932783.png" alt="image-20240617104932783"></p><h3 id="4、搜索功能："><a href="#4、搜索功能：" class="headerlink" title="4、搜索功能："></a><strong>4、搜索功能：</strong></h3><p>在主界面的上方，有一个搜索框，可以对联系人信息进行搜索：</p><p>例如这里输入“境”出现了王境博和环境专业的乌萨奇</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104942257.png" alt="image-20240617104942257"></p><p>模糊搜索功能实现代码如下：</p><p>如果输入框中没有内容，这段代码还可以是实现刷新功能</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617104951215.png" alt="image-20240617104951215"></p><p>查询成功后，用query去初始化model，再用model去渲染tableview就可以显示查找结果。（上文有类似代码）</p><h3 id="5、用户信息修改功能："><a href="#5、用户信息修改功能：" class="headerlink" title="5、用户信息修改功能："></a><strong>5、用户信息修改功能：</strong></h3><p>点击用户头像会弹出来一个菜单项</p><p>“修改头像”、“注销账号”的功能与联系人的添加头像和删除类似，这里不再赘述</p><p>“修改用户名”和“修改密码”类似，这里以后者为例</p><p>点击按钮后，弹出一个修改密码界面，要求用户先输入原密码，再两次输入新密码，</p><p>如果两次输入的密码相同，就用UPDATE语句修改accounts表中id&#x3D;user_id处的password字段信息</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105005480.png" alt="image-20240617105005480"></p><p>注销账号页面如下：<br><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105013921.png" alt="image-20240617105013921" style="zoom:50%;"></p><p>退出登录功能：</p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105028186.png" alt="image-20240617105028186" style="zoom: 67%;"><p>发送了一个exit_account信号，与Login类的on_show槽函数连接，退出登录后，将会返回到Login界面</p><h3 id="6、便利贴-备忘录"><a href="#6、便利贴-备忘录" class="headerlink" title="6、便利贴&#x2F;备忘录"></a><strong>6、便利贴&#x2F;备忘录</strong></h3><p>在主界面的左下角有一个便利贴功能，可以记录一下一些待办事项</p><p>便利贴（plainTextEdit）的样式设定为0.6的透明度</p><p>而一旦用户的鼠标悬停&#x2F;正在编辑plainTextEdit，就设定背景为黄色，方便用户看清内容（前文的搜索框也采用了类似的设计）</p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105042851.png" alt="image-20240617105042851" style="zoom: 67%;"><p>便利贴信息存储在accounts表格的memo字段，在mainfunc的init中进行初始化，同时，在每个退出主界面的地方都设定了自动保存（例如点击头像-退出登录、直接点击页面右上角的关闭）这样就实现了自动保存，减少用户操作的繁琐性。</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105101040.png" alt="image-20240617105101040"></p><h3 id="7、音效"><a href="#7、音效" class="headerlink" title="7、音效"></a><strong>7、音效</strong></h3><p>在MySound类中，有五个QSoundEffect成员变量，在构造函数进行初始化后，只需要在对应位置用QSoundEffect对象的.play()函数就可以实现音效的播放</p><p>MySound类的构造函数如下：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">MySound::<span class="built_in">MySound</span>(QObject *parent)</span><br><span class="line">    : QObject&#123;parent&#125;</span><br><span class="line">&#123;</span><br><span class="line">    ha.<span class="built_in">setSource</span>(QUrl::<span class="built_in">fromLocalFile</span>(<span class="string">&quot;:/sound/ha.wav&quot;</span>));</span><br><span class="line">    ha.<span class="built_in">setLoopCount</span>(<span class="number">1</span>);  <span class="comment">//循环次数</span></span><br><span class="line">    ha.<span class="built_in">setVolume</span>(<span class="number">0.25f</span>); <span class="comment">//音量  0~1之间</span></span><br><span class="line"></span><br><span class="line">    highest_yaha.<span class="built_in">setSource</span>(QUrl::<span class="built_in">fromLocalFile</span>(<span class="string">&quot;:/sound/highest_yaha.wav&quot;</span>));</span><br><span class="line">    highest_yaha.<span class="built_in">setLoopCount</span>(<span class="number">1</span>);</span><br><span class="line">    highest_yaha.<span class="built_in">setVolume</span>(<span class="number">0.25f</span>);</span><br><span class="line"></span><br><span class="line">    yaha.<span class="built_in">setSource</span>(QUrl::<span class="built_in">fromLocalFile</span>(<span class="string">&quot;:/sound/yaha.wav&quot;</span>));</span><br><span class="line">    yaha.<span class="built_in">setLoopCount</span>(<span class="number">1</span>);</span><br><span class="line">    yaha.<span class="built_in">setVolume</span>(<span class="number">0.25f</span>);</span><br><span class="line"></span><br><span class="line">    wula.<span class="built_in">setSource</span>(QUrl::<span class="built_in">fromLocalFile</span>(<span class="string">&quot;:/sound/wuda.wav&quot;</span>));</span><br><span class="line">    wula.<span class="built_in">setLoopCount</span>(<span class="number">1</span>);</span><br><span class="line">    wula.<span class="built_in">setVolume</span>(<span class="number">0.25f</span>);</span><br><span class="line"></span><br><span class="line">    wulayaha.<span class="built_in">setSource</span>(QUrl::<span class="built_in">fromLocalFile</span>(<span class="string">&quot;:/sound/wulayahayahawula.wav&quot;</span>));</span><br><span class="line">    wulayaha.<span class="built_in">setLoopCount</span>(<span class="number">1</span>);</span><br><span class="line">    wulayaha.<span class="built_in">setVolume</span>(<span class="number">0.25f</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="技术亮点"><a href="#技术亮点" class="headerlink" title="技术亮点"></a><strong>技术亮点</strong></h2><h3 id="数据库设计"><a href="#数据库设计" class="headerlink" title="数据库设计"></a><strong>数据库设计</strong></h3><p>accounts表设计</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105411435.png" alt="image-20240617105411435"></p><p>contacts表设计：</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105418807.png" alt="image-20240617105418807"></p><p>本数据库设计的亮点在于将accounts表的id字段（主键）设定为了 contacts表中user_id字段的外键，同时将On Update和On Delete都设定为了CASCADE,保证了数据的同步。</p><p>通过这样的方式，每一个联系人都用一个user_id字段表明这个联系人属于哪一个特定的用户，即便将所有联系人数据存储在一张表格中，也不会导致混乱。</p><h3 id="云端数据库"><a href="#云端数据库" class="headerlink" title="云端数据库"></a><strong>云端数据库</strong></h3><p>本项目将数据库建立在阿里云的RDS数据库，解决了最终.exe程序的连接问题，使得程序真正成为一个可独立运行的软件。</p><p>无论在什么地方，采用什么设备，只要能够正常联网，用户就可以查询到自己账户里的联系人信息。</p><p>部分用户展示（表明程序可正常运行）</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105450395.png" alt="image-20240617105450395"></p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105457358.png" alt="image-20240617105457358"></p><h3 id="右键菜单的实现："><a href="#右键菜单的实现：" class="headerlink" title="右键菜单的实现："></a><strong>右键菜单的实现：</strong></h3><p>本项目在头像处和主界面TableView处都使用了右键菜单，这里以主界面的实现方式进行讲解：</p><p>在Mainfunc的set_Menu函数中，首先调用QTableView的setContextMenuPolicy(Qt::CustomContextMenu)函数</p><p>使右键能够发送customContextMenuRequested信号</p><p>接着为QMenu对象添加两个QAction对象，然后将customContextMenuRequested信号与槽函数Menu_Slot连接</p><p>以及将两个QAction对象triggered信号与Modify_func对象中对应的对应槽函数连接（这两个槽函数在上文进行过讲解）</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Mainfunc::set_menu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//右键发送customContextMenuRequested 信号</span></span><br><span class="line">    ui-&gt;contact_table-&gt;<span class="built_in">setContextMenuPolicy</span>(Qt::CustomContextMenu);</span><br><span class="line">    <span class="keyword">this</span>-&gt;tableviewMenu = <span class="keyword">new</span> <span class="built_in">QMenu</span>( ui-&gt;contact_table); <span class="comment">// menu的父类是tableview 对象</span></span><br><span class="line"></span><br><span class="line">    Action1 = <span class="keyword">new</span> <span class="built_in">QAction</span>(<span class="string">&quot;查看&quot;</span>, ui-&gt;contact_table);</span><br><span class="line">    Action2 = <span class="keyword">new</span> <span class="built_in">QAction</span>(<span class="string">&quot;删除&quot;</span>, ui-&gt;contact_table);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加菜单项</span></span><br><span class="line">    tableviewMenu-&gt;<span class="built_in">addAction</span>(Action1);</span><br><span class="line">    tableviewMenu-&gt;<span class="built_in">addAction</span>(Action2);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">connect</span>( ui-&gt;contact_table,&amp;QTableView::customContextMenuRequested,<span class="keyword">this</span>,&amp;Mainfunc::Menu_Slot);</span><br><span class="line">    <span class="built_in">connect</span>(Action1, &amp;QAction::triggered,modify_func,&amp;Modify_func::Action1_Slot);</span><br><span class="line">    <span class="built_in">connect</span>(Action2, &amp;QAction::triggered,modify_func,&amp;Modify_func::Action2_Slot);</span><br><span class="line"></span><br><span class="line">    modify_func-&gt;user_id=user_id;<span class="comment">//传递user_id</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">connect</span>(modify_func, &amp;Modify_func::delete_ready,<span class="keyword">this</span>,&amp;Mainfunc::show_cot); <span class="comment">//删除成功后，刷新tableview</span></span><br><span class="line">    <span class="built_in">connect</span>(modify_func, &amp;Modify_func::change_ready,<span class="keyword">this</span>,&amp;Mainfunc::show_cot); <span class="comment">//修改成功后，刷新tableview</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在Menu_Slot函数中，首先获取鼠标点击位置的索引，并对点击的位置、选取的项目个数进行检验</p><p>只有在表格中的有效数据位置选中单个项目才会唤出右键菜单</p><p>接着将索引以及Mainfunc页面的model进行传递给了Modify_func对象</p><p>方便后续在数据库中通过这些信息进行查询，以更改&#x2F;删除某一特定联系人的数据。  </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//接收鼠标右键信号</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Mainfunc::Menu_Slot</span><span class="params">(QPoint p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    QModelIndex index = ui-&gt;contact_table-&gt;<span class="built_in">indexAt</span>(p);<span class="comment">//获取鼠标点击位置项的索引</span></span><br><span class="line">    <span class="keyword">if</span>(index.<span class="built_in">isValid</span>())<span class="comment">//检验数据项是否有效，空白处点击无菜单</span></span><br><span class="line">    &#123;</span><br><span class="line">        Action1-&gt;<span class="built_in">setVisible</span>(<span class="literal">true</span>);</span><br><span class="line">        Action2-&gt;<span class="built_in">setVisible</span>(<span class="literal">true</span>);</span><br><span class="line">        QItemSelectionModel* selections =  ui-&gt;contact_table-&gt;<span class="built_in">selectionModel</span>();<span class="comment">//获取当前的选择模型</span></span><br><span class="line">        QModelIndexList selected = selections-&gt;<span class="built_in">selectedIndexes</span>();<span class="comment">//返回当前选择的模型索引</span></span><br><span class="line">        <span class="keyword">if</span>(selected.<span class="built_in">count</span>() ==<span class="number">1</span>) <span class="comment">//选择单个项目时</span></span><br><span class="line">        &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>   <span class="comment">//多选，不显示菜单项</span></span><br><span class="line">        &#123;</span><br><span class="line">            Action1-&gt;<span class="built_in">setVisible</span>(<span class="literal">false</span>);</span><br><span class="line">            Action2-&gt;<span class="built_in">setVisible</span>(<span class="literal">false</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//先传递数据，再显示菜单</span></span><br><span class="line">        QAbstractItemModel *model =  ui-&gt;contact_table-&gt;<span class="built_in">model</span>(); <span class="comment">// 获取 QTableView 的模型</span></span><br><span class="line">        <span class="comment">//传递当前模型和鼠标位置索引</span></span><br><span class="line">        modify_func-&gt;model=model;</span><br><span class="line">        modify_func-&gt;index=index;</span><br><span class="line"></span><br><span class="line">        sound.wula.<span class="built_in">play</span>();</span><br><span class="line">        tableviewMenu-&gt;<span class="built_in">exec</span>(QCursor::<span class="built_in">pos</span>());<span class="comment">//显示菜单</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="生日查询功能"><a href="#生日查询功能" class="headerlink" title="生日查询功能"></a><strong>生日查询功能</strong></h3><p>在实现类似查询倒数xx天过生日的人的时候，网上的解决方案经常会存在一个跨年问题，也有很多人提出了自己的解决方案，我在这里也展示一下我自己想出的解决方案，如有错误欢迎指正。</p><p>如果没有发生跨年，DATE_FORMAT(birth,’%m%d’)先截取DATE类型的数据的月份和日期信息</p><p>接着要求这个数据</p><p>BETWEEN DATE_FORMAT(CURDATE()-INTERVAL :days DAY,’%m%d’) AND DATE_FORMAT(CURDATE()+INTERVAL :days DAY,’%m%d’)</p><p>其中：day将会被赋值为用户选择的那个天数，表明在当前日期前后各n天</p><p>假如发生了跨年，则改为：</p><p>BETWEEN DATE_FORMAT(CURDATE()-INTERVAL :days DAY,’%m%d’) AND STR_TO_DATE(‘12.31’,’%m.%d’)</p><p>OR</p><p>BETWEEN STR_TO_DATE(‘1.1’,’%m.%d’) AND DATE_FORMAT(CURDATE()+INTERVAL :days DAY,’%m%d’)</p><p>这表明将查询的日期限定在当前日期减去n天后的日期到12.31<strong>或</strong>1.1到当前日期加上n天后的日期</p><p>可以发现，无论是向去年跨一年还是向明年跨一年，这段代码都可以实现查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sound.wula.play();</span><br><span class="line">   QString days <span class="operator">=</span> ui<span class="operator">-</span><span class="operator">&gt;</span>spinBox_3<span class="operator">-</span><span class="operator">&gt;</span>text();</span><br><span class="line">   <span class="type">int</span> daysInt <span class="operator">=</span> days.toInt();</span><br><span class="line">   <span class="operator">/</span><span class="operator">/</span>往前后各数daysInt天</span><br><span class="line">   QSqlQuery query;</span><br><span class="line">   QDate currentDate <span class="operator">=</span> QDate::currentDate();</span><br><span class="line">   QDate past_searchDate <span class="operator">=</span> currentDate.addDays(<span class="operator">-</span>daysInt);</span><br><span class="line">   QDate future_searchDate <span class="operator">=</span> currentDate.addDays(daysInt);</span><br><span class="line">   if(past_searchDate.year()<span class="operator">=</span><span class="operator">=</span>currentDate.year()<span class="operator">&amp;&amp;</span>future_searchDate.year()<span class="operator">=</span><span class="operator">=</span>currentDate.year())&#123;</span><br><span class="line">       query.prepare(&quot;SELECT name, birth, phone, email, type FROM contacts WHERE user_id = :user_id&quot;</span><br><span class="line">                     &quot;AND &quot;</span><br><span class="line">                     &quot;(DATE_FORMAT(birth,&#x27;%m%d&#x27;) BETWEEN DATE_FORMAT(CURDATE()-INTERVAL :days DAY,&#x27;%m%d&#x27;) AND DATE_FORMAT(CURDATE()+INTERVAL :days DAY,&#x27;%m%d&#x27;))&quot;);</span><br><span class="line">   &#125;<span class="keyword">else</span> <span class="operator">/</span><span class="operator">/</span>跨年</span><br><span class="line">   &#123;</span><br><span class="line">       query.prepare(&quot;SELECT name, birth, phone, email, type FROM contacts WHERE user_id = :user_id&quot;</span><br><span class="line">                     &quot;AND &quot;</span><br><span class="line">                     &quot;((DATE_FORMAT(birth,&#x27;%m%d&#x27;) BETWEEN DATE_FORMAT(CURDATE()-INTERVAL :days DAY,&#x27;%m%d&#x27;) AND STR_TO_DATE(&#x27;12.31&#x27;,&#x27;%m.%d&#x27;)))&quot;</span><br><span class="line">                     &quot;OR&quot;</span><br><span class="line">                     &quot;(DATE_FORMAT(birth,&#x27;%m%d&#x27;) BETWEEN STR_TO_DATE(&#x27;1.1&#x27;,&#x27;%m.%d&#x27;) AND DATE_FORMAT(CURDATE()+INTERVAL :days DAY,&#x27;%m%d&#x27;))&quot;);</span><br><span class="line">   &#125;</span><br><span class="line">   query.bindValue(&quot;:user_id&quot;, this<span class="operator">-</span><span class="operator">&gt;</span>user_id);</span><br><span class="line">   query.bindValue(&quot;:days&quot;, daysInt);</span><br><span class="line">   query.exec();</span><br></pre></td></tr></table></figure><h2 id="难点攻破"><a href="#难点攻破" class="headerlink" title="难点攻破"></a><strong>难点攻破</strong></h2><p>&nbsp;</p><h3 id="缺少MySQL驱动的问题"><a href="#缺少MySQL驱动的问题" class="headerlink" title="缺少MySQL驱动的问题"></a><strong>缺少MySQL驱动的问题</strong></h3><p>在QT中使用MySQL的人可能会遭遇一个普遍的问题：QT没有提供MySQL的驱动</p><p>解决方法：先在MySQL的安装路径中找到libmysql.dll和libmysql.lib这两个文件</p><p>接着将这两个文件复制到qt的编译器安装路径（我用的是mingw）的bin文件夹中</p><p>下一步需要利用qt编译qsqlmysql.dll以及qsqlmysqld.dll这两个文件，并将它们添加到qt的编译器安装路径的\plugins\sqldrivers文件夹中，但是这一步较为繁琐，详细流程可以参考：</p><p><a href="https://blog.csdn.net/m0_46273020/article/details/104256219">关于解决Qt MySql没有QMYSQL驱动文件的问题详解_qt可发现的驱动没有qmysql-CSDN博客</a></p><p>我的解决方案是在github上找到了别人编译好的.dll文件，github链接为：</p><p><a href="https://github.com/thecodemonkey86/qt_mysql_driver">thecodemonkey86&#x2F;qt_mysql_driver: Typical symptom: QMYSQL driver not loaded. Solution: get pre-built Qt SQL driver plug-in required to establish a connection to MySQL &#x2F; MariaDB using Qt. Download qsqlmysql.dll binaries built from official Qt source code (github.com)</a></p><p>注意这里新版本的文件可以对版本兼容，但是添加老版本的库文件在新版本无法正常运行</p><p>最好下载对应qt版本的库文件。</p><h3 id="生日查询功能的简洁实现"><a href="#生日查询功能的简洁实现" class="headerlink" title="生日查询功能的简洁实现"></a><strong>生日查询功能的简洁实现</strong></h3><p>在实现年-月-日查询的时候，我希望“0”作为一个默认值，代表所有，这导致一个问题，假如我通过if语句去判断三个QspinBox</p><p>的数值是否为0，根据排列组合，共有8种可能情况，这样写会导致代码的臃肿。</p><p>最终我利用了Qstring可以直接相加的特性，大大简化了代码，代码如下：</p><p>首先获取用户输入，判定每个QspinBox数值是否为0，如果不为0，就加上AND …sql语句</p><p>同样的，在执行查询之前，也判定各个数值是否为0，如果不为0，就对对应的占位符进行赋值</p><p>这样就用较短的代码实现了查询功能。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (year != <span class="string">&quot;0&quot;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        whereClause += <span class="string">&quot; AND YEAR(birth) = :year&quot;</span>;</span><br><span class="line">        yearValue = year;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (month != <span class="string">&quot;0&quot;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        whereClause += <span class="string">&quot; AND MONTH(birth) = :month&quot;</span>;</span><br><span class="line">        monthValue = month;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (day != <span class="string">&quot;0&quot;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        whereClause += <span class="string">&quot; AND DAY(birth) = :day&quot;</span>;</span><br><span class="line">        dayValue = day;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    QSqlQuery query;</span><br><span class="line">    query.<span class="built_in">prepare</span>(<span class="string">&quot;SELECT name, birth, phone, email, type &quot;</span></span><br><span class="line">                  <span class="string">&quot;FROM contacts &quot;</span></span><br><span class="line">                  + whereClause);</span><br><span class="line">    query.<span class="built_in">bindValue</span>(<span class="string">&quot;:user_id&quot;</span>, <span class="keyword">this</span>-&gt;user_id);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (year != <span class="string">&quot;0&quot;</span>) query.<span class="built_in">bindValue</span>(<span class="string">&quot;:year&quot;</span>, yearValue);</span><br><span class="line">    <span class="keyword">if</span> (month != <span class="string">&quot;0&quot;</span>) query.<span class="built_in">bindValue</span>(<span class="string">&quot;:month&quot;</span>, monthValue);</span><br><span class="line">    <span class="keyword">if</span> (day != <span class="string">&quot;0&quot;</span>) query.<span class="built_in">bindValue</span>(<span class="string">&quot;:day&quot;</span>, dayValue);</span><br><span class="line"></span><br><span class="line">    query.<span class="built_in">exec</span>();</span><br></pre></td></tr></table></figure><h3 id="exe程序无法运行问题"><a href="#exe程序无法运行问题" class="headerlink" title=".exe程序无法运行问题"></a><strong>.exe程序无法运行问题</strong></h3><p>在将本程序打包为.exe程序供其他人使用的时候，遭遇了一系列问题，但最后都一一解决，接下来我将逐个介绍。</p><h4 id="Cannot-load-lilbrary-Qt6Core-dil问题"><a href="#Cannot-load-lilbrary-Qt6Core-dil问题" class="headerlink" title="Cannot load lilbrary Qt6Core.dil问题"></a>Cannot load lilbrary Qt6Core.dil问题</h4><p>在一开始将.exe发送给其他人时，他们反映会出现Cannot load lilbrary Qt6Core.dil的报错</p><p>这里我已经通过windeployqt 的方式添加了必要的文件，在文件夹中也的确有Qt6Core.dil这一个文件</p><p>查阅资料发现可能是因为缺少libgcc_s_seh-1.dll，libwinpthread.dll，libstdc++-6.dll这三个动态库，于是在编译器（mingw）的bin目录下拷贝这三个动态库，再次打包，发现报错仍然存在。</p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105807295.png" alt="image-20240617105807295" style="zoom:50%;"><p>进一步查阅资料，发现需要将编译器的bin目录添加到系统的环境变量中，添加后再次打包，程序在其他电脑上终于可以正常运行。</p><p><img src="/2024/06/17/%E4%B9%8C%E8%90%A8%E5%A5%87%E9%80%9A%E8%AE%AF%E5%BD%95%E9%A1%B9%E7%9B%AE%E6%8A%A5%E5%91%8A-1/image-20240617105827180-1718594953596-72.png" alt="image-20240617105827180"></p><h4 id="无法连接数据库问题"><a href="#无法连接数据库问题" class="headerlink" title="无法连接数据库问题"></a>无法连接数据库问题</h4><p>程序可以运行后，我的舍友尝试使用，点击按钮却没有响应，经排查，是因为没有连接上数据库</p><p>一开始我的数据库建立在本地，即便开放了防火墙，开放数据库的权限（允许任意ip地址的用户登录数据库账号）</p><p>其他人也很难连接到我本地的数据库，借用他的电脑进行测试，发现无法Ping通，而随后我用将ip改为我的阿里云ECS服务器，发现可以Ping通，于是决定将数据库建立在云端。</p><p>一开始我是使用ECS建立数据库，但是在Linux系统中运行MySQL的时候一直出现mysql: error while loading shared libraries: libssl.so.10的问题，于是决定采用RDS数据库服务。</p><p>通过在建立账号-开放权限-用workbench连接建立数据库-设置白名单允许所有ip地址的用户连接</p><p>等一系列操作后，用户终于可以正确连接我的数据库，exe程序也终于可以正常运行。</p><h2 id="结语-源代码获取"><a href="#结语-源代码获取" class="headerlink" title="结语&#x2F;源代码获取"></a>结语&#x2F;源代码获取</h2><p>本项目在11周开始开发，到14周周末基本开发完成，历时接近一个月。</p><p>在这个过程中，我学习了QT的使用，基本的SQL语法，学习了数据库的各种相关操作。</p><p>同时，第一次做一个这么完整的项目，也让我对真正的开发工作有了全新的理解，在开发过程中，各个头文件采取怎样的包含关系，如何做好内存管理，如何减少重复的代码量，如何对相关功能代码进行正确的封装，如何增加代码的可读性，这些都是我在之前的学习过程中从未遇到的挑战，也是我在此项目开发过程中收获的宝贵经验。</p><p>在开发过程中，我也遇到过一个又一个难题，从一开始的QT缺少MySQL驱动，到如何传递user_id保证联系人与账户对应，再到如何实现插入图片的功能，以及如何对图片大小进行压缩（有一次我插入一张画质较高的照片直接导致程序崩溃），到如何实现生日查询、如何发送邮件、如何实现右键菜单、如何进行页面美化以及如何正确打包以便发给其他用户使用，最终这些问题都通过在网上查找各种教程顺利解决，这一过程很好地锻炼了我的解决问题能力以及自学能力，为日后更加深入的学习打好了基础。</p><p>最后，感谢老师细致的教学，感谢所有帮助我测试程序、改进功能的朋友们，也感谢那个辛勤付出的自己。</p><p>本项目源代码已上传github，链接：<a href="https://github.com/HOWILLMAKEIT/UsagiContacts">HOWILLMAKEIT&#x2F;UsagiContacts: C++大作业之乌萨奇通讯录 (github.com)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> QT C++ </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
